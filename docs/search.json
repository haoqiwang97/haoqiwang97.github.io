[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Haoqi Wang",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nFeb 25, 2023\n\n\nData challenge - Snowflake 2023\n\n\nds\n\n\n\n\nFeb 20, 2023\n\n\nData challenge - Expedia 2022\n\n\nds\n\n\n\n\nFeb 20, 2023\n\n\nData useful notes\n\n\nds\n\n\n\n\nDec 11, 2022\n\n\nData challenge - 4. fraud\n\n\nds\n\n\n\n\nDec 11, 2022\n\n\nData challenge - 15. diversity workplace\n\n\nds\n\n\n\n\nDec 7, 2022\n\n\nData challenge - 14. shuttle stops\n\n\nds\n\n\n\n\nDec 7, 2022\n\n\nData challenge - 3. employee retention\n\n\nds\n\n\n\n\nDec 7, 2022\n\n\nData challenge - 19. subscription retention\n\n\nds\n\n\n\n\nDec 7, 2022\n\n\nData challenge - 10. credit card\n\n\nds\n\n\n\n\nDec 7, 2022\n\n\nData challenge - 12. loan grant\n\n\nds\n\n\n\n\nDec 6, 2022\n\n\nData challenge - 7. email\n\n\nds\n\n\n\n\nDec 4, 2022\n\n\nData challenge - 1. conversion rate\n\n\nds\n\n\n\n\nDec 1, 2022\n\n\nData challenge - 13. Json city similarities\n\n\nds\n\n\n\n\nDec 1, 2022\n\n\npandas groupby, apply and agg\n\n\npy\n\n\n\n\nNov 27, 2022\n\n\nExploratory data analysis\n\n\nds\n\n\n\n\nNov 26, 2022\n\n\nsklearn - pipeline\n\n\nundefined\n\n\n\n\nNov 24, 2022\n\n\nData challenge - 5. funnel analysis\n\n\nds\n\n\n\n\nAug 4, 2022\n\n\nPass by object reference\n\n\nalgo\n\n\n\n\nAug 1, 2022\n\n\nLeetCode\n\n\nalgo\n\n\n\n\nMay 31, 2022\n\n\nPython oop\n\n\nundefined\n\n\n\n\nOct 10, 2021\n\n\nTACC tutorial\n\n\nbash\n\n\n\n\nSep 2, 2021\n\n\nRmarkdown concatenate pdfs\n\n\nr\n\n\n\n\nSep 2, 2021\n\n\nStat consulting\n\n\nstat\n\n\n\n\nAug 15, 2021\n\n\nBash\n\n\nbash\n\n\n\n\nAug 13, 2021\n\n\nPrincipal component analysis\n\n\nml\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-10-10-tacc/index.html",
    "href": "posts/2021-10-10-tacc/index.html",
    "title": "TACC tutorial",
    "section": "",
    "text": "Notes of Texas Advanced Computing Center\n\nnodes = computers\ncores = processors\n\nexport BI=/work/projects/BioITeam\nexport PATH=$BI/common/bin:$PATH\necho $PATH | tr : '\\n'\n\nlauncher_creator.py -n hello_word -t 00:01:00 -q normal -w 1 -b 'echo \"hello world\"'\n\nless hello_word.slurm\nsbatch hello_word.slurm\nsqueue -u haoqi\n\nmodule load bedtools\nmodule spider bedtools\n\nmodule list\nmodule load python3\nmodule load intel/16.0.3\nmodule spider python\n\nkyoo list\nkyoo normal\nOther materials - TACC Workshop: https://containers-at-tacc.readthedocs.io/en/latest/ - R + TACC: https://learn.tacc.utexas.edu/mod/page/view.php?id=24"
  },
  {
    "objectID": "posts/2021-09-02-stat-consulting/index.html",
    "href": "posts/2021-09-02-stat-consulting/index.html",
    "title": "Stat consulting",
    "section": "",
    "text": "Summary of dealing with different situations.\nHelpful resources: - statistical collaboration training video"
  },
  {
    "objectID": "posts/2021-09-02-stat-consulting/index.html#experimental-design",
    "href": "posts/2021-09-02-stat-consulting/index.html#experimental-design",
    "title": "Stat consulting",
    "section": "experimental design",
    "text": "experimental design\n\nbudget\npower analysis, the best variable to use"
  },
  {
    "objectID": "posts/2021-09-02-stat-consulting/index.html#data",
    "href": "posts/2021-09-02-stat-consulting/index.html#data",
    "title": "Stat consulting",
    "section": "data",
    "text": "data\n\ndata collection\nreal data, sample data, makeup data\ndata type, image/number?\ndata format, long/wide format\ndata dictionary, rows, columns\ndata cleaning? I do not clean data, can you get the data cleaning?\n\ndecision to exclude variables should be justified\n\nvariable type"
  },
  {
    "objectID": "posts/2021-09-02-stat-consulting/index.html#data-visualization",
    "href": "posts/2021-09-02-stat-consulting/index.html#data-visualization",
    "title": "Stat consulting",
    "section": "data visualization",
    "text": "data visualization"
  },
  {
    "objectID": "posts/2021-09-02-stat-consulting/index.html#statistical-analysis",
    "href": "posts/2021-09-02-stat-consulting/index.html#statistical-analysis",
    "title": "Stat consulting",
    "section": "Statistical analysis",
    "text": "Statistical analysis\n\ncheck assumption"
  },
  {
    "objectID": "posts/2021-09-02-stat-consulting/index.html#section",
    "href": "posts/2021-09-02-stat-consulting/index.html#section",
    "title": "Stat consulting",
    "section": "2021-09-09",
    "text": "2021-09-09\n\n\nTo test the effectiveness of intervention on improving dietary intake [more fruit/veg intake (dietary recalls, plasma carotenoid biomarkers), less daily calories and sodium], weight loss, BP and QOL.\n\n\n\nso many variables, only pick the most important,\npick 2, separately calculate for power\n\n\n\nimproved. To test the effectiveness of intervention on weight loss and QOL.\n\n\n\nHypothesis 1a: - Participants in the intervention group will experience, on average, greater weight loss in comparison to the enhanced usual care group at week 6, after controlling for sex, and other significant covariates. - There will be a significant group by time interaction for weight loss, with participants in the intervention group experiencing, on average, greater weight loss in comparison to the usual care group over time, after controlling for sex, and other significant covariates.\n\n\n\nTo determine the effectiveness of intervention on building resources (mindful eating, health literacy) and diet-related self-efficacy.\n\n\n\nhow is this quantified, and it determines how we analyze the data"
  },
  {
    "objectID": "posts/2023-02-20-ds-useful/ds_useful_notes.html",
    "href": "posts/2023-02-20-ds-useful/ds_useful_notes.html",
    "title": "Data useful notes",
    "section": "",
    "text": "Table of contents:\n\n# Index\n\n- [Load data](#Load-data)\n- [Question 1](#q1)\n- [Question 2](#q2)\n- [Question 3](#q3)\n\nRed comments\n\n&lt;span style='color:red'&gt;Red comments&lt;/span&gt;\n\nExtract time features\n\ndata['year'] = data['timestamp'].dt.year\ndata['month'] = data['timestamp'].dt.month\ndata['dayofweek'] = data['timestamp'].dt.dayofweek\ndata['hour'] = data['timestamp'].dt.hour\n\nPlot\n\nfix, ax = plt.subplots(figsize=(8, 5))\nsns.histplot(x=X.ravel(), hue=kmeans.labels_, ax=ax)\nplt.show()\nTODO: ml & stat https://stats.stackexchange.com/questions/231285/dropping-one-of-the-columns-when-using-one-hot-encoding/329281#329281 if having regularization, no need to drop, https://stackoverflow.com/questions/67798922/necessity-of-droping-column-to-avoid-dumy-variable-traps-in-sklearn-one-hot-enco\ndisplay(df.describe(include=[‘category’])) # categorical types display(df.describe(include=[‘number’])) # numerical types\n6 https://github.com/stasi009/TakeHomeDataChallenges/blob/master/06.PriceTest/price_test.ipynb - Check whether test and control group is randomly splitted? - t test - decision tree visualization\n8 ‘average play event per hour’ as a metric to measure user engagement of a state.\n9 item-item similarity matrix\n11 ‘User Referral’ program has different effect in different countries\ndifferent country -&gt; cultural conflict/translation issue different os -&gt; compatible problem season -&gt; get cold/holiday\n16 I will define a metric called ‘first page ratio’, which measures within all pages about a certain city, how many of them are on page 1\n17 maybe there is some bug in implementation on Opera, which stops user visiting further pages maybe because the recommended friends aren’t shown in a noticeable position\n18 pd.read_csv(“video_features.csv”,index_col=‘video_id’) offline propagation time\nlaunch some program to improve “user referral”.\ncompanies have much more money to hire at the beginning of the year.\nYou probably also want to emphasize in the text that wrong data is worrisome and can be an indicator of some bug in the logging code. Therefore, you’d like to talk to the software engineer who implemented the code to see if, perhaps, there are some bugs which affect the data significantly.\nEx: I am going to pick a random forest to predict conversion rate. I pick a random forest cause: it usually requires very little time to optimize it (its default params are often close to the best ones) and it is strong with outliers, irrelevant variables, continuous and discrete variables. I will use the random forest to predict conversion, then I will use its partial dependence plots and variable importance to get insights about how it got information from the variables. Also, I will build a simple tree to find the most obvious user segments and see if they agree with RF partial dependence plots.\nAs you can see, conclusions usually end up being about: 1. tell marketing to get more of the good performing user segments 2. tell product to fix the experience for the bad performing ones\nhyper parameters tuning https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a https://towardsdatascience.com/crafting-one-pipeline-for-machine-learning-steps-373f03e44e1b\n“if we are resampling from our sample, how is it that we are learning something about the population rather than only about the sample?” Resampling is not done to provide an estimate of the population distribution–we take our sample itself as a model of the population. Rather, resampling is done to provide an estimate of the sampling distribution of the sample statistic in question.\nhow to select SVM kernel? - according to prior knowledge of invariances, what is particular to the geometry of your problem - Radial Basis Function kernel makes a good default - cross-valdiation based model selection\nYou can also try different techniques. For instance, the median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a ‘long tail’).\nImpurity-based feature importances can be misleading for high cardinality features (many unique values). See Permutation feature importance as an alternative below.\npandas axis 0, operate on rows, row will be gone axis 1, operate on columns, columns will be gone"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html",
    "title": "sklearn - pipeline",
    "section": "",
    "text": "https://towardsdatascience.com/from-ml-model-to-ml-pipeline-9f95c32c6512\n\n# Data manipulation\nfrom seaborn import load_dataset\nimport numpy as np\nimport pandas as pd\n# pd.options.display.precision = 4\n# pd.options.mode.chained_assignment = None\n\n# Machine learning pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import set_config\n# set_config(display=\"diagram\")\n\n\n# Load data\ncolumns = ['alive', 'class', 'embarked', 'who', 'alone', 'adult_male']\ndf = load_dataset('titanic').drop(columns=columns)\ndf['deck'] = df['deck'].astype('object')\nprint(df.shape)\ndf.head()\n\n(891, 9)\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\ndeck\nembark_town\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nNaN\nSouthampton\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nCherbourg\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nNaN\nSouthampton\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nC\nSouthampton\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nNaN\nSouthampton\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   survived     891 non-null    int64  \n 1   pclass       891 non-null    int64  \n 2   sex          891 non-null    object \n 3   age          714 non-null    float64\n 4   sibsp        891 non-null    int64  \n 5   parch        891 non-null    int64  \n 6   fare         891 non-null    float64\n 7   deck         203 non-null    object \n 8   embark_town  889 non-null    object \ndtypes: float64(2), int64(4), object(3)\nmemory usage: 62.8+ KB\n\n\n\nSEED = 42\nTARGET = 'survived'\nFEATURES = df.columns.drop(TARGET)\n\nNUMERICAL = df[FEATURES].select_dtypes('number').columns\nprint(f\"Numerical features: {', '.join(NUMERICAL)}\")\n\nCATEGORICAL = pd.Index(np.setdiff1d(FEATURES, NUMERICAL))\nprint(f\"Categorical features: {', '.join(CATEGORICAL)}\")\n\nNumerical features: pclass, age, sibsp, parch, fare\nCategorical features: deck, embark_town, sex\n\n\n\n\n\n# Impute numerical variables with mean\ndf_num_imputed = df[NUMERICAL].fillna(df[NUMERICAL].mean())\n# Normalise numerical variables\ndf_num_scaled = df_num_imputed.subtract(df_num_imputed.min(), axis=1)\\\n                              .divide(df_num_imputed.max()-df_num_imputed.min(), axis=1)\n\n# Impute categorical variables with a constant\ndf_cat_imputed = df[CATEGORICAL].fillna('missing')\n# One-hot-encode categorical variables\ndf_cat_encoded = pd.get_dummies(df_cat_imputed, drop_first=True)\n\n# Merge data\ndf_preprocessed = df_num_scaled.join(df_cat_encoded)\ndf_preprocessed.head()\n\n\n\n\n\n\n\n\npclass\nage\nsibsp\nparch\nfare\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\ndeck_missing\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_missing\nsex_male\n\n\n\n\n0\n1.0\n0.271174\n0.125\n0.0\n0.014151\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n1\n0.0\n0.472229\n0.125\n0.0\n0.139136\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1.0\n0.321438\n0.000\n0.0\n0.015469\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n3\n0.0\n0.434531\n0.125\n0.0\n0.103644\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n1.0\n0.434531\n0.000\n0.0\n0.015713\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n# Partition data\nX_train, X_test, y_train, y_test = train_test_split(df_preprocessed, df[TARGET],\n                                                    test_size=.2, random_state=SEED,\n                                                    stratify=df[TARGET])\n\n# Train a model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nLogisticRegression()\n\n\n\ndef calculate_roc_auc(model_pipe, X, y):\n    \"\"\"Calculate roc auc score.\n\n    Parameters:\n    ===========\n    model_pipe: sklearn model or pipeline\n    X: features\n    y: true target\n    \"\"\"\n    y_proba = model_pipe.predict_proba(X)[:,1]\n    return roc_auc_score(y, y_proba)\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(model, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(model, X_test, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8669\nTest ROC-AUC: 0.8329\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns=TARGET), df[TARGET],\n                                                    test_size=.2, random_state=SEED,\n                                                    stratify=df[TARGET])\nnum_imputer = SimpleImputer(strategy='mean')\ntrain_num_imputed = num_imputer.fit_transform(X_train[NUMERICAL])\n\nscaler = MinMaxScaler()\ntrain_num_scaled = scaler.fit_transform(train_num_imputed)\n\ncat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\ntrain_cat_imputed = cat_imputer.fit_transform(X_train[CATEGORICAL])\n\nencoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False)\ntrain_cat_encoded = encoder.fit_transform(train_cat_imputed)\n\ntrain_preprocessed = np.concatenate((train_num_scaled, train_cat_encoded), axis=1)\n\ncolumns = np.append(NUMERICAL, encoder.get_feature_names_out(CATEGORICAL))\npd.DataFrame(train_preprocessed, columns=columns, index=X_train.index).head()\n\n\n\n\n\n\n\n\npclass\nage\nsibsp\nparch\nfare\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\ndeck_missing\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_missing\nsex_male\n\n\n\n\n692\n1.0\n0.369285\n0.000\n0.000000\n0.110272\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n481\n0.5\n0.369285\n0.000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n527\n0.0\n0.369285\n0.000\n0.000000\n0.432884\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n855\n1.0\n0.220910\n0.000\n0.166667\n0.018250\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n801\n0.5\n0.384267\n0.125\n0.166667\n0.051237\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nmodel = LogisticRegression()\nmodel.fit(train_preprocessed, y_train)\n\nLogisticRegression()\n\n\n\ntest_num_imputed = num_imputer.transform(X_test[NUMERICAL])\ntest_num_scaled = scaler.transform(test_num_imputed)\ntest_cat_imputed = cat_imputer.transform(X_test[CATEGORICAL])\ntest_cat_encoded = encoder.transform(test_cat_imputed)\ntest_preprocessed = np.concatenate((test_num_scaled, test_cat_encoded), axis=1)\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(model, train_preprocessed, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(model, test_preprocessed, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8670\nTest ROC-AUC: 0.8332\n\n\n\n\n\n◼️ Splits input data into numerical and categorical groups ◼️ Preprocesses both groups in parallel ◼️ Concatenates the preprocessed data from both groups ◼️ Passes the preprocessed data into the model\n\nnumerical_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())\n])\n\ncategorical_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('num', numerical_pipe, NUMERICAL),\n    ('cat', categorical_pipe, CATEGORICAL)\n])\n\n\npreprocessors.fit(X_train)\n\nColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', MinMaxScaler())]),\n                                 Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('encoder',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 Index(['deck', 'embark_town', 'sex'], dtype='object'))])\n\n\n\npreprocessors.fit_transform(X_train)\n\narray([[1.        , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       [0.5       , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       [0.        , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       ...,\n       [1.        , 0.59788892, 0.125     , ..., 1.        , 0.        ,\n        0.        ],\n       [0.        , 0.58532295, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       [0.        , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ]])\n\n\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  Index(['deck', 'embark_town', 'sex'], dtype='object'))])),\n                ('model', LogisticRegression())])\n\n\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_test, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8670\nTest ROC-AUC: 0.8332\n\n\n\n\n\n\nclass Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self, features, method='constant', value='missing'):\n        self.features = features\n        self.method = method\n        self.value = value\n\n    def fit(self, X, y=None):\n        if self.method=='mean':\n            self.value = X[self.features].mean()\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n        X_transformed[self.features] = X[self.features].fillna(self.value)\n        return X_transformed\n\nclass Scaler(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n\n    def fit(self, X, y=None):\n        self.min = X[self.features].min()\n        self.range = X[self.features].max()-self.min\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n        X_transformed[self.features] = (X[self.features]-self.min)/self.range\n        return X_transformed\n\nclass Encoder(BaseEstimator, TransformerMixin):\n    def __init__(self, features, drop='first'):\n        self.features = features\n        self.drop = drop\n\n    def fit(self, X, y=None):\n        self.encoder = OneHotEncoder(sparse=False, drop=self.drop)\n        self.encoder.fit(X[self.features])\n        return self\n\n    def transform(self, X):\n        X_transformed = pd.concat([X.drop(columns=self.features).reset_index(drop=True),\n                                   pd.DataFrame(self.encoder.transform(X[self.features]),\n                                                columns=self.encoder.get_feature_names_out(self.features))],\n                                  axis=1)\n        return X_transformed\n\n\nfeat_pipe = Pipeline([\n    ('num_imputer', Imputer(NUMERICAL, method='mean')),\n    ('scaler', Scaler(NUMERICAL)),\n    ('cat_imputer', Imputer(CATEGORICAL)),\n    ('encoder', Encoder(CATEGORICAL))\n])\n\n\nfeat_pipe.fit(X_train, y_train)\n\nPipeline(steps=[('num_imputer',\n                 Imputer(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'),\n                         method='mean',\n                         value=pclass     2.308989\nage       29.807687\nsibsp      0.492978\nparch      0.390449\nfare      31.819826\ndtype: float64)),\n                ('scaler',\n                 Scaler(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'))),\n                ('cat_imputer',\n                 Imputer(features=Index(['deck', 'embark_town', 'sex'], dtype='object'))),\n                ('encoder',\n                 Encoder(features=Index(['deck', 'embark_town', 'sex'], dtype='object')))])\n\n\n\nfeat_pipe.fit_transform(X_train, y_train)\n\n\n\n\n\n\n\n\npclass\nage\nsibsp\nparch\nfare\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\ndeck_missing\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_missing\nsex_male\n\n\n\n\n0\n1.0\n0.369285\n0.000\n0.000000\n0.110272\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n0.5\n0.369285\n0.000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n0.0\n0.369285\n0.000\n0.000000\n0.432884\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n1.0\n0.220910\n0.000\n0.166667\n0.018250\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n0.5\n0.384267\n0.125\n0.166667\n0.051237\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n707\n1.0\n0.369285\n0.000\n0.000000\n0.015379\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n708\n0.0\n0.434531\n0.000\n0.000000\n1.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n709\n1.0\n0.597889\n0.125\n0.500000\n0.067096\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n710\n0.0\n0.585323\n0.000\n0.000000\n0.075147\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n711\n0.0\n0.369285\n0.000\n0.000000\n0.051822\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n712 rows × 16 columns\n\n\n\n\npipe = Pipeline([\n    ('num_imputer', Imputer(NUMERICAL, method='mean')),\n    ('scaler', Scaler(NUMERICAL)),\n    ('cat_imputer', Imputer(CATEGORICAL)),\n    ('encoder', Encoder(CATEGORICAL)),\n    ('model', LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('num_imputer',\n                 Imputer(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'),\n                         method='mean',\n                         value=pclass     2.308989\nage       29.807687\nsibsp      0.492978\nparch      0.390449\nfare      31.819826\ndtype: float64)),\n                ('scaler',\n                 Scaler(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'))),\n                ('cat_imputer',\n                 Imputer(features=Index(['deck', 'embark_town', 'sex'], dtype='object'))),\n                ('encoder',\n                 Encoder(features=Index(['deck', 'embark_town', 'sex'], dtype='object'))),\n                ('model', LogisticRegression())])\n\n\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_test, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8670\nTest ROC-AUC: 0.8332"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#wrong-approach",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#wrong-approach",
    "title": "sklearn - pipeline",
    "section": "",
    "text": "# Impute numerical variables with mean\ndf_num_imputed = df[NUMERICAL].fillna(df[NUMERICAL].mean())\n# Normalise numerical variables\ndf_num_scaled = df_num_imputed.subtract(df_num_imputed.min(), axis=1)\\\n                              .divide(df_num_imputed.max()-df_num_imputed.min(), axis=1)\n\n# Impute categorical variables with a constant\ndf_cat_imputed = df[CATEGORICAL].fillna('missing')\n# One-hot-encode categorical variables\ndf_cat_encoded = pd.get_dummies(df_cat_imputed, drop_first=True)\n\n# Merge data\ndf_preprocessed = df_num_scaled.join(df_cat_encoded)\ndf_preprocessed.head()\n\n\n\n\n\n\n\n\npclass\nage\nsibsp\nparch\nfare\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\ndeck_missing\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_missing\nsex_male\n\n\n\n\n0\n1.0\n0.271174\n0.125\n0.0\n0.014151\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n1\n0.0\n0.472229\n0.125\n0.0\n0.139136\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1.0\n0.321438\n0.000\n0.0\n0.015469\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n3\n0.0\n0.434531\n0.125\n0.0\n0.103644\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n1.0\n0.434531\n0.000\n0.0\n0.015713\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n# Partition data\nX_train, X_test, y_train, y_test = train_test_split(df_preprocessed, df[TARGET],\n                                                    test_size=.2, random_state=SEED,\n                                                    stratify=df[TARGET])\n\n# Train a model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nLogisticRegression()\n\n\n\ndef calculate_roc_auc(model_pipe, X, y):\n    \"\"\"Calculate roc auc score.\n\n    Parameters:\n    ===========\n    model_pipe: sklearn model or pipeline\n    X: features\n    y: true target\n    \"\"\"\n    y_proba = model_pipe.predict_proba(X)[:,1]\n    return roc_auc_score(y, y_proba)\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(model, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(model, X_test, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8669\nTest ROC-AUC: 0.8329"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#correct-approach-but-not-good",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#correct-approach-but-not-good",
    "title": "sklearn - pipeline",
    "section": "",
    "text": "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=TARGET), df[TARGET],\n                                                    test_size=.2, random_state=SEED,\n                                                    stratify=df[TARGET])\nnum_imputer = SimpleImputer(strategy='mean')\ntrain_num_imputed = num_imputer.fit_transform(X_train[NUMERICAL])\n\nscaler = MinMaxScaler()\ntrain_num_scaled = scaler.fit_transform(train_num_imputed)\n\ncat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\ntrain_cat_imputed = cat_imputer.fit_transform(X_train[CATEGORICAL])\n\nencoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False)\ntrain_cat_encoded = encoder.fit_transform(train_cat_imputed)\n\ntrain_preprocessed = np.concatenate((train_num_scaled, train_cat_encoded), axis=1)\n\ncolumns = np.append(NUMERICAL, encoder.get_feature_names_out(CATEGORICAL))\npd.DataFrame(train_preprocessed, columns=columns, index=X_train.index).head()\n\n\n\n\n\n\n\n\npclass\nage\nsibsp\nparch\nfare\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\ndeck_missing\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_missing\nsex_male\n\n\n\n\n692\n1.0\n0.369285\n0.000\n0.000000\n0.110272\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n481\n0.5\n0.369285\n0.000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n527\n0.0\n0.369285\n0.000\n0.000000\n0.432884\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n855\n1.0\n0.220910\n0.000\n0.166667\n0.018250\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n801\n0.5\n0.384267\n0.125\n0.166667\n0.051237\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nmodel = LogisticRegression()\nmodel.fit(train_preprocessed, y_train)\n\nLogisticRegression()\n\n\n\ntest_num_imputed = num_imputer.transform(X_test[NUMERICAL])\ntest_num_scaled = scaler.transform(test_num_imputed)\ntest_cat_imputed = cat_imputer.transform(X_test[CATEGORICAL])\ntest_cat_encoded = encoder.transform(test_cat_imputed)\ntest_preprocessed = np.concatenate((test_num_scaled, test_cat_encoded), axis=1)\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(model, train_preprocessed, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(model, test_preprocessed, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8670\nTest ROC-AUC: 0.8332"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-approach-1",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-approach-1",
    "title": "sklearn - pipeline",
    "section": "",
    "text": "◼️ Splits input data into numerical and categorical groups ◼️ Preprocesses both groups in parallel ◼️ Concatenates the preprocessed data from both groups ◼️ Passes the preprocessed data into the model\n\nnumerical_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())\n])\n\ncategorical_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('num', numerical_pipe, NUMERICAL),\n    ('cat', categorical_pipe, CATEGORICAL)\n])\n\n\npreprocessors.fit(X_train)\n\nColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', MinMaxScaler())]),\n                                 Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('encoder',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 Index(['deck', 'embark_town', 'sex'], dtype='object'))])\n\n\n\npreprocessors.fit_transform(X_train)\n\narray([[1.        , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       [0.5       , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       [0.        , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       ...,\n       [1.        , 0.59788892, 0.125     , ..., 1.        , 0.        ,\n        0.        ],\n       [0.        , 0.58532295, 0.        , ..., 1.        , 0.        ,\n        1.        ],\n       [0.        , 0.36928483, 0.        , ..., 1.        , 0.        ,\n        1.        ]])\n\n\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  Index(['deck', 'embark_town', 'sex'], dtype='object'))])),\n                ('model', LogisticRegression())])\n\n\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_test, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8670\nTest ROC-AUC: 0.8332"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-approach-2",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-approach-2",
    "title": "sklearn - pipeline",
    "section": "",
    "text": "class Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self, features, method='constant', value='missing'):\n        self.features = features\n        self.method = method\n        self.value = value\n\n    def fit(self, X, y=None):\n        if self.method=='mean':\n            self.value = X[self.features].mean()\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n        X_transformed[self.features] = X[self.features].fillna(self.value)\n        return X_transformed\n\nclass Scaler(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n\n    def fit(self, X, y=None):\n        self.min = X[self.features].min()\n        self.range = X[self.features].max()-self.min\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n        X_transformed[self.features] = (X[self.features]-self.min)/self.range\n        return X_transformed\n\nclass Encoder(BaseEstimator, TransformerMixin):\n    def __init__(self, features, drop='first'):\n        self.features = features\n        self.drop = drop\n\n    def fit(self, X, y=None):\n        self.encoder = OneHotEncoder(sparse=False, drop=self.drop)\n        self.encoder.fit(X[self.features])\n        return self\n\n    def transform(self, X):\n        X_transformed = pd.concat([X.drop(columns=self.features).reset_index(drop=True),\n                                   pd.DataFrame(self.encoder.transform(X[self.features]),\n                                                columns=self.encoder.get_feature_names_out(self.features))],\n                                  axis=1)\n        return X_transformed\n\n\nfeat_pipe = Pipeline([\n    ('num_imputer', Imputer(NUMERICAL, method='mean')),\n    ('scaler', Scaler(NUMERICAL)),\n    ('cat_imputer', Imputer(CATEGORICAL)),\n    ('encoder', Encoder(CATEGORICAL))\n])\n\n\nfeat_pipe.fit(X_train, y_train)\n\nPipeline(steps=[('num_imputer',\n                 Imputer(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'),\n                         method='mean',\n                         value=pclass     2.308989\nage       29.807687\nsibsp      0.492978\nparch      0.390449\nfare      31.819826\ndtype: float64)),\n                ('scaler',\n                 Scaler(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'))),\n                ('cat_imputer',\n                 Imputer(features=Index(['deck', 'embark_town', 'sex'], dtype='object'))),\n                ('encoder',\n                 Encoder(features=Index(['deck', 'embark_town', 'sex'], dtype='object')))])\n\n\n\nfeat_pipe.fit_transform(X_train, y_train)\n\n\n\n\n\n\n\n\npclass\nage\nsibsp\nparch\nfare\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\ndeck_missing\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_missing\nsex_male\n\n\n\n\n0\n1.0\n0.369285\n0.000\n0.000000\n0.110272\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n0.5\n0.369285\n0.000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n0.0\n0.369285\n0.000\n0.000000\n0.432884\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n1.0\n0.220910\n0.000\n0.166667\n0.018250\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n0.5\n0.384267\n0.125\n0.166667\n0.051237\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n707\n1.0\n0.369285\n0.000\n0.000000\n0.015379\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n708\n0.0\n0.434531\n0.000\n0.000000\n1.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n709\n1.0\n0.597889\n0.125\n0.500000\n0.067096\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n710\n0.0\n0.585323\n0.000\n0.000000\n0.075147\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n711\n0.0\n0.369285\n0.000\n0.000000\n0.051822\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n712 rows × 16 columns\n\n\n\n\npipe = Pipeline([\n    ('num_imputer', Imputer(NUMERICAL, method='mean')),\n    ('scaler', Scaler(NUMERICAL)),\n    ('cat_imputer', Imputer(CATEGORICAL)),\n    ('encoder', Encoder(CATEGORICAL)),\n    ('model', LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('num_imputer',\n                 Imputer(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'),\n                         method='mean',\n                         value=pclass     2.308989\nage       29.807687\nsibsp      0.492978\nparch      0.390449\nfare      31.819826\ndtype: float64)),\n                ('scaler',\n                 Scaler(features=Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object'))),\n                ('cat_imputer',\n                 Imputer(features=Index(['deck', 'embark_town', 'sex'], dtype='object'))),\n                ('encoder',\n                 Encoder(features=Index(['deck', 'embark_town', 'sex'], dtype='object'))),\n                ('model', LogisticRegression())])\n\n\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_test, y_test):.4f}\")\n\nTrain ROC-AUC: 0.8670\nTest ROC-AUC: 0.8332"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-1-pipe-transformer",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-1-pipe-transformer",
    "title": "sklearn - pipeline",
    "section": "elegant 1, pipe transformer",
    "text": "elegant 1, pipe transformer\n\n# Fit pipeline to training data\npipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\nprint(pipe)\npipe.fit(X_train)\n\n# Inspect training data before and after\nprint(\"******************** Training data ********************\")\ndisplay(X_train)\ndisplay(pd.DataFrame(pipe.transform(X_train), columns=pipe['encoder'].get_feature_names_out(X_train.columns)))\n\n# Inspect test data before and after\nprint(\"******************** Test data ********************\")\ndisplay(X_test)\ndisplay(pd.DataFrame(pipe.transform(X_test), columns=pipe['encoder'].get_feature_names_out(X_train.columns)))\n\nPipeline(steps=[('imputer',\n                 SimpleImputer(fill_value='missing', strategy='constant')),\n                ('encoder',\n                 OneHotEncoder(handle_unknown='ignore', sparse=False))])\n******************** Training data ********************\n******************** Test data ********************\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\n\n\n\n\n169\nYes\nSat\nDinner\n\n\n31\nNo\nNaN\nDinner\n\n\n112\nNo\nSun\nDinner\n\n\n187\nYes\nNaN\nDinner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker_No\nsmoker_Yes\nday_Sat\nday_Sun\nday_missing\ntime_Dinner\n\n\n\n\n0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n2\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\n\n\n\n\n19\nNo\nNaN\nDinner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker_No\nsmoker_Yes\nday_Sat\nday_Sun\nday_missing\ntime_Dinner\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-2-pipe-transformer-and-model-all-columns-the-same-way",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#elegant-2-pipe-transformer-and-model-all-columns-the-same-way",
    "title": "sklearn - pipeline",
    "section": "elegant 2, pipe transformer and model, all columns the same way",
    "text": "elegant 2, pipe transformer and model, all columns the same way\n\n# Fit pipeline to training data\npipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n    ('model', LinearRegression())\n])\npipe.fit(X_train, y_train)\n\n# Predict training data\ny_train_pred = pipe.predict(X_train)\nprint(f\"Predictions on training data: {y_train_pred}\")\n\n# Predict test data\ny_test_pred = pipe.predict(X_test)\nprint(f\"Predictions on test data: {y_test_pred}\")\n\nPredictions on training data: [10.63 18.35 38.07 30.46]\nPredictions on test data: [18.35]"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#columntransformer-numerical-and-categorical-features",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#columntransformer-numerical-and-categorical-features",
    "title": "sklearn - pipeline",
    "section": "columntransformer, numerical and categorical features",
    "text": "columntransformer, numerical and categorical features\n\n# Partition data\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['total_bill']),\n                                                    df['total_bill'],\n                                                    test_size=.2,\n                                                    random_state=seed)\n\n# Define categorical columns\ncategorical = list(X_train.select_dtypes('category').columns)\nprint(f\"Categorical columns are: {categorical}\")\n\n# Define numerical columns\nnumerical = list(X_train.select_dtypes('number').columns)\nprint(f\"Numerical columns are: {numerical}\")\n\nCategorical columns are: ['smoker', 'day', 'time']\nNumerical columns are: ['size']\n\n\n\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 4 entries, 169 to 187\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype   \n---  ------  --------------  -----   \n 0   smoker  4 non-null      category\n 1   day     2 non-null      category\n 2   time    4 non-null      category\n 3   size    2 non-null      float64 \ndtypes: category(3), float64(1)\nmemory usage: 528.0 bytes\n\n\n\n# Define categorical pipeline\ncat_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Define numerical pipeline\nnum_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\n# Fit column transformer to training data\npreprocessor = ColumnTransformer([\n    ('cat', cat_pipe, categorical),\n    ('num', num_pipe, numerical)\n])\npreprocessor.fit(X_train)\n\n# Prepare column names\ncat_columns = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical)\ncolumns = np.append(cat_columns, numerical)\n\n# Inspect training data before and after\nprint(\"******************** Training data ********************\")\ndisplay(X_train)\ndisplay(pd.DataFrame(preprocessor.transform(X_train), columns=columns))\n\n# Inspect test data before and after\nprint(\"******************** Test data ********************\")\ndisplay(X_test)\ndisplay(pd.DataFrame(preprocessor.transform(X_test), columns=columns))\n\n******************** Training data ********************\n******************** Test data ********************\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\nsize\n\n\n\n\n169\nYes\nSat\nDinner\n2.0\n\n\n31\nNo\nNaN\nDinner\nNaN\n\n\n112\nNo\nSun\nDinner\n3.0\n\n\n187\nYes\nNaN\nDinner\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker_No\nsmoker_Yes\nday_Sat\nday_Sun\nday_missing\ntime_Dinner\nsize\n\n\n\n\n0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.5\n\n\n2\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\nsize\n\n\n\n\n19\nNo\nNaN\nDinner\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker_No\nsmoker_Yes\nday_Sat\nday_Sun\nday_missing\ntime_Dinner\nsize\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.5\n\n\n\n\n\n\n\n\npreprocessor\n\nColumnTransformer(transformers=[('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['smoker', 'day', 'time']),\n                                ('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['size'])])\n\n\n\n# try changing sequence of cat and num\n# Define categorical pipeline\ncat_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False))\n])\n\n# Define numerical pipeline\nnum_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\n# Fit column transformer to training data\npreprocessor = ColumnTransformer([\n    ('num', num_pipe, numerical),\n    ('cat', cat_pipe, categorical)\n\n])\npreprocessor.fit(X_train)\n\n# Prepare column names\ncat_columns = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical)\n# columns = np.append(cat_columns, numerical)\ncolumns = np.append(numerical, cat_columns)\n\n# Inspect training data before and after\nprint(\"******************** Training data ********************\")\ndisplay(X_train)\ndisplay(pd.DataFrame(preprocessor.transform(X_train), columns=columns))\n\n# Inspect test data before and after\nprint(\"******************** Test data ********************\")\ndisplay(X_test)\ndisplay(pd.DataFrame(preprocessor.transform(X_test), columns=columns))\n\n******************** Training data ********************\n******************** Test data ********************\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\nsize\n\n\n\n\n169\nYes\nSat\nDinner\n2.0\n\n\n31\nNo\nNaN\nDinner\nNaN\n\n\n112\nNo\nSun\nDinner\n3.0\n\n\n187\nYes\nNaN\nDinner\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsize\nsmoker_Yes\nday_Sun\nday_missing\n\n\n\n\n0\n0.0\n1.0\n0.0\n0.0\n\n\n1\n0.5\n0.0\n0.0\n1.0\n\n\n2\n1.0\n0.0\n1.0\n0.0\n\n\n3\n0.5\n1.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\nsize\n\n\n\n\n19\nNo\nNaN\nDinner\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsize\nsmoker_Yes\nday_Sun\nday_missing\n\n\n\n\n0\n0.5\n0.0\n0.0\n1.0"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#columntransformer-with-model-numerical-and-categorical-features",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#columntransformer-with-model-numerical-and-categorical-features",
    "title": "sklearn - pipeline",
    "section": "columntransformer with model, numerical and categorical features",
    "text": "columntransformer with model, numerical and categorical features\n\n# Define categorical pipeline\ncat_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Define numerical pipeline\nnum_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\n# Combine categorical and numerical pipelines\npreprocessor = ColumnTransformer([\n    ('cat', cat_pipe, categorical),\n    ('num', num_pipe, numerical)\n])\n\n# Fit a pipeline with transformers and an estimator to the training data\npipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', LinearRegression())\n])\npipe.fit(X_train, y_train)\n\n# Predict training data\ny_train_pred = pipe.predict(X_train)\nprint(f\"Predictions on training data: {y_train_pred}\")\n\n# Predict test data\ny_test_pred = pipe.predict(X_test)\nprint(f\"Predictions on test data: {y_test_pred}\")\n\nPredictions on training data: [10.63 18.35 38.07 30.46]\nPredictions on test data: [18.35]\n\n\n\npipe\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['smoker', 'day', 'time']),\n                                                 ('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['size'])])),\n                ('model', LinearRegression())])"
  },
  {
    "objectID": "posts/2022-11-26-sklearn-pipeline/pipeline.html#feature-union",
    "href": "posts/2022-11-26-sklearn-pipeline/pipeline.html#feature-union",
    "title": "sklearn - pipeline",
    "section": "feature union",
    "text": "feature union\n\n# Define custom transformer\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Select only specified columns.\"\"\"\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.columns]\n\n# Define categorical pipeline\ncat_pipe = Pipeline([\n    ('selector', ColumnSelector(categorical)),\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Define numerical pipeline\nnum_pipe = Pipeline([\n    ('selector', ColumnSelector(numerical)),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\n# Fit feature union to training data\npreprocessor = FeatureUnion([\n    ('cat', cat_pipe),\n    ('num', num_pipe)\n])\npreprocessor.fit(X_train)\n\n# Prepare column names\ncat_columns = preprocessor.transformer_list[0][1][2].get_feature_names_out(categorical)\ncolumns = np.append(cat_columns, numerical)\n\n# Inspect training data before and after\nprint(\"******************** Training data ********************\")\ndisplay(X_train)\ndisplay(pd.DataFrame(preprocessor.transform(X_train), columns=columns))\n\n# Inspect test data before and after\nprint(\"******************** Test data ********************\")\ndisplay(X_test)\ndisplay(pd.DataFrame(preprocessor.transform(X_test), columns=columns))\n\n******************** Training data ********************\n******************** Test data ********************\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\nsize\n\n\n\n\n169\nYes\nSat\nDinner\n2.0\n\n\n31\nNo\nNaN\nDinner\nNaN\n\n\n112\nNo\nSun\nDinner\n3.0\n\n\n187\nYes\nNaN\nDinner\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker_No\nsmoker_Yes\nday_Sat\nday_Sun\nday_missing\ntime_Dinner\nsize\n\n\n\n\n0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.5\n\n\n2\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker\nday\ntime\nsize\n\n\n\n\n19\nNo\nNaN\nDinner\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker_No\nsmoker_Yes\nday_Sat\nday_Sun\nday_missing\ntime_Dinner\nsize\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.5\n\n\n\n\n\n\n\n\n# Define categorical pipeline\ncat_pipe = Pipeline([\n    ('selector', ColumnSelector(categorical)),\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Define numerical pipeline\nnum_pipe = Pipeline([\n    ('selector', ColumnSelector(numerical)),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\n# Combine categorical and numerical pipeline\npreprocessor = FeatureUnion([\n    ('cat', cat_pipe),\n    ('num', num_pipe)\n])\n\n# Combine categorical and numerical pipeline\npipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', LinearRegression())\n])\npipe.fit(X_train, y_train)\n\n# Predict training data\ny_train_pred = pipe.predict(X_train)\nprint(f\"Predictions on training data: {y_train_pred}\")\n\n# Predict test data\ny_test_pred = pipe.predict(X_test)\nprint(f\"Predictions on test data: {y_test_pred}\")\n\nPredictions on training data: [10.63 18.35 38.07 30.46]\nPredictions on test data: [18.35]"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html",
    "href": "posts/2022-11-27-eda/eda.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Reference:\n# Import packages\nimport pandas as pd\nfrom ydata_profiling import ProfileReport  # pandas_profiling -&gt; ydata_profiling\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import helpers # custom module\n# Update default settings\n# pd.options.display.float_format = '{:.4f}'.format\n# sns.set(style='darkgrid', context='talk', palette='Set1')\n# Import data\nexclude = ['pclass', 'embarked', 'who', 'adult_male', 'alive',\n           'alone']\ndf = sns.load_dataset('titanic').drop(columns=exclude)"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#basic",
    "href": "posts/2022-11-27-eda/eda.html#basic",
    "title": "Exploratory data analysis",
    "section": "basic",
    "text": "basic\n\nshape\n\n# Inspect shape of the data and top rows\nprint(f\"{df.shape[0]} rows, {df.shape[1]} columns\")\n\n891 rows, 9 columns\n\n\n\n\ndisplay a few rows\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\nsibsp\nparch\nfare\nclass\ndeck\nembark_town\n\n\n\n\n0\n0\nmale\n22.0\n1\n0\n7.2500\nThird\nNaN\nSouthampton\n\n\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nFirst\nC\nCherbourg\n\n\n2\n1\nfemale\n26.0\n0\n0\n7.9250\nThird\nNaN\nSouthampton\n\n\n3\n1\nfemale\n35.0\n1\n0\n53.1000\nFirst\nC\nSouthampton\n\n\n4\n0\nmale\n35.0\n0\n0\n8.0500\nThird\nNaN\nSouthampton"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#split-train-test",
    "href": "posts/2022-11-27-eda/eda.html#split-train-test",
    "title": "Exploratory data analysis",
    "section": "split train, test",
    "text": "split train, test\n\ntarget = 'survived'\nfeatures = df.drop(columns=target).columns\nfrom sklearn.model_selection import train_test_split\n\n# Split data into train & test\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns=target), df[target], test_size=0.2, random_state=8, stratify=df[target])\n\n# Append target back using indices\ntrain = pd.concat([X_train, y_train], axis=1)\ntest = pd.concat([X_test, y_test], axis=1)"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#automation-tool",
    "href": "posts/2022-11-27-eda/eda.html#automation-tool",
    "title": "Exploratory data analysis",
    "section": "automation tool",
    "text": "automation tool\n\n# Create profile report\nprofile_train = ProfileReport(train)\ntry:\n    profile_train.to_notebook_iframe()\nexcept:\n    print(\"An error occured in automation tool\")"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#target-variable-balance",
    "href": "posts/2022-11-27-eda/eda.html#target-variable-balance",
    "title": "Exploratory data analysis",
    "section": "target variable, balance",
    "text": "target variable, balance\n\ndef find_frequency(series):\n    \"\"\"Provide summary on frequency counts and proportions.\n    Parameters\n    ----------\n    series: A pandas Series containing discrete values\n    Returns\n    -------\n    A pandas DataFrame containing frequency counts and proportions for each\n    category.\n    \"\"\"\n    columns = ['p_frequency', 'n_frequency']\n    frequency = pd.concat([series.value_counts(normalize=True),\n                           series.value_counts()], keys=columns, axis=1)\n    return frequency\n\n\n# Inspect training data\nprint(f\"Training data ({train.shape[0]} rows): Target distribution\")\ndisplay(find_frequency(y_train))\nprint(f\"Test data ({test.shape[0]} rows): Target distribution\")\ndisplay(find_frequency(y_test))\n\nTraining data (712 rows): Target distribution\nTest data (179 rows): Target distribution\n\n\n\n\n\n\n\n\n\np_frequency\nn_frequency\n\n\n\n\n0\n0.616573\n439\n\n\n1\n0.383427\n273\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_frequency\nn_frequency\n\n\n\n\n0\n0.614525\n110\n\n\n1\n0.385475\n69"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#missing-values-datatype",
    "href": "posts/2022-11-27-eda/eda.html#missing-values-datatype",
    "title": "Exploratory data analysis",
    "section": "missing values & datatype",
    "text": "missing values & datatype\n\ndef summarise(df):\n    \"\"\"Provide summary on missing values, unique values and data type.\n    Parameters\n    ----------\n    df: A pandas DataFrame to summarise\n    Returns\n    -------\n    A pandas DataFrame containing count and proportion of missing values,\n    count of unique values and data type for each column.\n    \"\"\"\n    columns = ['n_missing', 'p_missing', 'n_unique', 'dtype']\n    summary = pd.concat([df.isnull().sum(),\n                         df.isnull().mean(),\n                         df.nunique(),\n                         df.dtypes], keys=columns, axis=1)\n    return summary.sort_values(by='n_missing', ascending=False)\n\n\nsummarise(train)\n\n\n\n\n\n\n\n\nn_missing\np_missing\nn_unique\ndtype\n\n\n\n\ndeck\n560\n0.786517\n7\ncategory\n\n\nage\n150\n0.210674\n83\nfloat64\n\n\nembark_town\n1\n0.001404\n3\nobject\n\n\nsex\n0\n0.000000\n2\nobject\n\n\nsibsp\n0\n0.000000\n7\nint64\n\n\nparch\n0\n0.000000\n7\nint64\n\n\nfare\n0\n0.000000\n221\nfloat64\n\n\nclass\n0\n0.000000\n3\ncategory\n\n\nsurvived\n0\n0.000000\n2\nint64"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#duplicate",
    "href": "posts/2022-11-27-eda/eda.html#duplicate",
    "title": "Exploratory data analysis",
    "section": "duplicate",
    "text": "duplicate\n\ndf.loc[df.duplicated(), :]\n# df.drop_duplicates(inplace=True)\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\nsibsp\nparch\nfare\nclass\ndeck\nembark_town\n\n\n\n\n47\n1\nfemale\nNaN\n0\n0\n7.7500\nThird\nNaN\nQueenstown\n\n\n76\n0\nmale\nNaN\n0\n0\n7.8958\nThird\nNaN\nSouthampton\n\n\n77\n0\nmale\nNaN\n0\n0\n8.0500\nThird\nNaN\nSouthampton\n\n\n87\n0\nmale\nNaN\n0\n0\n8.0500\nThird\nNaN\nSouthampton\n\n\n95\n0\nmale\nNaN\n0\n0\n8.0500\nThird\nNaN\nSouthampton\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n870\n0\nmale\n26.0\n0\n0\n7.8958\nThird\nNaN\nSouthampton\n\n\n877\n0\nmale\n19.0\n0\n0\n7.8958\nThird\nNaN\nSouthampton\n\n\n878\n0\nmale\nNaN\n0\n0\n7.8958\nThird\nNaN\nSouthampton\n\n\n884\n0\nmale\n25.0\n0\n0\n7.0500\nThird\nNaN\nSouthampton\n\n\n886\n0\nmale\n27.0\n0\n0\n13.0000\nSecond\nNaN\nSouthampton\n\n\n\n\n107 rows × 9 columns"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#numerical-features-categorical-target",
    "href": "posts/2022-11-27-eda/eda.html#numerical-features-categorical-target",
    "title": "Exploratory data analysis",
    "section": "numerical features ~ categorical target",
    "text": "numerical features ~ categorical target\n\n# Define feature groups\ncontinuous = train[features].select_dtypes(['float']).columns\ndiscrete = train[features].columns.difference(continuous)\n# Convert to category dtype\ntrain[discrete] = train[discrete].astype('category')\n\n\nsummary statistics\n\ndef find_outlier(series, k=1.5):\n    \"\"\"Find outlier using first and third quartiles and interquartile range.\n    Parameters\n    ----------\n    series: A pandas Series to find outlier in\n    k: (optional) An integer indicating threshold of outlier in IQR from Q1/Q3\n    Returns\n    -------\n    A pandas Series containing boolean values where True indicates an outlier.\n    \"\"\"\n    q1 = series.quantile(.25)\n    q3 = series.quantile(.75)\n    iqr = q3-q1\n    lower_bound = q1 - k*iqr\n    upper_bound = q3 + k*iqr\n    is_outlier = (series&lt;lower_bound) | (series&gt;upper_bound)\n    return is_outlier\n\n\ndef describe_more(df, features, k=1.5):\n    \"\"\"Provide descriptive statistics and outlier summary for numerical features.\n    Parameters\n    ----------\n    df: A pandas DataFrame to describe\n    features: A list of numerical feature column names to use\n    k: (optional) An integer indicating threshold of outlier in IQR from Q1/Q3\n    Returns\n    -------\n    A pandas DataFrame containing descriptive statistics and outlier summary.\n    \"\"\"\n    descriptives = df[features].describe()\n    outliers = df[features].apply(find_outlier)\n    descriptives.loc['n_outliers']= outliers.sum()\n    descriptives.loc['p_outliers']= outliers.mean()\n    return descriptives\n\n\ndescribe_more(train, continuous)\n\n\n\n\n\n\n\n\nage\nfare\n\n\n\n\ncount\n562.000000\n712.000000\n\n\nmean\n29.311690\n31.047325\n\n\nstd\n14.475009\n49.241159\n\n\nmin\n0.420000\n0.000000\n\n\n25%\n20.000000\n7.895800\n\n\n50%\n28.000000\n14.427100\n\n\n75%\n38.000000\n30.017700\n\n\nmax\n80.000000\n512.329200\n\n\nn_outliers\n4.000000\n87.000000\n\n\np_outliers\n0.005618\n0.122191\n\n\n\n\n\n\n\n\n\nplot\n\n# def plot_continuous(df, feature, target, bins=30, figsize=(14, 5)):\n#     \"\"\"Plot histogram, density plot, box plot and swarm plot for feature colour\n#     coded by target.\n#     Parameters\n#     ----------\n#     df: A pandas DataFrame to use\n#     feature: A string specifying the name of the feature column\n#     target: A string specifying the name of the target column\n#     bins: (optional) An integer for number of bins in histogram\n#     figsize: (optional) A tuple specifying the shape of the plot\n#     Returns\n#     -------\n#     A plot containing 4 subplots. Top left subplot shows number of histogram.\n#     Top right subplot shows density plot. Bottom left subplot shows box plot.\n#     Bottom right subplot shows swarm plot. Each contains overlaying graphs for\n#     each class in target.\n#     \"\"\"\n#     fig, ax = plt.subplots(2, 2, figsize=(14,8))\n\n#     sns.histplot(data=df, x=feature, hue=target, bins=bins, ax=ax[0,0])\n#     ax[0,0].set_title(f'Histogram of {feature} by {target}')\n\n#     sns.kdeplot(data=df, x=feature, hue=target, fill=True, common_norm=False, ax=ax[0,1])\n#     ax[0,1].set_title(f'Density plot of {feature} by {target}')\n\n#     sns.boxplot(data=df, y=feature, x=target, ax=ax[1,0])\n#     ax[1,0].set_title(f'Box plot of {feature} by {target}')\n\n#     sns.violinplot(data=df, y=feature, x=target, ax=ax[1,1])\n#     ax[1,1].set_title(f'Violin plot of {feature} by {target}')\n#     plt.tight_layout() # To ensure subplots don't overlay\n# for feature in continuous:\n#     plot_continuous(train, feature, target)\n\n\ndef plot_continuous(df, feature, target, bins=30, figsize=(14, 5)):\n    \"\"\"Plot histogram, density plot, box plot and swarm plot for feature colour\n    coded by target.\n    Parameters\n    ----------\n    df: A pandas DataFrame to use\n    feature: A string specifying the name of the feature column\n    target: A string specifying the name of the target column\n    bins: (optional) An integer for number of bins in histogram\n    figsize: (optional) A tuple specifying the shape of the plot\n    Returns\n    -------\n    A plot containing 4 subplots. Top left subplot shows number of histogram.\n    Top right subplot shows density plot. Bottom left subplot shows box plot.\n    Bottom right subplot shows swarm plot. Each contains overlaying graphs for\n    each class in target.\n    \"\"\"\n    fig, ax = plt.subplots(2, 2, figsize=(14,8))\n\n    sns.histplot(data=df, x=feature, hue=target, bins=bins, ax=ax[0,0])\n    ax[0,0].set_title(f'Histogram of {feature} by {target}')\n\n    sns.kdeplot(data=df, x=feature, hue=target, fill=True, common_norm=False, ax=ax[0,1])\n    ax[0,1].set_title(f'Density plot of {feature} by {target}')\n\n    sns.violinplot(data=df, y=feature, x=target, ax=ax[1,0])\n    ax[1,0].set_title(f'Violin plot of {feature} by {target}')\n\n    grouped = df[[feature, target]].groupby(feature).mean().reset_index().sort_values(feature)\n    sns.lineplot(data=grouped, x=feature, y=target, ax=ax[1,1])\n    # sns.violinplot(data=df, y=feature, x=target, ax=ax[1,1])\n    ax[1,1].set_title(f'Plot of {target} rate vs. {feature}')\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\nfor feature in continuous:\n    plot_continuous(train, feature, target)"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#categorical-features",
    "href": "posts/2022-11-27-eda/eda.html#categorical-features",
    "title": "Exploratory data analysis",
    "section": "categorical features",
    "text": "categorical features\n\nsummary statistics\n\ntrain.describe(exclude=['number'])\n\n\n\n\n\n\n\n\nsex\nsibsp\nparch\nclass\ndeck\nembark_town\n\n\n\n\ncount\n712\n712\n712\n712\n152\n711\n\n\nunique\n2\n7\n7\n3\n7\n3\n\n\ntop\nmale\n0\n0\nThird\nC\nSouthampton\n\n\nfreq\n467\n486\n548\n399\n46\n514\n\n\n\n\n\n\n\n\n\nplot\n\n# def plot_discrete(df, feature, target, orientation='v', figsize=(14, 4)):\n#     \"\"\"Plot target mean and counts for unique values in feature.\n#     Parameters\n#     ----------\n#     df: A pandas DataFrame to use\n#     feature: A string specifying the name of the feature column\n#     target: A string specifying the name of the target column\n#     orientation: (optional) 'h' for horizontal and 'v' for  orientation of bars\n#     figsize: (optional) A tuple specifying the shape of the plot\n#     Returns\n#     -------\n#     A plot containing 2 subplots. Left subplot shows counts of categories. Right\n#     subplot shows target mean value for each category.\n#     \"\"\"\n#     fig, ax = plt.subplots(1, 2, figsize=figsize)\n#     if orientation=='v':\n#         sns.countplot(data=df, x=feature, ax=ax[0])\n#         sns.barplot(data=df, x=feature, y=target, ax=ax[1])\n#         ax[1].set_ylim([0,1])\n\n#     elif orientation=='h':\n#         sns.countplot(data=df, y=feature, ax=ax[0])\n#         sns.barplot(data=df, x=target, y=feature, orient='h', ax=ax[1])\n#         ax[1].set_xlim([0,1])\n\n#     ax[0].set_title(f\"Category counts in {feature}\")\n#     ax[1].set_title(f\"Mean target by category in {feature}\")\n#     plt.tight_layout() # To ensure subplots don't overlay\n# # Fill missing\n# for feature in discrete:\n#     if train[feature].isnull().sum()&gt;0:\n#         train[feature] = train[feature].cat.add_categories('missing')\n#         train[feature] = train[feature].fillna('missing')\n# # Visualise\n# for feature in discrete:\n#     plot_discrete(train, feature, target)\n\n\ndef plot_discrete(df, feature, target, orientation='v', figsize=(14, 4)):\n    \"\"\"Plot target mean and counts for unique values in feature.\n    Parameters\n    ----------\n    df: A pandas DataFrame to use\n    feature: A string specifying the name of the feature column\n    target: A string specifying the name of the target column\n    orientation: (optional) 'h' for horizontal and 'v' for  orientation of bars\n    figsize: (optional) A tuple specifying the shape of the plot\n    Returns\n    -------\n    A plot containing 2 subplots. Left subplot shows counts of categories. Right\n    subplot shows target mean value for each category.\n    \"\"\"\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    if orientation=='v':\n        sns.countplot(data=df, x=feature, ax=ax[0])\n        sns.countplot(data=df, x=feature, hue=target, ax=ax[1])\n        sns.barplot(data=df, x=feature, y=target, ax=ax[2])\n        ax[2].set_ylim([0,1])\n\n    elif orientation=='h':\n        sns.countplot(data=df, y=feature, ax=ax[0])\n        sns.countplot(data=df, y=feature, hue=target, ax=ax[1])\n        sns.barplot(data=df, x=target, y=feature, orient='h', ax=ax[2])\n        ax[2].set_xlim([0,1])\n\n    ax[0].set_title(f\"Category counts in {feature}\")\n    ax[1].set_title(f\"Category counts in {feature} by {target}\")\n    ax[2].set_title(f\"Mean target by category in {feature}\")\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\n# Fill missing\nfor feature in discrete:\n    if train[feature].isnull().sum()&gt;0:\n        train[feature] = train[feature].cat.add_categories('missing')\n        train[feature] = train[feature].fillna('missing')\n# Visualise\nfor feature in discrete:\n    plot_discrete(train, feature, target)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Visualise\nfor feature in discrete:\n    plot_discrete(train, feature, target, orientation='h')"
  },
  {
    "objectID": "posts/2022-11-27-eda/eda.html#correlation",
    "href": "posts/2022-11-27-eda/eda.html#correlation",
    "title": "Exploratory data analysis",
    "section": "correlation",
    "text": "correlation\n\ncorrs = train.corr()\ncorrs\n\n/var/folders/28/qnfs_5ld2rsdgtf5g7l_vwl80000gn/T/ipykernel_21269/1394483245.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corrs = train.corr()\n\n\n\n\n\n\n\n\n\nage\nfare\nsurvived\n\n\n\n\nage\n1.000000\n0.094913\n-0.076475\n\n\nfare\n0.094913\n1.000000\n0.237252\n\n\nsurvived\n-0.076475\n0.237252\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(corrs, annot=True)\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html",
    "title": "Data challenge - 12. loan grant",
    "section": "",
    "text": "choosing if granting a loan “loan_table” - general information about the loan\ncreate column “gain”, assign value predict “gain”, -1, 1, 0 sum up, get profit\npredict grant loan or not predict get repaid or not?"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#create-target-variable-gain",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#create-target-variable-gain",
    "title": "Data challenge - 12. loan grant",
    "section": "create target variable, gain",
    "text": "create target variable, gain\n\ndef get_gain(loan_granted, loan_repaid):\n    if loan_granted == 1 and loan_repaid == 0:\n        return -1\n    if loan_granted == 1 and loan_repaid == 1:\n        return 1\n    if loan_granted == 0:\n        return 0\n\n\nloan['gain'] = loan.apply(lambda x: get_gain(x['loan_granted'], x['loan_repaid']), axis=1)\n\n\nloan.head()\n\n\n\n\n\n\n\n\nloan_id\nloan_purpose\ndate\nloan_granted\nloan_repaid\ngain\n\n\n\n\n0\n19454\ninvestment\n2012-03-15\n0\nNaN\n0\n\n\n1\n496811\ninvestment\n2012-01-17\n0\nNaN\n0\n\n\n2\n929493\nother\n2012-02-09\n0\nNaN\n0\n\n\n3\n580653\nother\n2012-06-27\n1\n1.0\n1\n\n\n4\n172419\nbusiness\n2012-05-21\n1\n0.0\n-1\n\n\n\n\n\n\n\n\nloan.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 101100 entries, 0 to 101099\nData columns (total 6 columns):\n #   Column        Non-Null Count   Dtype  \n---  ------        --------------   -----  \n 0   loan_id       101100 non-null  int64  \n 1   loan_purpose  101100 non-null  object \n 2   date          101100 non-null  object \n 3   loan_granted  101100 non-null  int64  \n 4   loan_repaid   47654 non-null   float64\n 5   gain          101100 non-null  int64  \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 4.6+ MB\n\n\n\nborrower = pd.read_csv(\"borrower_table.csv\")\n\nborrower.head()\n\n\n\n\n\n\n\n\nloan_id\nis_first_loan\nfully_repaid_previous_loans\ncurrently_repaying_other_loans\ntotal_credit_card_limit\navg_percentage_credit_card_limit_used_last_year\nsaving_amount\nchecking_amount\nis_employed\nyearly_salary\nage\ndependent_number\n\n\n\n\n0\n289774\n1\nNaN\nNaN\n8000\n0.49\n3285\n1073\n0\n0\n47\n3\n\n\n1\n482590\n0\n1.0\n0.0\n4500\n1.03\n636\n5299\n1\n13500\n33\n1\n\n\n2\n135565\n1\nNaN\nNaN\n6900\n0.82\n2085\n3422\n1\n24500\n38\n8\n\n\n3\n207797\n0\n1.0\n0.0\n1200\n0.82\n358\n3388\n0\n0\n24\n1\n\n\n4\n828078\n0\n0.0\n0.0\n6900\n0.80\n2138\n4282\n1\n18100\n36\n1\n\n\n\n\n\n\n\n\nborrower.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 101100 entries, 0 to 101099\nData columns (total 12 columns):\n #   Column                                           Non-Null Count   Dtype  \n---  ------                                           --------------   -----  \n 0   loan_id                                          101100 non-null  int64  \n 1   is_first_loan                                    101100 non-null  int64  \n 2   fully_repaid_previous_loans                      46153 non-null   float64\n 3   currently_repaying_other_loans                   46153 non-null   float64\n 4   total_credit_card_limit                          101100 non-null  int64  \n 5   avg_percentage_credit_card_limit_used_last_year  94128 non-null   float64\n 6   saving_amount                                    101100 non-null  int64  \n 7   checking_amount                                  101100 non-null  int64  \n 8   is_employed                                      101100 non-null  int64  \n 9   yearly_salary                                    101100 non-null  int64  \n 10  age                                              101100 non-null  int64  \n 11  dependent_number                                 101100 non-null  int64  \ndtypes: float64(3), int64(9)\nmemory usage: 9.3 MB"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#merge-data",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#merge-data",
    "title": "Data challenge - 12. loan grant",
    "section": "merge data",
    "text": "merge data\n\ndata_all = pd.merge(loan, borrower, how='left', on='loan_id')\n\ndata_all.head()\n\n\n\n\n\n\n\n\nloan_id\nloan_purpose\ndate\nloan_granted\nloan_repaid\ngain\nis_first_loan\nfully_repaid_previous_loans\ncurrently_repaying_other_loans\ntotal_credit_card_limit\navg_percentage_credit_card_limit_used_last_year\nsaving_amount\nchecking_amount\nis_employed\nyearly_salary\nage\ndependent_number\n\n\n\n\n0\n19454\ninvestment\n2012-03-15\n0\nNaN\n0\n1\nNaN\nNaN\n8600\n0.79\n1491\n6285\n1\n45200\n42\n7\n\n\n1\n496811\ninvestment\n2012-01-17\n0\nNaN\n0\n1\nNaN\nNaN\n5300\n0.52\n141\n5793\n0\n0\n42\n5\n\n\n2\n929493\nother\n2012-02-09\n0\nNaN\n0\n1\nNaN\nNaN\n0\nNaN\n660\n3232\n1\n26500\n60\n4\n\n\n3\n580653\nother\n2012-06-27\n1\n1.0\n1\n0\n1.0\n0.0\n5400\n0.52\n3345\n2764\n1\n15800\n58\n4\n\n\n4\n172419\nbusiness\n2012-05-21\n1\n0.0\n-1\n1\nNaN\nNaN\n2900\n0.76\n1050\n3695\n1\n34800\n31\n4"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#split-into-training-and-testing",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#split-into-training-and-testing",
    "title": "Data challenge - 12. loan grant",
    "section": "split into training and testing",
    "text": "split into training and testing\n\ndata = data_all.sample(frac=0.8, axis=0, random_state=10)\ntest_data = data_all.drop(index=data.index)"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#loan_purpose",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#loan_purpose",
    "title": "Data challenge - 12. loan grant",
    "section": "loan_purpose",
    "text": "loan_purpose\n\nsns.countplot(data, y='loan_purpose', hue='gain')\n\n&lt;AxesSubplot: xlabel='count', ylabel='loan_purpose'&gt;\n\n\n\n\n\nmost loan are not granted grant loan and get repaid is usually more than grant loan but not get repaid, except for emergency funds\n\n# x,y = 'loan_purpose', 'gain'\n#\n# (data\n# .groupby(x)[y]\n# .value_counts(normalize=True)\n# .mul(100)\n# .rename('percent')\n# .reset_index()\n# .pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#date",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#date",
    "title": "Data challenge - 12. loan grant",
    "section": "date",
    "text": "date\n\ndata['month'] = pd.to_datetime(data['date']).dt.month#data['date'].apply(lambda x: x.month)\ndata['week'] = pd.to_datetime(data['date']).dt.isocalendar().week # data['date'].apply(lambda x: x.week)\ndata['dayofweek'] = pd.to_datetime(data['date']).dt.isocalendar().day#data['date'].apply(lambda x: x.dayofweek)\n\n\nsns.countplot(data, y='month', hue='gain')\n\n&lt;AxesSubplot: xlabel='count', ylabel='month'&gt;\n\n\n\n\n\n\nsns.countplot(data, y='dayofweek')\n\n&lt;AxesSubplot: xlabel='count', ylabel='dayofweek'&gt;\n\n\n\n\n\nless loan for later day of week\n\nsns.countplot(data, y='dayofweek', hue='gain')\n\n&lt;AxesSubplot: xlabel='count', ylabel='dayofweek'&gt;"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#is-first-loan",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#is-first-loan",
    "title": "Data challenge - 12. loan grant",
    "section": "is first loan",
    "text": "is first loan\n\nsns.countplot(data, y='is_first_loan', hue='gain')\n\n&lt;AxesSubplot: xlabel='count', ylabel='is_first_loan'&gt;"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#fully_repaid_previous_loans",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#fully_repaid_previous_loans",
    "title": "Data challenge - 12. loan grant",
    "section": "fully_repaid_previous_loans",
    "text": "fully_repaid_previous_loans\n\nsns.countplot(data, y='fully_repaid_previous_loans', hue='gain')\n\n&lt;AxesSubplot: xlabel='count', ylabel='fully_repaid_previous_loans'&gt;"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#is_first_loan-fully_repaid_previous_loans-currently_repaying_other_loans",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#is_first_loan-fully_repaid_previous_loans-currently_repaying_other_loans",
    "title": "Data challenge - 12. loan grant",
    "section": "is_first_loan, fully_repaid_previous_loans, currently_repaying_other_loans",
    "text": "is_first_loan, fully_repaid_previous_loans, currently_repaying_other_loans\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 80880 entries, 57304 to 2811\nData columns (total 20 columns):\n #   Column                                           Non-Null Count  Dtype  \n---  ------                                           --------------  -----  \n 0   loan_id                                          80880 non-null  int64  \n 1   loan_purpose                                     80880 non-null  object \n 2   date                                             80880 non-null  object \n 3   loan_granted                                     80880 non-null  int64  \n 4   loan_repaid                                      38194 non-null  float64\n 5   gain                                             80880 non-null  int64  \n 6   is_first_loan                                    80880 non-null  int64  \n 7   fully_repaid_previous_loans                      36923 non-null  float64\n 8   currently_repaying_other_loans                   36923 non-null  float64\n 9   total_credit_card_limit                          80880 non-null  int64  \n 10  avg_percentage_credit_card_limit_used_last_year  75306 non-null  float64\n 11  saving_amount                                    80880 non-null  int64  \n 12  checking_amount                                  80880 non-null  int64  \n 13  is_employed                                      80880 non-null  int64  \n 14  yearly_salary                                    80880 non-null  int64  \n 15  age                                              80880 non-null  int64  \n 16  dependent_number                                 80880 non-null  int64  \n 17  month                                            80880 non-null  int64  \n 18  week                                             80880 non-null  UInt32 \n 19  dayofweek                                        80880 non-null  UInt32 \ndtypes: UInt32(2), float64(4), int64(12), object(2)\nmemory usage: 12.5+ MB\n\n\n\n# data.loc[data.is_first_loan == 1, ['is_first_loan', 'fully_repaid_previous_loans', 'currently_repaying_other_loans']]\ndata.loc[data.is_first_loan == 1, ['is_first_loan', 'fully_repaid_previous_loans', 'currently_repaying_other_loans']].isna().sum()\n\nis_first_loan                         0\nfully_repaid_previous_loans       43957\ncurrently_repaying_other_loans    43957\ndtype: int64\n\n\nis_first_loan == 1, fully_repaid_previous_loans = nan, currently_repaying_other_loans = nan, replace with -1 remove is_first_loan, since the information is included in fully_repaid_previous_loans, currently_repaying_other_loans"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#avg_percentage_credit_card_limit_used_last_year",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#avg_percentage_credit_card_limit_used_last_year",
    "title": "Data challenge - 12. loan grant",
    "section": "avg_percentage_credit_card_limit_used_last_year",
    "text": "avg_percentage_credit_card_limit_used_last_year\n\ndata['avg_percentage_credit_card_limit_used_last_year'].describe()\n\ncount    75306.000000\nmean         0.723924\nstd          0.186582\nmin          0.000000\n25%          0.600000\n50%          0.730000\n75%          0.860000\nmax          1.090000\nName: avg_percentage_credit_card_limit_used_last_year, dtype: float64\n\n\n\ndata.loc[data['avg_percentage_credit_card_limit_used_last_year'].isna()]\n\n\n\n\n\n\n\n\nloan_id\nloan_purpose\ndate\nloan_granted\nloan_repaid\ngain\nis_first_loan\nfully_repaid_previous_loans\ncurrently_repaying_other_loans\ntotal_credit_card_limit\navg_percentage_credit_card_limit_used_last_year\nsaving_amount\nchecking_amount\nis_employed\nyearly_salary\nage\ndependent_number\nmonth\nweek\ndayofweek\n\n\n\n\n84928\n310919\nother\n2012-05-11\n0\nNaN\n0\n0\n1.0\n1.0\n0\nNaN\n502\n650\n1\n12500\n48\n4\n5\n19\n5\n\n\n60675\n964829\nother\n2012-04-09\n0\nNaN\n0\n0\n0.0\n1.0\n0\nNaN\n683\n3689\n1\n31000\n42\n2\n4\n15\n1\n\n\n5250\n928209\nhome\n2012-07-03\n0\nNaN\n0\n1\nNaN\nNaN\n0\nNaN\n1150\n974\n0\n0\n44\n7\n7\n27\n2\n\n\n80527\n325709\nhome\n2012-08-14\n0\nNaN\n0\n0\n0.0\n1.0\n0\nNaN\n301\n1520\n1\n22100\n25\n5\n8\n33\n2\n\n\n28423\n116927\nhome\n2012-10-18\n0\nNaN\n0\n1\nNaN\nNaN\n0\nNaN\n1789\n911\n1\n26500\n42\n7\n10\n42\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95879\n525503\ninvestment\n2012-02-08\n1\n0.0\n-1\n0\n1.0\n1.0\n0\nNaN\n1358\n2166\n0\n0\n26\n2\n2\n6\n3\n\n\n93197\n422490\nemergency_funds\n2012-04-18\n0\nNaN\n0\n0\n1.0\n0.0\n0\nNaN\n3551\n5156\n1\n49800\n53\n3\n4\n16\n3\n\n\n35949\n661231\nother\n2012-01-20\n0\nNaN\n0\n1\nNaN\nNaN\n0\nNaN\n334\n1552\n1\n11500\n39\n8\n1\n3\n5\n\n\n21727\n832295\nemergency_funds\n2012-05-14\n1\n0.0\n-1\n0\n1.0\n0.0\n0\nNaN\n1173\n907\n1\n21000\n50\n1\n5\n20\n1\n\n\n21420\n328268\nother\n2012-03-29\n0\nNaN\n0\n1\nNaN\nNaN\n0\nNaN\n437\n852\n1\n23500\n35\n8\n3\n13\n4\n\n\n\n\n5574 rows × 20 columns\n\n\n\navg_percentage_credit_card_limit_used_last_year, fill with median"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#is_employed-yearly_salary",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#is_employed-yearly_salary",
    "title": "Data challenge - 12. loan grant",
    "section": "is_employed, yearly_salary",
    "text": "is_employed, yearly_salary\n\ndata.loc[data['is_employed'] == 0, ['is_employed', 'yearly_salary']]\n\n\n\n\n\n\n\n\nis_employed\nyearly_salary\n\n\n\n\n40116\n0\n0\n\n\n69039\n0\n0\n\n\n73516\n0\n0\n\n\n33303\n0\n0\n\n\n78700\n0\n0\n\n\n...\n...\n...\n\n\n54894\n0\n0\n\n\n89617\n0\n0\n\n\n22325\n0\n0\n\n\n100211\n0\n0\n\n\n2811\n0\n0\n\n\n\n\n27549 rows × 2 columns\n\n\n\n\ndata.loc[data['is_employed'] == 0, ['is_employed', 'yearly_salary']].value_counts()\n\nis_employed  yearly_salary\n0            0                27549\ndtype: int64\n\n\nif is_employed == 0, yearly_salary = 0, so yearly_salary contains the information in is_employed drop is_employed\nfeature engineering plan\nimpute is_first_loan == 1, fully_repaid_previous_loans = nan, currently_repaying_other_loans = nan, replace with -1 avg_percentage_credit_card_limit_used_last_year, fill with median\ndrop is_first_loan (since the information is included in fully_repaid_previous_loans, currently_repaying_other_loans) is_employed date loan_granted loan_repaid loan_id week\nordinal encoder\n\ndata = data.fillna({'fully_repaid_previous_loans': -1, 'currently_repaying_other_loans': -1})\nmedian = data['avg_percentage_credit_card_limit_used_last_year'].median()\ndata = data.fillna({'avg_percentage_credit_card_limit_used_last_year': median})\n\n\ndata.head()\n\n\n\n\n\n\n\n\nloan_id\nloan_purpose\ndate\nloan_granted\nloan_repaid\ngain\nis_first_loan\nfully_repaid_previous_loans\ncurrently_repaying_other_loans\ntotal_credit_card_limit\navg_percentage_credit_card_limit_used_last_year\nsaving_amount\nchecking_amount\nis_employed\nyearly_salary\nage\ndependent_number\nmonth\nweek\ndayofweek\n\n\n\n\n57304\n950973\nhome\n2012-08-27\n0\nNaN\n0\n1\n-1.0\n-1.0\n4100\n0.70\n1476\n2251\n1\n11900\n59\n8\n8\n35\n1\n\n\n40116\n105727\nother\n2012-09-12\n0\nNaN\n0\n0\n1.0\n0.0\n5000\n0.85\n1468\n1733\n0\n0\n41\n2\n9\n37\n3\n\n\n64446\n886954\nhome\n2012-12-20\n1\n0.0\n-1\n0\n1.0\n1.0\n4100\n0.89\n1008\n3742\n1\n28700\n29\n2\n12\n51\n4\n\n\n69039\n925979\nother\n2012-01-11\n0\nNaN\n0\n0\n1.0\n0.0\n2500\n0.96\n1304\n3369\n0\n0\n20\n6\n1\n2\n3\n\n\n52924\n199955\nother\n2012-06-04\n1\n1.0\n1\n0\n1.0\n0.0\n2700\n0.75\n180\n6149\n1\n38500\n31\n3\n6\n23\n1\n\n\n\n\n\n\n\n\ndata = data.drop(columns=['is_first_loan', 'is_employed', 'date', 'loan_granted', 'loan_repaid', 'loan_id', 'week'])\n\n\ndata.head()\n\n\n\n\n\n\n\n\nloan_purpose\ngain\nfully_repaid_previous_loans\ncurrently_repaying_other_loans\ntotal_credit_card_limit\navg_percentage_credit_card_limit_used_last_year\nsaving_amount\nchecking_amount\nyearly_salary\nage\ndependent_number\nmonth\ndayofweek\n\n\n\n\n57304\nhome\n0\n-1.0\n-1.0\n4100\n0.70\n1476\n2251\n11900\n59\n8\n8\n1\n\n\n40116\nother\n0\n1.0\n0.0\n5000\n0.85\n1468\n1733\n0\n41\n2\n9\n3\n\n\n64446\nhome\n-1\n1.0\n1.0\n4100\n0.89\n1008\n3742\n28700\n29\n2\n12\n4\n\n\n69039\nother\n0\n1.0\n0.0\n2500\n0.96\n1304\n3369\n0\n20\n6\n1\n3\n\n\n52924\nother\n1\n1.0\n0.0\n2700\n0.75\n180\n6149\n38500\n31\n3\n6\n1"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#split-data-into-training-and-validation",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#split-data-into-training-and-validation",
    "title": "Data challenge - 12. loan grant",
    "section": "split data into training and validation",
    "text": "split data into training and validation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nX_train, X_val, y_train, y_val = train_test_split(data.drop(columns=['gain']), data['gain'], test_size=0.2, random_state=42)\ncategorical_features = ['loan_purpose', 'fully_repaid_previous_loans', 'currently_repaying_other_loans', 'month', 'dayofweek']\nct = make_column_transformer((OrdinalEncoder(), categorical_features), remainder='passthrough')\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_purpose\nfully_repaid_previous_loans\ncurrently_repaying_other_loans\ntotal_credit_card_limit\navg_percentage_credit_card_limit_used_last_year\nsaving_amount\nchecking_amount\nyearly_salary\nage\ndependent_number\nmonth\ndayofweek\n\n\n\n\n11535\nother\n1.0\n1.0\n4600\n0.71\n1295\n2924\n31000\n33\n2\n8\n4\n\n\n51647\nother\n-1.0\n-1.0\n2000\n0.95\n973\n1383\n14700\n35\n4\n5\n4\n\n\n13251\nbusiness\n-1.0\n-1.0\n5100\n0.41\n3090\n5405\n45700\n57\n1\n3\n4\n\n\n76583\nhome\n-1.0\n-1.0\n5000\n0.95\n2058\n4185\n16200\n24\n3\n4\n2\n\n\n83111\ninvestment\n-1.0\n-1.0\n5900\n0.91\n3445\n3525\n37600\n47\n1\n1\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50477\nbusiness\n1.0\n0.0\n5400\n0.64\n4826\n7067\n50400\n38\n2\n6\n4\n\n\n60829\ninvestment\n-1.0\n-1.0\n4700\n0.92\n3054\n4403\n0\n43\n4\n10\n5\n\n\n38691\ninvestment\n1.0\n1.0\n6200\n0.59\n2922\n4128\n31900\n30\n7\n3\n3\n\n\n69438\nemergency_funds\n-1.0\n-1.0\n2700\n0.73\n1651\n1506\n38900\n34\n8\n1\n4\n\n\n66586\ninvestment\n1.0\n0.0\n6700\n0.48\n5107\n8021\n32500\n30\n8\n3\n2\n\n\n\n\n64704 rows × 12 columns\n\n\n\n\nX_train = ct.fit_transform(X_train)\n\n\nct.feature_names_in_\n\narray(['loan_purpose', 'fully_repaid_previous_loans',\n       'currently_repaying_other_loans', 'total_credit_card_limit',\n       'avg_percentage_credit_card_limit_used_last_year', 'saving_amount',\n       'checking_amount', 'yearly_salary', 'age', 'dependent_number',\n       'month', 'dayofweek'], dtype=object)\n\n\n\nX_train\n\narray([[4.00e+00, 2.00e+00, 2.00e+00, ..., 3.10e+04, 3.30e+01, 2.00e+00],\n       [4.00e+00, 0.00e+00, 0.00e+00, ..., 1.47e+04, 3.50e+01, 4.00e+00],\n       [0.00e+00, 0.00e+00, 0.00e+00, ..., 4.57e+04, 5.70e+01, 1.00e+00],\n       ...,\n       [3.00e+00, 2.00e+00, 2.00e+00, ..., 3.19e+04, 3.00e+01, 7.00e+00],\n       [1.00e+00, 0.00e+00, 0.00e+00, ..., 3.89e+04, 3.40e+01, 8.00e+00],\n       [3.00e+00, 2.00e+00, 1.00e+00, ..., 3.25e+04, 3.00e+01, 8.00e+00]])"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#random-forest",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#random-forest",
    "title": "Data challenge - 12. loan grant",
    "section": "random forest",
    "text": "random forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_jobs=-1, random_state=10)\n\n\nclf.fit(X_train, y_train)\n\nRandomForestClassifier(n_jobs=-1, random_state=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=10)\n\n\nclassify == -1, not loan to these people we can also change threshold\n\nclf.classes_\n\narray([-1,  0,  1])\n\n\n\nclf.predict_proba(X_train)\n\narray([[0.87, 0.08, 0.05],\n       [0.26, 0.74, 0.  ],\n       [0.  , 0.74, 0.26],\n       ...,\n       [0.01, 0.71, 0.28],\n       [0.81, 0.15, 0.04],\n       [0.  , 0.09, 0.91]])\n\n\n\n# bank model\ny_train.sum()\n\n9087\n\n\n\ntruth_pred = pd.DataFrame({'truth': y_train, 'pred': clf.predict(X_train)})\n\ntruth_pred.head()\n\n\n\n\n\n\n\n\ntruth\npred\n\n\n\n\n11535\n-1\n-1\n\n\n51647\n0\n0\n\n\n13251\n0\n0\n\n\n76583\n0\n0\n\n\n83111\n1\n1\n\n\n\n\n\n\n\n\n# our model\ntruth_pred.loc[truth_pred['pred'] == 1, 'truth'].sum()  # only loan to prediction == 1, sum up the truth is our model profit\n\n19772\n\n\n\ntruth_pred = pd.DataFrame({'truth': y_val, 'pred': clf.predict(ct.transform(X_val))})\n\ntruth_pred.head()\n\n\n\n\n\n\n\n\ntruth\npred\n\n\n\n\n61611\n1\n1\n\n\n51495\n0\n0\n\n\n57379\n-1\n-1\n\n\n1992\n0\n1\n\n\n17623\n-1\n-1\n\n\n\n\n\n\n\n\n# bank model\ny_val.sum()\n\n2109\n\n\n\n# our model\ntruth_pred.loc[truth_pred['pred'] == 1, 'truth'].sum()  # only loan to prediction == 1, sum up the truth is our model profit\n\n4109\n\n\n\ntruth_pred.loc[truth_pred['pred'] == 1, 'truth'].sum() / y_val.sum()\n\n1.948316737790422\n\n\n1.95 times profit"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#performance-on-testing-data",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#performance-on-testing-data",
    "title": "Data challenge - 12. loan grant",
    "section": "performance on testing data",
    "text": "performance on testing data\n\ndata = test_data\n\ndata['month'] = pd.to_datetime(data['date']).dt.month#data['date'].apply(lambda x: x.month)\ndata['week'] = pd.to_datetime(data['date']).dt.isocalendar().week # data['date'].apply(lambda x: x.week)\ndata['dayofweek'] = pd.to_datetime(data['date']).dt.isocalendar().day#data['date'].apply(lambda x: x.dayofweek)\n\ndata = data.drop(columns=['is_first_loan', 'is_employed', 'date', 'loan_granted', 'loan_repaid', 'loan_id', 'week'])\n\ndata = data.fillna({'fully_repaid_previous_loans': -1, 'currently_repaying_other_loans': -1})\n# median = data['avg_percentage_credit_card_limit_used_last_year'].median()\ndata = data.fillna({'avg_percentage_credit_card_limit_used_last_year': median})\n\n\nX_test = data.drop(columns=['gain'])\ny_test = data['gain']\n\n\ntruth_pred = pd.DataFrame({'truth': y_test, 'pred': clf.predict(ct.transform(X_test))})\n\ntruth_pred.head()\n\n\n\n\n\n\n\n\ntruth\npred\n\n\n\n\n2\n0\n0\n\n\n10\n0\n0\n\n\n19\n0\n0\n\n\n21\n0\n0\n\n\n24\n0\n0\n\n\n\n\n\n\n\n\n# bank model\nsum(y_test)\n\n2562\n\n\n\n# our model\ntruth_pred.loc[truth_pred['pred'] == 1, 'truth'].sum()  # only loan to prediction == 1, sum up the truth is our model profit\n\n5012\n\n\n\n# our model\nsum(y_test[clf.predict(ct.transform(X_test)) == 1])\n# todo: change previous\n\n5012\n\n\n\ntruth_pred.loc[truth_pred['pred'] == 1, 'truth'].sum() / sum(y_test)\n\n1.9562841530054644\n\n\nour model is about 2 times profit of bank model"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#question-3",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#question-3",
    "title": "Data challenge - 12. loan grant",
    "section": "question 3",
    "text": "question 3\naccording to the plot, the most important variable is dependent number.\nas mentioned, “is_employed” is merged with feature “yearly_salary”, where is_employed == 0, yearly_salary == 0 correspondingly. according to plot, yearly_salary is an important feature.\n\nif yearly_salary == 0, not likely to be granted loan\nif employed, high salary indicates high repay\n\n\nsns.countplot(data, y='dependent_number', hue='gain')\n\n&lt;AxesSubplot: xlabel='count', ylabel='dependent_number'&gt;\n\n\n\n\n\n\nsns.histplot(data, x='yearly_salary', hue='gain')\n\n&lt;AxesSubplot: xlabel='yearly_salary', ylabel='Count'&gt;\n\n\n\n\n\n\ndata.columns\n\nIndex(['loan_purpose', 'gain', 'fully_repaid_previous_loans',\n       'currently_repaying_other_loans', 'total_credit_card_limit',\n       'avg_percentage_credit_card_limit_used_last_year', 'saving_amount',\n       'checking_amount', 'yearly_salary', 'age', 'dependent_number', 'month',\n       'dayofweek'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#question-4",
    "href": "posts/2022-12-07-data12-loan-grant/loan_grant_data12.html#question-4",
    "title": "Data challenge - 12. loan grant",
    "section": "question 4",
    "text": "question 4\n\nmarriage status\n\nmore stable, spouse helps paying\n\nhousing status\n\nown house? rent house?\nif cannot repay, can sell house to pay\n\neducation level\n\nhigher education, possibly more income to repay"
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html",
    "title": "Data challenge - Expedia 2022",
    "section": "",
    "text": "Points:"
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#data-description",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#data-description",
    "title": "Data challenge - Expedia 2022",
    "section": "Data Description",
    "text": "Data Description\n\n\n\nColumn\nDescription\n\n\n\n\nid\nIdentification number for the flight.\n\n\ndep_stn\nDeparture point.\n\n\narr_stn\nArrival point.\n\n\nac_code\nAircraft Code.\n\n\ndep_date\nScheduled departure date.\n\n\narr_date\nScheduled arrival date.\n\n\nweather\nObserved weather conditions at departure.\n\n\nhobbs_meter\nThe time in hours that an aircraft has been in use.\n\n\nyear_man\nYear of manufacturer for the aircraft.\n\n\ndep_country\nDeparture country.\n\n\narr_country\nDestination country.\n\n\ndelay\nNumber of minutes a flight was delayed."
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#data-wrangling-visualization",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#data-wrangling-visualization",
    "title": "Data challenge - Expedia 2022",
    "section": "Data Wrangling & Visualization",
    "text": "Data Wrangling & Visualization\n\n# Dataset is already loaded below\ndata = pd.read_csv(\"train.csv\")\n\n\ndata.head()\n\n\n\n\n\n\n\n\nid\ndep_stn\narr_stn\nac_code\ndep_date\narr_date\nweather\nhobbs_meter\nyear_man\ndep_country\narr_country\ndelay\n\n\n\n\n0\nTU 0930\nTUN\nMRS\nTU 32AIMD\n2016-01-01 07:55:00\n2016-01-01 09.30.00\nsunny\n4390\n2004-08\nTunisia\nFrance\n18.0\n\n\n1\nTU 0526\nTUN\nDUS\nTU 736IOQ\n2016-01-01 08:20:00\n2016-01-01 11.05.00\nrainy\n5882\n2010-01\nTunisia\nGermany\n39.0\n\n\n2\nTU 0718\nTUN\nORY\nTU 320IMU\n2016-01-01 10:05:00\n2016-01-01 12.25.00\nfoggy\n6117\n2011-03\nTunisia\nFrance\n14.0\n\n\n3\nTU 0997\nNCE\nTUN\nTU 320IMT\n2016-01-01 10:15:00\n2016-01-01 11.50.00\nsunny\n8941\n2010-01\nFrance\nTunisia\n25.0\n\n\n4\nTU 0700\nTUN\nGVA\nTU 320IMV\n2016-01-01 12:40:00\n2016-01-01 14.35.00\nstormy\n4301\n2004-08\nTunisia\nSwitzerland\n21.0\n\n\n\n\n\n\n\n\n#Explore columns\ndata.columns\n\nIndex(['id', 'dep_stn', 'arr_stn', 'ac_code', 'dep_date', 'arr_date',\n       'weather', 'hobbs_meter', 'year_man', 'dep_country', 'arr_country',\n       'delay'],\n      dtype='object')\n\n\n\n#Description\ndata.describe()\n\n\n\n\n\n\n\n\nhobbs_meter\ndelay\n\n\n\n\ncount\n5000.000000\n5000.00000\n\n\nmean\n6186.797000\n20.51060\n\n\nstd\n1693.004034\n12.27381\n\n\nmin\n3329.000000\n2.00000\n\n\n25%\n4501.000000\n10.00000\n\n\n50%\n6099.000000\n18.00000\n\n\n75%\n7437.000000\n30.00000\n\n\nmax\n9137.000000\n49.00000\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   id           5000 non-null   object \n 1   dep_stn      5000 non-null   object \n 2   arr_stn      5000 non-null   object \n 3   ac_code      5000 non-null   object \n 4   dep_date     5000 non-null   object \n 5   arr_date     5000 non-null   object \n 6   weather      5000 non-null   object \n 7   hobbs_meter  5000 non-null   int64  \n 8   year_man     5000 non-null   object \n 9   dep_country  5000 non-null   object \n 10  arr_country  5000 non-null   object \n 11  delay        5000 non-null   float64\ndtypes: float64(1), int64(1), object(10)\nmemory usage: 468.9+ KB\n\n\n\nNo missing values\n\n\ndata.describe(exclude=[np.number])\n\n\n\n\n\n\n\n\nid\ndep_stn\narr_stn\nac_code\ndep_date\narr_date\nweather\nyear_man\ndep_country\narr_country\n\n\n\n\ncount\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n\n\nunique\n422\n70\n68\n47\n4865\n4881\n6\n9\n35\n33\n\n\ntop\nTU 0744\nTUN\nTUN\nTU 320IMV\n2017-01-05 10:30:00\n2016-09-05 09.10.00\nrainy\n2004-08\nTunisia\nTunisia\n\n\nfreq\n90\n2072\n1963\n256\n3\n3\n851\n1426\n2471\n2478"
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#target",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#target",
    "title": "Data challenge - Expedia 2022",
    "section": "Target",
    "text": "Target\n\ntarget = 'delay'\ndata[target].hist()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nThe target distribution is left skewed. I will transform the target variable."
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#categorical-features",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#categorical-features",
    "title": "Data challenge - Expedia 2022",
    "section": "Categorical features",
    "text": "Categorical features\n\ndef plot_discrete(df, feature, target, bins=30, figsize=(14, 4)):\n    fig, ax = plt.subplots(2, 2, figsize=(14,8))\n\n    sns.histplot(data=df, x=target, hue=feature, bins=bins, ax=ax[0,0])\n    ax[0,0].set_title(f'Histogram of {feature} by {target}')\n\n    sns.kdeplot(data=df, x=target, hue=feature, fill=True, common_norm=False, ax=ax[0,1])\n    ax[0,1].set_title(f'Density plot of {feature} by {target}')\n\n    sns.boxplot(data=df, y=target, x=feature, ax=ax[1,0])\n    ax[1,0].set_title(f'Box plot of {feature} by {target}')\n\n    sns.violinplot(data=df, y=target, x=feature, ax=ax[1,1])\n    ax[1,1].set_title(f'Violin plot of {feature} by {target}')\n    plt.tight_layout()\n\n\ndata.columns\n\nIndex(['id', 'dep_stn', 'arr_stn', 'ac_code', 'dep_date', 'arr_date',\n       'weather', 'hobbs_meter', 'year_man', 'dep_country', 'arr_country',\n       'delay', 'dep_date_year', 'dep_date_month', 'dep_date_dayofweek',\n       'dep_date_hour', 'arr_date_year', 'arr_date_month',\n       'arr_date_dayofweek', 'arr_date_hour', 'fly_minutes'],\n      dtype='object')\n\n\n\nplot_discrete(data, 'weather', target)\n\n\n\n\n\nNo obvious difference for different weathers.\n\n\nsns.boxplot(data=data, y=target, x='arr_date_dayofweek')\n\n&lt;AxesSubplot:xlabel='arr_date_dayofweek', ylabel='delay'&gt;\n\n\n\n\n\n\nsns.boxplot(data=data, y=target, x='arr_date_month')\n\n&lt;AxesSubplot:xlabel='arr_date_month', ylabel='delay'&gt;\n\n\n\n\n\n\nNo obvious patterns found"
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#numerical-features",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#numerical-features",
    "title": "Data challenge - Expedia 2022",
    "section": "Numerical features",
    "text": "Numerical features\n\nsns.lineplot(data=data, x='fly_minutes', y=target)\n\n&lt;AxesSubplot:xlabel='fly_minutes', ylabel='delay'&gt;\n\n\n\n\n\n\nsns.lineplot(data=data, x='hobbs_meter', y=target)\n\n&lt;AxesSubplot:xlabel='hobbs_meter', ylabel='delay'&gt;"
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#visualization-modeling-machine-learning",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#visualization-modeling-machine-learning",
    "title": "Data challenge - Expedia 2022",
    "section": "Visualization, Modeling, Machine Learning",
    "text": "Visualization, Modeling, Machine Learning\nBuild a regression model to predict how many minutes a flight will be delayed. Please explain the findings effectively to technical and non-technical audiences using comments and visualizations, if appropriate. - Build an optimized model that effectively solves the business problem. - The model’s performance will be evaluated on mean absolute percent error. - Read the test.csv file and prepare features for testing.\n\ncategorical_features = ['dep_stn', 'arr_stn', 'weather', 'dep_date_month', \n                        'dep_date_dayofweek', 'dep_date_hour']\n\nnumerical_features = ['hobbs_meter', 'fly_minutes']\n\n# target = 'delay'\n\n\nfrom sklearn.model_selection import train_test_split\n\ndf = data[categorical_features + numerical_features + [target]]\nX_train, X_val, y_train, y_val = train_test_split(df.drop(columns=[target]), df[target], random_state=42)\n\n\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.preprocessing import QuantileTransformer\n\n\ndef evaluate_model(pipe, X_train, y_train, X_val, y_val):\n    print(f'mape train = {mean_absolute_percentage_error(pipe.predict(X_train), y_train)}')\n    print(f'mape val = {mean_absolute_percentage_error(pipe.predict(X_val), y_val)}')\n\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_features)], remainder = 'passthrough', verbose_feature_names_out=False)\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1, random_state=0), \n                                         transformer=QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")))\n])\n\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nmape train = 0.17945977149388548\nmape val = 0.6083875022929235\n\n\n\nThe model is overtrained. I will tune hyperparameters.\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [25, 50, 100] # number of trees in the random forest\nmax_depth = [5, 7, 9]#[int(x) for x in np.linspace(1, 19, num = 10)] # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10] # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\n\nrandom_grid = {'model__regressor__n_estimators': n_estimators,\n               'model__regressor__max_depth': max_depth,\n               'model__regressor__min_samples_split': min_samples_split,\n               'model__regressor__min_samples_leaf': min_samples_leaf}\n\n\nrandom_search = RandomizedSearchCV(pipe, random_grid, n_iter=20, n_jobs=-1)\nrandom_search.fit(X_train, y_train)\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['dep_stn',\n                                                                                'arr_stn',\n                                                                                'weather',\n                                                                                'dep_date_month',\n                                                                                'dep_date_dayofweek',\n                                                                                'dep_date_hour'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('mod...\n                                              TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                                                                         random_state=0),\n                                                                         transformer=QuantileTransformer(n_quantiles=100,\n                                                                                                         output_distribution='normal')))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__regressor__max_depth': [5, 7,\n                                                                        9],\n                                        'model__regressor__min_samples_leaf': [1,\n                                                                               3,\n                                                                               4],\n                                        'model__regressor__min_samples_split': [2,\n                                                                                6,\n                                                                                10],\n                                        'model__regressor__n_estimators': [25,\n                                                                           50,\n                                                                           100]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['dep_stn',\n                                                                                'arr_stn',\n                                                                                'weather',\n                                                                                'dep_date_month',\n                                                                                'dep_date_dayofweek',\n                                                                                'dep_date_hour'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('mod...\n                                              TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                                                                         random_state=0),\n                                                                         transformer=QuantileTransformer(n_quantiles=100,\n                                                                                                         output_distribution='normal')))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__regressor__max_depth': [5, 7,\n                                                                        9],\n                                        'model__regressor__min_samples_leaf': [1,\n                                                                               3,\n                                                                               4],\n                                        'model__regressor__min_samples_split': [2,\n                                                                                6,\n                                                                                10],\n                                        'model__regressor__n_estimators': [25,\n                                                                           50,\n                                                                           100]})estimator: PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['dep_stn', 'arr_stn',\n                                                   'weather', 'dep_date_month',\n                                                   'dep_date_dayofweek',\n                                                   'dep_date_hour'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                                            random_state=0),\n                                            transformer=QuantileTransformer(n_quantiles=100,\n                                                                            output_distribution='normal')))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['dep_stn', 'arr_stn', 'weather',\n                                  'dep_date_month', 'dep_date_dayofweek',\n                                  'dep_date_hour'])],\n                  verbose_feature_names_out=False)cat['dep_stn', 'arr_stn', 'weather', 'dep_date_month', 'dep_date_dayofweek', 'dep_date_hour']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['hobbs_meter', 'fly_minutes']passthroughpassthroughmodel: TransformedTargetRegressorTransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                           random_state=0),\n                           transformer=QuantileTransformer(n_quantiles=100,\n                                                           output_distribution='normal'))regressor: RandomForestRegressorRandomForestRegressor(n_jobs=-1, random_state=0)RandomForestRegressorRandomForestRegressor(n_jobs=-1, random_state=0)transformer: QuantileTransformerQuantileTransformer(n_quantiles=100, output_distribution='normal')QuantileTransformerQuantileTransformer(n_quantiles=100, output_distribution='normal')\n\n\n\nprint(random_search.best_params_)\nmodel_pipe = random_search.best_estimator_\nevaluate_model(model_pipe, X_train, y_train, X_val, y_val)\n\n{'model__regressor__n_estimators': 100, 'model__regressor__min_samples_split': 6, 'model__regressor__min_samples_leaf': 3, 'model__regressor__max_depth': 7}\nmape train = 0.493856735702503\nmape val = 0.5615795113972523\n\n\n\nThe model improved some. If I have more time, I will do a more through search.\n\nDescribe the most important features of the model to management.\n\nTask:\n\nVisualize the top 10 features and their feature importance."
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#feature-importance",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#feature-importance",
    "title": "Data challenge - Expedia 2022",
    "section": "Feature importance",
    "text": "Feature importance\n\n### Gini importance\n\n\nfeature_names = model_pipe[:-1].get_feature_names_out()\nimportances = model_pipe[-1].regressor_.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_pipe[-1].regressor_.estimators_], axis=0)\n\nfeature_importances = pd.Series(importances, index=feature_names)\n\nfeature_importances\n\ndep_stn               0.119596\narr_stn               0.111263\nweather               0.073223\ndep_date_month        0.137528\ndep_date_dayofweek    0.089303\ndep_date_hour         0.201855\nhobbs_meter           0.133635\nfly_minutes           0.133596\ndtype: float64\n\n\n\nfig, ax = plt.subplots(figsize=(5, 8))\n\nfeature_importances.plot.barh(xerr=std, ax=ax)\nax.set_title(\"Feature importances using Gini impurity\")\nax.set_ylabel(\"Mean decrease in impurity\")\nplt.show()\n\n\n\n\n\nPermutation importance\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    model_pipe, X_val, y_val, scoring='neg_mean_absolute_percentage_error', n_repeats=30, random_state=42, n_jobs=-1\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\n\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=feature_names\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (validation set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in score\")\nax.figure.tight_layout()"
  },
  {
    "objectID": "posts/2023-02-20-data-expedia/Expedia oa.html#partial-dependence",
    "href": "posts/2023-02-20-data-expedia/Expedia oa.html#partial-dependence",
    "title": "Data challenge - Expedia 2022",
    "section": "Partial dependence",
    "text": "Partial dependence\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": -1,\n    \"random_state\": 0,\n}\n\nfeatures_info = {\n    # features of interest\n    \"features\": ['dep_stn', 'arr_stn', 'weather', 'dep_date_month', 'dep_date_dayofweek', 'dep_date_hour', \n                 'hobbs_meter', 'fly_minutes'],\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": ['dep_stn', 'arr_stn', 'weather', 'dep_date_month', 'dep_date_dayofweek', 'dep_date_hour']\n}\n\n_, ax = plt.subplots(ncols=4, nrows=2, figsize=(16, 8), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(\n    model_pipe,\n    X_val,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n\n_ = display.figure_.suptitle(\n    \"Partial dependence of the delay\\n\"\n    \"for the random forest\",\n    fontsize=16,\n)\n\n\n\n\n\nMost important features are\nfly minutes, too short or too long probably delay more.\nhobbs_metter, small hobbs meter seems have more delay.\ndep hour, 10 AM - 11:59 PM, and 1 AM have more delay.\ndep day of week, weekends have more delay\n\n\nTask:\n\nSubmit the predictions on the test dataset using the optimized model  For each record in the test set (test.csv), predict how many minutes a flight will be delayed. Submit a CSV file with a header row and one row per test entry.\n\n\nThe file (submissions.csv) should have exactly 2 columns: - id - delay\n\n#Loading Test data\ntest_data=pd.read_csv('test.csv')\ntest_data.head()\n\n\n\n\n\n\n\n\nid\ndep_stn\narr_stn\nac_code\ndep_date\narr_date\nweather\nhobbs_meter\nyear_man\ndep_country\narr_country\n\n\n\n\n0\nTU 0216\nTUN\nIST\nTU 320IMV\n2017-11-08 16:00:00\n2017-11-08 18.35.00\nstormy\n4301\n2004-08\nTunisia\nTurkey\n\n\n1\nTU 0543\nMUC\nTUN\nTU 320IMU\n2017-11-08 16:05:00\n2017-11-08 18.10.00\nstormy\n6117\n2011-03\nGermany\nTunisia\n\n\n2\nTU 0527\nDUS\nTUN\nTU 32AIMH\n2017-11-08 16:20:00\n2017-11-08 18.55.00\nfoggy\n7437\n2011-03\nGermany\nTunisia\n\n\n3\nUG 0011\nDJE\nTUN\nTU 320IMT\n2017-11-08 19:00:00\n2017-11-08 19.50.00\nfoggy\n8941\n2010-01\nTunisia\nTunisia\n\n\n4\nTU 0214\nTUN\nIST\nTU 320IMT\n2017-11-09 06:20:00\n2017-11-09 08.55.00\nfoggy\n8941\n2010-01\nTunisia\nTurkey\n\n\n\n\n\n\n\n\ntest_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   id           3000 non-null   object\n 1   dep_stn      3000 non-null   object\n 2   arr_stn      3000 non-null   object\n 3   ac_code      3000 non-null   object\n 4   dep_date     3000 non-null   object\n 5   arr_date     3000 non-null   object\n 6   weather      3000 non-null   object\n 7   hobbs_meter  3000 non-null   int64 \n 8   year_man     3000 non-null   object\n 9   dep_country  3000 non-null   object\n 10  arr_country  3000 non-null   object\ndtypes: int64(1), object(10)\nmemory usage: 257.9+ KB\n\n\n\ndata = test_data\n\ndata['dep_date_year'] = pd.to_datetime(data.dep_date).dt.year\ndata['dep_date_month'] = pd.to_datetime(data.dep_date).dt.month\ndata['dep_date_dayofweek'] = pd.to_datetime(data.dep_date).dt.dayofweek\ndata['dep_date_hour'] = pd.to_datetime(data.dep_date).dt.hour\ndata.arr_date = data.arr_date.str.replace('.', ':', regex=True)\ndata['fly_minutes'] = (pd.to_datetime(data.arr_date) - pd.to_datetime(data.dep_date)).dt.total_seconds() / 60\n\n\ndf_test = data[categorical_features + numerical_features]\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\ndep_stn\narr_stn\nweather\ndep_date_month\ndep_date_dayofweek\ndep_date_hour\nhobbs_meter\nfly_minutes\n\n\n\n\n0\nTUN\nIST\nstormy\n11\n2\n16\n4301\n155.0\n\n\n1\nMUC\nTUN\nstormy\n11\n2\n16\n6117\n125.0\n\n\n2\nDUS\nTUN\nfoggy\n11\n2\n16\n7437\n155.0\n\n\n3\nDJE\nTUN\nfoggy\n11\n2\n19\n8941\n50.0\n\n\n4\nTUN\nIST\nfoggy\n11\n3\n6\n8941\n155.0\n\n\n\n\n\n\n\n\n# retrain model with all the data\nmodel_pipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(\n        regressor=RandomForestRegressor(n_jobs=-1, \n                                        random_state=0,\n                                        n_estimators=100,\n                                        min_samples_split=10,\n                                        min_samples_leaf=3,\n                                        max_depth=7), \n        transformer=QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")))\n])\n\nmodel_pipe.fit(df.drop(columns=[target]), df[target])\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['dep_stn', 'arr_stn',\n                                                   'weather', 'dep_date_month',\n                                                   'dep_date_dayofweek',\n                                                   'dep_date_hour'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 TransformedTargetRegressor(regressor=RandomForestRegressor(max_depth=7,\n                                                                            min_samples_leaf=3,\n                                                                            min_samples_split=10,\n                                                                            n_jobs=-1,\n                                                                            random_state=0),\n                                            transformer=QuantileTransformer(n_quantiles=100,\n                                                                            output_distribution='normal')))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['dep_stn', 'arr_stn',\n                                                   'weather', 'dep_date_month',\n                                                   'dep_date_dayofweek',\n                                                   'dep_date_hour'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 TransformedTargetRegressor(regressor=RandomForestRegressor(max_depth=7,\n                                                                            min_samples_leaf=3,\n                                                                            min_samples_split=10,\n                                                                            n_jobs=-1,\n                                                                            random_state=0),\n                                            transformer=QuantileTransformer(n_quantiles=100,\n                                                                            output_distribution='normal')))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['dep_stn', 'arr_stn', 'weather',\n                                  'dep_date_month', 'dep_date_dayofweek',\n                                  'dep_date_hour'])],\n                  verbose_feature_names_out=False)cat['dep_stn', 'arr_stn', 'weather', 'dep_date_month', 'dep_date_dayofweek', 'dep_date_hour']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['hobbs_meter', 'fly_minutes']passthroughpassthroughmodel: TransformedTargetRegressorTransformedTargetRegressor(regressor=RandomForestRegressor(max_depth=7,\n                                                           min_samples_leaf=3,\n                                                           min_samples_split=10,\n                                                           n_jobs=-1,\n                                                           random_state=0),\n                           transformer=QuantileTransformer(n_quantiles=100,\n                                                           output_distribution='normal'))regressor: RandomForestRegressorRandomForestRegressor(max_depth=7, min_samples_leaf=3, min_samples_split=10,\n                      n_jobs=-1, random_state=0)RandomForestRegressorRandomForestRegressor(max_depth=7, min_samples_leaf=3, min_samples_split=10,\n                      n_jobs=-1, random_state=0)transformer: QuantileTransformerQuantileTransformer(n_quantiles=100, output_distribution='normal')QuantileTransformerQuantileTransformer(n_quantiles=100, output_distribution='normal')\n\n\n\ntest_pred = model_pipe.predict(df_test)\n\n\nsubmission_df = pd.read_csv('sample_submission.csv')\n\nsubmission_df.head()\n\n\n\n\n\n\n\n\nid\ndelay\n\n\n\n\n0\nTU 0216\n1\n\n\n1\nTU 0543\n3\n\n\n2\nTU 0527\n7\n\n\n3\nUG 0011\n49\n\n\n4\nTU 0214\n35\n\n\n\n\n\n\n\n\nsubmission_df['delay'] = test_pred\n\nsubmission_df.head()\n\n\n\n\n\n\n\n\nid\ndelay\n\n\n\n\n0\nTU 0216\n18.000000\n\n\n1\nTU 0543\n17.000000\n\n\n2\nTU 0527\n17.000000\n\n\n3\nUG 0011\n16.993423\n\n\n4\nTU 0214\n13.000000\n\n\n\n\n\n\n\n\n#Submission\nsubmission_df.to_csv('submissions.csv',index=False)"
  },
  {
    "objectID": "posts/2022-08-04-pass-by-object-reference/index.html",
    "href": "posts/2022-08-04-pass-by-object-reference/index.html",
    "title": "Pass by object reference",
    "section": "",
    "text": "Everying in python is object, variable = value (assign value to variable) means bind/point name to an object.\nTwo types of object: - mutable object - Can be changed in place - list, dict - class - immutable object - Cannot be changed in place, change it means creating a new object - number, string, tuple\nNot pass by value or pass by reference, but pass by object reference\nExamples from https://www.geeksforgeeks.org/is-python-call-by-reference-or-call-by-value/, https://www.geeksforgeeks.org/pass-by-reference-vs-value-in-python/\n\n# call by value\n\nstring = \"Geeks\"\n\ndef test(string):\n    \n    string = \"GeeksforGeeks\"\n    print(\"Inside Function:\", string)\n\n# Driver's code\ntest(string)\nprint(\"Outside Function:\", string)\n# pass immutable object, not change value of the immutable object, but create a new object\n# once leave the scope of function test, string = \"GeeksforGeeks\" is no longer in the name space\n\nInside Function: GeeksforGeeks\nOutside Function: Geeks\n\n\n\n# call by reference\n\ndef add_more(list):\n    list.append(50)\n    print(\"Inside Function\", list)\n\n# Driver's code\nmylist = [10,20,30,40]\n\nadd_more(mylist)\nprint(\"Outside Function:\", mylist)\n# pass mutable object, change value of the mutable object in place\n\nInside Function [10, 20, 30, 40, 50]\nOutside Function: [10, 20, 30, 40, 50]\n\n\n\na = \"first\"\nb = \"first\"\n\n\n# Returns the actual location where the variable is stored\nprint(id(a))\n\n# Returns the actual location where the variable is stored\nprint(id(b))\n\n# Returns true if both the variables are stored in same location\nprint(a is b)\n\n139861259092848\n139861259092848\nTrue\n\n\n\na = [10, 20, 30]\nb = [10, 20, 30]\n\n# return the location where the variable is stored\nprint(id(a))\n\n# return the location where the variable is stored\nprint(id(b))\n\n# returns false if the location is not same\nprint(a is b)\n\n139860915120800\n139860915120480\nFalse"
  },
  {
    "objectID": "posts/2021-08-13-pca/index.html",
    "href": "posts/2021-08-13-pca/index.html",
    "title": "Principal component analysis",
    "section": "",
    "text": "Notations\nDataset\n\\[\n\\{ x^{(i)}, i = 1...n \\}, x^{(i)} \\in \\mathbb{R}^p\n\\]\n\\[\n\\begin{bmatrix}\nx_1^{(1)}&x_2^{(1)}&...&x_p^{(1)}\\\\x_1^{(2)}&x_2^{(2)}&...&x_p^{(2)}\\\\...&...&...&...\\\\x_1^{(n)}&x_2^{(n)}&...&x_p^{(n)}\\end{bmatrix}\n=X\\in \\mathbb{R}^{n\\times p}\n\\]\n\\(n\\), number of examples\n\\(p\\), number of features\n\\[u \\in \\mathbb{R}^p\\]\n\\[\\Sigma \\in \\mathbb{R}^{p \\times p}\\]\n\n\nData preprocessing: normalization\n\\[\n\\mu_j=\\frac1n \\sum_{i=1}^{n} X^{(i)}_j\n\\]\n\\[\n\\sigma_j=\\frac1n \\sum_{i=1}^{n}(X^{(i)}_j-\\mu_j)^2\n\\]\n\\[\nX^{(i)}_j=\\frac{X^{(i)}_j-\\mu_j}{\\sigma_j}\n\\]\n\n\n\n\n\nProblem: dataset is parameters of car, \\(x_1\\) is speed in \\(km/h\\), \\(x_2\\) is speed in \\(mile/h\\), there are redundant information\nGoal: reduce redundancy -&gt; compute the major axis of variation -&gt; find unit vector u, when projecting data onto u, the variation of projected data is maximized\n\n\nPCA\n\\[\\begin{align*}\n&\\max_u \\quad \\frac1n \\sum_{i=1}^{n}(x^{(i)^T}u)^2\\\\\n& \\begin{array}{r@{\\quad}r@{}l@{\\quad}l}\ns.t.& u^Tu=1\n\\end{array}\n\\end{align*}\\]\nHere\n\\[\n  \\begin{aligned}\n    \\frac1n \\sum_{i=1}^{n}(x^{(i)^T}u)^2 &= \\frac1n \\sum_{i=1}^{n}(x^{(i)^T}u)^T(x^{(i)^T}u) \\\\\n         &= \\frac1n \\sum_{i=1}^{n} u^Tx^{(i)}x^{(i)^T}u \\\\\n         &= u^T(\\frac1n \\sum_{i=1}^{n} x^{(i)}x^{(i)^T})u \\\\\n         &= u^T \\Sigma u \\\\\n  \\end{aligned}\n\\]\nwhere\n\\[\n  \\begin{aligned}\n    \\Sigma &= \\frac1n \\sum_{i=1}^{n} x^{(i)}x^{(i)^T} \\\\\n           &= \\frac1n X^TX \\\\\n  \\end{aligned}\n\\]\nSo, rewrite the problem\n\\[\n\\begin{align*}\n&\\max_u \\quad f(u)=u^T \\Sigma u\\\\\n& \\begin{array}{r@{\\quad}r@{}l@{\\quad}l}\ns.t.& g(u)=u^Tu-1=0\n\\end{array}\n\\end{align*}\n\\]\nLagrange multiplier\n\\[\n  \\begin{aligned}\n    L(u, \\lambda) &= f(u)-\\lambda g(u) \\\\\n           &= u^T \\Sigma u - \\lambda (u^Tu-1) \\\\\n  \\end{aligned}\n\\]\nTake gradient\n\\[\n  \\begin{aligned}\n    0 = \\nabla_uL &= \\nabla_u(u^T \\Sigma u)-\\lambda \\nabla_u (u^Tu-1) \\\\\n           &= 2 \\Sigma u - \\lambda 2 u\\\\\n  \\end{aligned}\n\\]\nThis is \\[\\Sigma u=\\lambda u\\]\nSo the problem is to find eigenvalues \\(\\lambda\\), eigenvectors \\(u\\)\n\\(u_i\\)s are principal components (PC), \\(i=1...p\\)\nBorrow the notations from MATLAB, define\n\\[\n\\operatorname{coeff}=\\left[\\begin{array}{ccc}\n\\mid & & \\mid \\\\\nu_{1} & \\ldots & u_{p} \\\\\n\\mid & & \\mid\n\\end{array}\\right] \\in \\mathbb{R}^{p \\times p}\n\\]\n\\[\n\\text { score }=\\left[\\begin{array}{c}\ny^{(1)} \\\\\n\\vdots \\\\\ny^{(n)}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times p}\n\\]\n\n\nMap data to eigenspace\n\\[\ny^{(i)}=\\left[\\begin{array}{c}\nu_{1}^{T} x^{(i)} \\\\\n\\vdots \\\\\nu_{p}^{T} x^{(i)}\n\\end{array}\\right]=\\operatorname{coeff} f^{T} x^{(i)}\n\\]\nOnly keep first \\(k\\) PCs\nVectorization\n\\[\n\\text { score }=X^{n \\times p} \\text { coeff } ^{p \\times p} \\in \\mathbb{R}^{n \\times p}\n\\]\n\n\nReconstruct data from \\(k\\) PCs\n\\[\nz^{(i)}=\\text{coeff} y^{(i)}=\\text{coeff}\\text{coeff}^Tx^{(i)} \\in \\mathbb{R}^{p}\n\\]\nVectorization \\[\nZ^{T^{p \\times n}}=\\text{coeff}\\text{coeff}^TX^T\n\\]\nOnly keep \\(k\\) PCs, we just use the first \\(k\\) columns of \\(\\text{coeff}\\) \\[\nZ^{n \\times k}=X^{n \\times p}\\text{coeff}^{p \\times k}\\text{coeff}^T\n\\]"
  },
  {
    "objectID": "posts/2022-12-07-data19-subscription/subscription19.html",
    "href": "posts/2022-12-07-data19-subscription/subscription19.html",
    "title": "Data challenge - 19. subscription retention",
    "section": "",
    "text": "model subscription retention rate\nhelp executives understand how the subscription model is doing.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = pd.read_csv('subscription.csv')\ndata.head()\n\n\n\n\n\n\n\n\nuser_id\nsubscription_signup_date\nsubscription_monthly_cost\ncountry\nsource\nbilling_cycles\nis_active\n\n\n\n\n0\n1459\nJanuary, 2015\n29\nSpain\nads\n4\n0\n\n\n1\n12474\nJanuary, 2015\n49\nFrance\nads\n5\n0\n\n\n2\n12294\nJanuary, 2015\n49\nGermany\nads\n2\n0\n\n\n3\n3878\nJanuary, 2015\n49\nChina\nads\n1\n0\n\n\n4\n9567\nJanuary, 2015\n49\nUK\nads\n5\n0\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500000 entries, 0 to 499999\nData columns (total 7 columns):\n #   Column                     Non-Null Count   Dtype \n---  ------                     --------------   ----- \n 0   user_id                    500000 non-null  int64 \n 1   subscription_signup_date   500000 non-null  object\n 2   subscription_monthly_cost  500000 non-null  int64 \n 3   country                    500000 non-null  object\n 4   source                     500000 non-null  object\n 5   billing_cycles             500000 non-null  int64 \n 6   is_active                  500000 non-null  int64 \ndtypes: int64(4), object(3)\nmemory usage: 26.7+ MB"
  },
  {
    "objectID": "posts/2022-12-07-data19-subscription/subscription19.html#how-many-subscriptions-of-each-billing-cycle",
    "href": "posts/2022-12-07-data19-subscription/subscription19.html#how-many-subscriptions-of-each-billing-cycle",
    "title": "Data challenge - 19. subscription retention",
    "section": "How many subscriptions of each billing cycle?",
    "text": "How many subscriptions of each billing cycle?\n\ndata.value_counts('billing_cycles', sort=False)\n\nbilling_cycles\n1    287374\n2     79838\n3     35198\n4     19753\n5     12631\n6      8767\n7      6440\n8     49999\ndtype: int64\n\n\n\n# data.groupby(['subscription_monthly_cost', 'billing_cycles']).count()['user_id'].unstack()\n\ndef count_unsubscription(data):\n    n_subscription_list = []\n    billiing_cycles = list(range(1, 9))\n    for cycle in billiing_cycles:\n        n_subscription_list.append(len(data) - len(data.query('billing_cycles &gt;= @cycle')))\n    # create series, because only 1 row/1 column\n    return pd.Series(n_subscription_list, index=billiing_cycles)\n\n\n# data.apply(count_subscription)\ncount_unsubscription(data)\n\n1         0\n2    287374\n3    367212\n4    402410\n5    422163\n6    434794\n7    443561\n8    450001\ndtype: int64"
  },
  {
    "objectID": "posts/2022-12-07-data3-employee-retention/employee_retention3.html",
    "href": "posts/2022-12-07-data3-employee-retention/employee_retention3.html",
    "title": "Data challenge - 3. employee retention",
    "section": "",
    "text": "Understanding why and when employees are most likely to leave can lead to actions to improve employee retention as well as planning new hiring in advance.\npeople analytics or people data science\nIn this challenge, you have a data set with info about the employees and have to predict when employees are going to quit by understanding the main drivers of employee churn.\n\nimport pandas as pd\n\n\nLoad data\n\ndata = pd.read_csv('employee_rentation.csv').drop(columns=['Unnamed: 0'])\ndata['join_date'] = pd.to_datetime(data['join_date'])\ndata['quit_date'] = pd.to_datetime(data['quit_date'])\ndata.head()\n\n\n\n\n\n\n\n\nemployee_id\ncompany_id\ndept\nseniority\nsalary\njoin_date\nquit_date\n\n\n\n\n0\n13021.0\n7\ncustomer_service\n28\n89000.0\n2014-03-24\n2015-10-30\n\n\n1\n825355.0\n7\nmarketing\n20\n183000.0\n2013-04-29\n2014-04-04\n\n\n2\n927315.0\n4\nmarketing\n14\n101000.0\n2014-10-13\nNaT\n\n\n3\n662910.0\n7\ncustomer_service\n20\n115000.0\n2012-05-14\n2013-06-07\n\n\n4\n256971.0\n2\ndata_science\n23\n276000.0\n2011-10-17\n2014-08-22\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 24702 entries, 0 to 24701\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   employee_id  24702 non-null  float64       \n 1   company_id   24702 non-null  int64         \n 2   dept         24702 non-null  object        \n 3   seniority    24702 non-null  int64         \n 4   salary       24702 non-null  float64       \n 5   join_date    24702 non-null  datetime64[ns]\n 6   quit_date    13510 non-null  datetime64[ns]\ndtypes: datetime64[ns](2), float64(2), int64(2), object(1)\nmemory usage: 1.3+ MB\n\n\n\ndata.employee_id.nunique()\n\n24702\n\n\n\n\nQ1\nAssume, for each company, that the headcount starts from zero on 2011/01/23. Estimate employee headcount, for each company, on each day, from 2011/01/24 to 2015/12/13. That is, if by 2012/03/02 2000 people have joined company 1 and 1000 of them have already quit, then company headcount on 2012/03/02 for company 1 would be 1000. You should create a table with 3 columns: day, employee_headcount, company_id.\n\ndata.sort_values('join_date')['join_date']\n\n21049   2011-01-24\n17260   2011-01-24\n14668   2011-01-24\n1212    2011-01-24\n12426   2011-01-24\n           ...    \n15854   2015-12-09\n8620    2015-12-09\n24611   2015-12-09\n14533   2015-12-10\n16467   2015-12-10\nName: join_date, Length: 24702, dtype: datetime64[ns]\n\n\n\n# # group by company, for each day from 2011-01-24 to 2015-12-13, count number of employee\n#\n# def calc_headcount(data):\n#     start_date = '2011-01-24'\n#     end_date = '2015-12-13'\n#     dates = pd.date_range(start=start_date, end=end_date)\n#     headcount_list = []\n#     for date in dates:\n#         # date = dates[5]\n#\n#         n_employees = len(data.query('join_date &lt;= @date'))  # number of employees\n#         n_employees_left = len(data.query('join_date &lt;= @date and quit_date &gt;= @start_date'))\n#         # number of employees what left the company, if quit_date is not NaT, it must be later than start_date\n#         headcount = n_employees - n_employees_left\n#         headcount_list.append(headcount)\n#     return pd.Series(headcount_list, index=dates, name='day')\n# headcount = data.groupby('company_id').apply(calc_headcount)\n\n\ndef calc_headcount(data):\n    start_date = '2011-01-24'\n    end_date = '2015-12-13'\n    dates = pd.date_range(start=start_date, end=end_date)\n    headcount_list = []\n    for date in dates:\n        joined_employees = data.query('join_date &lt;= @date')  # number of employees\n        # if on date, an employee stayed, the quit date should be larger or NaT\n        headcount = (joined_employees['quit_date'] &gt; date).sum() + joined_employees['quit_date'].isnull().sum()  # stayed company\n        headcount_list.append(headcount)\n    return pd.Series(headcount_list, index=dates, name='day')\n\nheadcount = data.groupby('company_id').apply(calc_headcount)\n\n\nheadcount\n\n\n\n\n\n\n\nday\n2011-01-24\n2011-01-25\n2011-01-26\n2011-01-27\n2011-01-28\n2011-01-29\n2011-01-30\n2011-01-31\n2011-02-01\n2011-02-02\n...\n2015-12-04\n2015-12-05\n2015-12-06\n2015-12-07\n2015-12-08\n2015-12-09\n2015-12-10\n2015-12-11\n2015-12-12\n2015-12-13\n\n\ncompany_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n25\n27\n29\n29\n29\n29\n29\n59\n66\n67\n...\n3841\n3841\n3841\n3863\n3863\n3864\n3865\n3865\n3865\n3865\n\n\n2\n17\n19\n19\n19\n19\n19\n19\n31\n33\n35\n...\n1997\n1997\n1997\n2012\n2014\n2016\n2016\n2016\n2016\n2016\n\n\n3\n9\n10\n12\n12\n12\n12\n12\n19\n20\n20\n...\n1207\n1207\n1207\n1218\n1217\n1218\n1218\n1218\n1218\n1218\n\n\n4\n12\n12\n13\n13\n13\n13\n13\n18\n19\n19\n...\n906\n906\n906\n910\n910\n909\n909\n909\n909\n909\n\n\n5\n5\n5\n6\n6\n6\n6\n6\n13\n13\n13\n...\n766\n766\n766\n772\n772\n772\n772\n772\n772\n772\n\n\n6\n3\n3\n3\n3\n3\n3\n3\n10\n11\n11\n...\n576\n576\n576\n578\n579\n579\n579\n579\n579\n579\n\n\n7\n1\n1\n1\n1\n1\n1\n1\n7\n7\n7\n...\n528\n528\n528\n532\n532\n532\n532\n532\n532\n532\n\n\n8\n6\n6\n6\n6\n6\n6\n6\n11\n11\n12\n...\n463\n463\n463\n468\n468\n468\n468\n468\n468\n468\n\n\n9\n3\n3\n3\n3\n3\n3\n3\n6\n7\n7\n...\n425\n425\n425\n431\n432\n432\n432\n432\n432\n432\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n1\n1\n2\n...\n381\n381\n381\n384\n384\n384\n385\n385\n385\n385\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\n\n\n\n12 rows × 1785 columns\n\n\n\n\ndef calc_headcount(data):\n    start_date = '2011-01-24'\n    end_date = '2015-12-13'\n    dates = pd.date_range(start=start_date, end=end_date)\n    # company_id = data.name  # access a groups index value\n    headcount_list = []\n    for date in dates:\n        joined_employees = data.query('join_date &lt;= @date')  # number of employees\n        # if on date, an employee stayed, the quit date should be larger or NaT\n        headcount = (joined_employees['quit_date'] &gt; date).sum() + joined_employees['quit_date'].isnull().sum()  # stayed company\n        headcount_list.append(headcount)\n    return pd.DataFrame({'employee_headcount': headcount_list}, index=dates).rename_axis('day')\n    # return pd.DataFrame({'day': dates, 'employee_headcount': headcount_list})\n\nheadcount = data.groupby('company_id').apply(calc_headcount)\n\n\nheadcount = headcount.reset_index()\n\n\nheadcount.sort_values(['day', 'company_id']).head()\n\n\n\n\n\n\n\n\ncompany_id\nday\nemployee_headcount\n\n\n\n\n0\n1\n2011-01-24\n25\n\n\n1785\n2\n2011-01-24\n17\n\n\n3570\n3\n2011-01-24\n9\n\n\n5355\n4\n2011-01-24\n12\n\n\n7140\n5\n2011-01-24\n5\n\n\n\n\n\n\n\n\n\nQ2\nWhat are the main factors that drive employee churn? Do they make sense? Explain your findings.\nHow long can be considered churn? look at working length distribution?\n\n\nQ3\nIf you could add to this data set just one variable that could help explain employee churn, what would that be?\n\nGiven how important is salary, I would definitely love to have as a variable the salary the employee who quit was offered in the next job. Otherwise, things like: promotions or raises received during the employee tenure would be interesting.\nThe major findings are that employees quit at year anniversaries or at the beginning of the year. Both cases make sense. Even if you don’t like your current job, you often stay for 1 yr before quitting + you often get stocks after 1 yr so it makes sense to wait. Also, the beginning of the year is well known to be the best time to change job: companies are hiring more and you often want to stay until end of Dec to get the calendar year bonus.\nEmployees with low and high salaries are less likely to quit. Probably because employees with high salaries are happy there and employees with low salaries are not that marketable, so they have a hard time finding a new job.\n\nhttps://github.com/JifuZhao/DS-Take-Home/blob/master/03.%20Employee%20Retention.ipynb"
  },
  {
    "objectID": "posts/2021-09-02-rmd-concatenate-pdfs/index.html",
    "href": "posts/2021-09-02-rmd-concatenate-pdfs/index.html",
    "title": "Rmarkdown concatenate pdfs",
    "section": "",
    "text": "Rmarkdown can be used to concatenate pdfs. This can be helpful when combining rmd report with pdf. Here is a summary of the workflow:\n---\ntitle: \"MIT6_006 Algorithm\"\nauthor: \"Haoqi Wang (hw9335)\"\ndate: \"2021-09-21\"\noutput:\n  pdf_document: \n    toc: true\n  html_document: default\nheader-includes: \\usepackage{pdfpages} # add this in yaml header\n---\n\nnotes &lt;- list.files(pattern=\"^MIT6\") # list files starting with MIT6\n\nmake_name &lt;- function(note) {\n  str_c('\\\\includepdf[pages={-}]{', note, \"}\\n\")\n}\n\nmap_chr(notes, make_name) %&gt;% cat() # generate command\n\n\\newpage # create new page in rmd\n\n\\includepdf[pages={-}]{MIT6_006F11_lec01.pdf} # pages={-} means print all pages"
  },
  {
    "objectID": "posts/2022-12-01-data13-city-similarity/user_activity13.html",
    "href": "posts/2022-12-01-data13-city-similarity/user_activity13.html",
    "title": "Data challenge - 13. Json city similarities",
    "section": "",
    "text": "Reference:\n\nhttps://github.com/stasi009/TakeHomeDataChallenges/blob/master/13.CitySimilarity/city_search.ipynb\nhttps://github.com/JifuZhao/DS-Take-Home/blob/master/13.%20Json%20City%20Similarities.ipynb\n\nNotes:\n\nGenerate similarity matrix\n\nCompany XYZ is an Online Travel Agent, such as Expedia, Booking.com, etc.\nThey store their data in JSON ﬁles. Each row in the json shows all diﬀerent cities which have been searched for by a user within the same session (as well as some other info about the user). That is, if I go to company XYZ site and look for hotels in NY and SF within the same session, the corresponding JSON row will show my user id, some basic info about me and the two cities.\nYou are given the following tasks: - There was a bug in the code and one country didn’t get logged. It just shows up as an empty field (““). Can you guess which country was that? How? - For each city, find the most likely city to be also searched for within the same session. - Travel sites are browsed by two kinds of users. Users who are actually planning a trip and users who just dream about a vacation. The first ones have obviously a much higher purchasing intent. Users planning a trip often search for cities close to each other, while users who search for cities far away from each other are often just dreaming about a vacation. That is, a user searching for LA, SF and Las Vegas in the same session is much more likely to book a hotel than a user searching for NY, Paris, Kuala Lumpur (makes sense, right?). Based on this idea, come up with an algorithm that clusters sessions into two groups: high intent and low intent. Explain all assumptions you make along the way.\n\nIndex\n\nLoad data\nQuestion 1\nQuestion 2\nQuestion 3\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nLoad data\n\nimport datetime\n\n\ndef clean_json(d):\n    \"\"\"\n    clean the json data, make the data easier to be processed\n    \"\"\"\n    assert len(d['cities']) == 1\n    d['cities'] = d['cities'][0]\n\n    assert len(d['session_id']) == 1\n    d['session_id'] = d['session_id'][0]\n\n    assert len(d['unix_timestamp']) == 1\n    d['timestamp'] = datetime.datetime.utcfromtimestamp(d['unix_timestamp'][0])\n    del d['unix_timestamp']\n\n    # -------- retrieve users\n    user_dict = d['user']\n\n    assert len(user_dict) == 1\n    user_dict = user_dict[0]\n\n    assert len(user_dict) == 1\n    user_dict = user_dict[0]\n\n    d['user_id'] = user_dict['user_id']\n    d['user_country'] = user_dict['country']\n\n    del d['user']\n    return d\n\n\nimport json\n\nwith open(\"city_searches.json\",'rt') as f:\n    data = json.load(f)\n\nfor d in data:\n    clean_json(d)\n\ndata = pd.DataFrame(data)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nsession_id\ncities\ntimestamp\nuser_id\nuser_country\n\n\n\n\n0\nD258NVMV202LS\nSan Jose CA, Montreal QC\n2015-09-19 05:29:12\n5749\nFR\n\n\n1\nTDG10UKG7I4LR\nNew York NY\n2015-05-20 08:22:17\n10716\nDE\n\n\n2\nOH4ZDIGN9BLQS\nMontreal QC, Quebec QC\n2015-07-16 12:21:51\n2941\n\n\n\n3\nCWHIAYKQ7RA28\nChicago IL\n2015-05-21 13:45:08\n2164\nFR\n\n\n4\nGI8GZJAWAC80P\nToronto ON, Houston TX\n2015-09-29 19:50:26\n10493\nUS\n\n\n\n\n\n\n\n\ndata['cities'][0]\n\n'San Jose CA, Montreal QC'\n\n\n\n\nq1\nThere was a bug in the code and one country didn’t get logged. It just shows up as an empty field (““). Can you guess which country was that? How?\n\ndata['user_country'].value_counts()\n\nUS    3772\nUK    3754\nDE    3741\n      2769\nFR    2341\nIT    1843\nES    1802\nName: user_country, dtype: int64\n\n\ncheck timestamp, time difference between the countries may give some hint. Since people in same countries willl search in similar time, we can use the information to guess the country.\nextract year, month, day of week, hour from time stamp\n\ndata['year'] = data['timestamp'].dt.year\ndata['month'] = data['timestamp'].dt.month\ndata['dayofweek'] = data['timestamp'].dt.dayofweek\ndata['hour'] = data['timestamp'].dt.hour\n\n\ndata.head()\n\n\n\n\n\n\n\n\nsession_id\ncities\ntimestamp\nuser_id\nuser_country\nyear\nmonth\ndayofweek\nhour\n\n\n\n\n0\nD258NVMV202LS\nSan Jose CA, Montreal QC\n2015-09-19 05:29:12\n5749\nFR\n2015\n9\n5\n5\n\n\n1\nTDG10UKG7I4LR\nNew York NY\n2015-05-20 08:22:17\n10716\nDE\n2015\n5\n2\n8\n\n\n2\nOH4ZDIGN9BLQS\nMontreal QC, Quebec QC\n2015-07-16 12:21:51\n2941\n\n2015\n7\n3\n12\n\n\n3\nCWHIAYKQ7RA28\nChicago IL\n2015-05-21 13:45:08\n2164\nFR\n2015\n5\n3\n13\n\n\n4\nGI8GZJAWAC80P\nToronto ON, Houston TX\n2015-09-29 19:50:26\n10493\nUS\n2015\n9\n1\n19\n\n\n\n\n\n\n\n\ndata.groupby('user_country')['hour'].agg(['mean', 'median', 'std'])\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\n\n\nuser_country\n\n\n\n\n\n\n\n\n6.624774\n7.0\n3.203995\n\n\nDE\n11.092756\n11.0\n3.219792\n\n\nES\n11.052719\n11.0\n3.230201\n\n\nFR\n10.970525\n11.0\n3.200759\n\n\nIT\n11.157895\n11.0\n3.254251\n\n\nUK\n12.062067\n12.0\n3.249405\n\n\nUS\n16.600212\n18.0\n5.987105\n\n\n\n\n\n\n\n\ncountry_list = list(data['user_country'].unique())\norder = list(range(23))\n\nfig, ax = plt.subplots(nrows=len(country_list), ncols=1, figsize=(18, 12))\nfor i, country in enumerate(country_list):\n    sns.countplot(x='hour', data=data.query('user_country == @country'), order=order, ax=ax[i])\n    ax[i].set_title(country)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nfrom above plot, we can see that, “the searching time” habit of the “Missing” and US are very different from ‘UK’,‘DE’, ‘ES’, ‘FR’, ‘IT’. so ‘Missing’ country cannot be in Europe. also from the plot above, the Missing country has a time difference about 11~12 hours with US.\nbased on above two facts, I guess the Missing country is in Asia, and it must have good economic to allow people to travel aboard. Hence, I guess the Missing country may be China, Japan or South Korean.\n\n\nq2\nFor each city, find the most likely city to be also searched for within the same session.\nfor searched more than 1 cities, count the frequency\n\nsearched_cities = data['cities'].apply(lambda x: x.split(', '))\n\n\ncity_list = []\ncity_dict = {}\nindex = 0\n\nfor cities in searched_cities:\n    for city in cities:\n        if city not in city_dict:\n            city_dict[city] = index\n            index += 1\n            city_list.append(city)\n\n\ncity_dict\n\n{'San Jose CA': 0,\n 'Montreal QC': 1,\n 'New York NY': 2,\n 'Quebec QC': 3,\n 'Chicago IL': 4,\n 'Toronto ON': 5,\n 'Houston TX': 6,\n 'Los Angeles CA': 7,\n 'San Diego CA': 8,\n 'Santa Ana CA': 9,\n 'Saint Catharines-Niagara ON': 10,\n 'Edmonton AB': 11,\n 'Philadelphia PA': 12,\n 'Vancouver BC': 13,\n 'Detroit MI': 14,\n 'Phoenix AZ': 15,\n 'Calgary AB': 16,\n 'Jersey City NJ': 17,\n 'Newark NJ': 18,\n 'Columbus OH': 19,\n 'Portland OR': 20,\n 'Austin TX': 21,\n 'Boston MA': 22,\n 'Buffalo NY': 23,\n 'Anaheim CA': 24,\n 'Indianapolis IN': 25,\n 'OTTAWA ON': 26,\n 'Riverside CA': 27,\n 'Long Beach CA': 28,\n 'Toledo OH': 29,\n 'San Antonio TX': 30,\n 'Jacksonville FL': 31,\n 'Baltimore MD': 32,\n 'Hamilton ON': 33,\n 'Seattle WA': 34,\n 'Victoria BC': 35,\n 'Corpus Christi TX': 36,\n 'Miami FL': 37,\n 'Hialeah FL': 38,\n 'Saint Petersburg FL': 39,\n 'Tampa FL': 40,\n 'Glendale AZ': 41,\n 'Milwaukee WI': 42,\n 'Madison WI': 43,\n 'Dallas TX': 44,\n 'WASHINGTON DC': 45,\n 'Stockton CA': 46,\n 'San Francisco CA': 47,\n 'Oshawa ON': 48,\n 'Arlington TX': 49,\n 'Chandler AZ': 50,\n 'Louisville KY': 51,\n 'Plano TX': 52,\n 'Mesa AZ': 53,\n 'Kitchener ON': 54,\n 'Cincinnati OH': 55,\n 'Chesapeake VA': 56,\n 'Virginia Beach VA': 57,\n 'Lexington KY': 58,\n 'Fort Worth TX': 59,\n 'Sacramento CA': 60,\n 'Oakland CA': 61,\n 'Fresno CA': 62,\n 'Nashville TN': 63,\n 'Windsor ON': 64,\n 'Cleveland OH': 65,\n 'Pittsburgh PA': 66,\n 'Oklahoma City OK': 67,\n 'Bakersfield CA': 68,\n 'Fort Wayne IN': 69,\n 'Scottsdale AZ': 70,\n 'Saint Paul MN': 71,\n 'Halifax NS': 72,\n 'Wichita KS': 73,\n 'Tulsa OK': 74,\n 'London ON': 75,\n 'Tucson AZ': 76,\n 'Norfolk VA': 77,\n 'Atlanta GA': 78,\n 'Charlotte NC': 79,\n 'Raleigh NC': 80,\n 'Lincoln NE': 81,\n 'Omaha NE': 82,\n 'Kansas City MO': 83,\n 'Birmingham AL': 84,\n 'Minneapolis MN': 85,\n 'Memphis TN': 86}\n\n\n\ncity_list\n\n['San Jose CA',\n 'Montreal QC',\n 'New York NY',\n 'Quebec QC',\n 'Chicago IL',\n 'Toronto ON',\n 'Houston TX',\n 'Los Angeles CA',\n 'San Diego CA',\n 'Santa Ana CA',\n 'Saint Catharines-Niagara ON',\n 'Edmonton AB',\n 'Philadelphia PA',\n 'Vancouver BC',\n 'Detroit MI',\n 'Phoenix AZ',\n 'Calgary AB',\n 'Jersey City NJ',\n 'Newark NJ',\n 'Columbus OH',\n 'Portland OR',\n 'Austin TX',\n 'Boston MA',\n 'Buffalo NY',\n 'Anaheim CA',\n 'Indianapolis IN',\n 'OTTAWA ON',\n 'Riverside CA',\n 'Long Beach CA',\n 'Toledo OH',\n 'San Antonio TX',\n 'Jacksonville FL',\n 'Baltimore MD',\n 'Hamilton ON',\n 'Seattle WA',\n 'Victoria BC',\n 'Corpus Christi TX',\n 'Miami FL',\n 'Hialeah FL',\n 'Saint Petersburg FL',\n 'Tampa FL',\n 'Glendale AZ',\n 'Milwaukee WI',\n 'Madison WI',\n 'Dallas TX',\n 'WASHINGTON DC',\n 'Stockton CA',\n 'San Francisco CA',\n 'Oshawa ON',\n 'Arlington TX',\n 'Chandler AZ',\n 'Louisville KY',\n 'Plano TX',\n 'Mesa AZ',\n 'Kitchener ON',\n 'Cincinnati OH',\n 'Chesapeake VA',\n 'Virginia Beach VA',\n 'Lexington KY',\n 'Fort Worth TX',\n 'Sacramento CA',\n 'Oakland CA',\n 'Fresno CA',\n 'Nashville TN',\n 'Windsor ON',\n 'Cleveland OH',\n 'Pittsburgh PA',\n 'Oklahoma City OK',\n 'Bakersfield CA',\n 'Fort Wayne IN',\n 'Scottsdale AZ',\n 'Saint Paul MN',\n 'Halifax NS',\n 'Wichita KS',\n 'Tulsa OK',\n 'London ON',\n 'Tucson AZ',\n 'Norfolk VA',\n 'Atlanta GA',\n 'Charlotte NC',\n 'Raleigh NC',\n 'Lincoln NE',\n 'Omaha NE',\n 'Kansas City MO',\n 'Birmingham AL',\n 'Minneapolis MN',\n 'Memphis TN']\n\n\n\nn_cities = len(city_list)\ncity_matrix = np.zeros((n_cities, n_cities))\n\nfor cities in searched_cities:\n    if len(cities) == 1:\n        continue\n    for i in range(len(cities) - 1):\n        index1 = city_dict[cities[i]]\n        for j in range(i + 1, len(cities)):\n            index2 = city_dict[cities[j]]\n            city_matrix[index1, index2] += 1\n            city_matrix[index2, index1] += 1\n\n\ndf_similarity = pd.DataFrame(city_matrix, index=city_list, columns=city_list)\ndf_similarity.head()\n\n\n\n\n\n\n\n\nSan Jose CA\nMontreal QC\nNew York NY\nQuebec QC\nChicago IL\nToronto ON\nHouston TX\nLos Angeles CA\nSan Diego CA\nSanta Ana CA\n...\nNorfolk VA\nAtlanta GA\nCharlotte NC\nRaleigh NC\nLincoln NE\nOmaha NE\nKansas City MO\nBirmingham AL\nMinneapolis MN\nMemphis TN\n\n\n\n\nSan Jose CA\n0.0\n17.0\n40.0\n0.0\n12.0\n25.0\n7.0\n20.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nMontreal QC\n17.0\n0.0\n339.0\n111.0\n101.0\n254.0\n77.0\n154.0\n18.0\n0.0\n...\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nNew York NY\n40.0\n339.0\n0.0\n7.0\n260.0\n427.0\n172.0\n318.0\n62.0\n0.0\n...\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nQuebec QC\n0.0\n111.0\n7.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nChicago IL\n12.0\n101.0\n260.0\n0.0\n0.0\n145.0\n74.0\n116.0\n20.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n3.0\n2.0\n1.0\n0.0\n12.0\n0.0\n\n\n\n\n5 rows × 87 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\nplt.tight_layout()\nsns.heatmap(df_similarity, ax=ax)\nplt.show()\n\n\n\n\n\ndf_similarity.iloc[0].sort_values(ascending=False)\n\nOakland CA          58.0\nSan Francisco CA    51.0\nNew York NY         40.0\nStockton CA         37.0\nToronto ON          25.0\n                    ... \nSeattle WA           0.0\nHamilton ON          0.0\nBaltimore MD         0.0\nJacksonville FL      0.0\nMemphis TN           0.0\nName: San Jose CA, Length: 87, dtype: float64\n\n\n\n\nq3\nTravel sites are browsed by two kinds of users. Users who are actually planning a trip and users who just dream about a vacation. Users planning a trip often search for cities close to each other, while users who search for cities far away from each other are often just dreaming about a vacation. Based on this idea, come up with an algorithm that clusters sessions into two groups: high intent and low intent.\nUsers planning a trip:\n\ncalculate the average distance between searched cities. Use similarity matrix we got from q2 to approximate geographic distance. The similarity matrix is not exactly the distance, i.e., larger similarity, smaller distance.\nlook at distribution of distance, make two clusters\n\n\ndef find_distance(cities):\n    # deal with one row, use apply, no need to write for loop of all rows\n    cities = cities.split(', ')\n\n    if len(cities) == 1:  # if search one city, distance 0\n        return 0\n\n    sum_distance = 0\n    for i in range(len(cities) - 1):\n        city1 = cities[i]\n        for j in range(i + 1, len(cities)):\n            city2 = cities[j]\n            sum_distance += df_similarity.loc[city1, city2]\n\n    return sum_distance / len(cities)\n\ndata['distance'] = data['cities'].apply(find_distance)\n\n\nfix, ax = plt.subplots(figsize=(8, 5))\nsns.histplot(data.query('distance &gt; 0')['distance'], ax=ax)\n# plt.tight_layout()\nplt.show()\n\n\n\n\n\nfrom sklearn.cluster import KMeans\n\nX = data.query('distance &gt; 0')['distance']\n# np.array(X).shape  # (9469, )\n# np.array(X).reshape(-1, 1).shape  # (9469, 1)\nX = np.array(X).reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\n\n/Users/haoqiwang/.conda/envs/ibm_data_challenge/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n\nkmeans_result = pd.DataFrame(data={'distance': X.ravel(), 'label': kmeans.labels_})\n\n\ndef find_first_last(df):\n    return pd.DataFrame(data={'max': df['distance'].max(), 'min': df['distance'].min()}, index=['distance'])\n\n\nkmeans_result.groupby('label').apply(find_first_last)\n\n\n\n\n\n\n\n\n\nmax\nmin\n\n\nlabel\n\n\n\n\n\n\n\n0\ndistance\n154.20\n2.00\n\n\n1\ndistance\n537.75\n155.25\n\n\n\n\n\n\n\n\nfix, ax = plt.subplots(figsize=(8, 5))\nsns.histplot(x=X.ravel(), hue=kmeans.labels_, ax=ax)\nplt.show()\n\n\n\n\nFrom the clustering result, Label 0 refers to low intent and label 1 refers to high intent. The cutoff distance is around 155. If the distance is higher than the cutoff, it is classified as high intent, otherwise, low intent.\n\ncutoff_distance = 155\n\nhigh_intent = data.loc[data['distance'] &gt;= cutoff_distance, ['cities']]\nlow_intent = data.loc[data['distance'] &lt; cutoff_distance, ['cities']]\n\n\nhigh_intent.sample(10)\n\n\n\n\n\n\n\n\ncities\n\n\n\n\n1240\nNew York NY, Jersey City NJ\n\n\n5253\nVancouver BC, Los Angeles CA, New York NY\n\n\n9704\nNew York NY, Jersey City NJ, Newark NJ\n\n\n2825\nMontreal QC, New York NY\n\n\n11050\nHouston TX, Montreal QC, New York NY\n\n\n6457\nNew York NY, Newark NJ, Jersey City NJ, Philad...\n\n\n6534\nNew York NY, Boston MA, Philadelphia PA\n\n\n11451\nNew York NY, Los Angeles CA\n\n\n8667\nNew York NY, Montreal QC\n\n\n12971\nNew York NY, Baltimore MD, Newark NJ, Jersey C...\n\n\n\n\n\n\n\n\nlow_intent.head(10)\n\n\n\n\n\n\n\n\ncities\n\n\n\n\n0\nSan Jose CA, Montreal QC\n\n\n1\nNew York NY\n\n\n2\nMontreal QC, Quebec QC\n\n\n3\nChicago IL\n\n\n4\nToronto ON, Houston TX\n\n\n5\nNew York NY\n\n\n6\nLos Angeles CA\n\n\n7\nSan Diego CA, Santa Ana CA\n\n\n8\nNew York NY\n\n\n9\nToronto ON, Saint Catharines-Niagara ON"
  },
  {
    "objectID": "posts/2022-12-07-data14-shuttle-stops/shuttle_stops14.html",
    "href": "posts/2022-12-07-data14-shuttle-stops/shuttle_stops14.html",
    "title": "Data challenge - 14. shuttle stops",
    "section": "",
    "text": "figure out the optimal stops for a bus shuttle\nnot have more than 10 stops within the city\nthe most efficient way to select the bus stops is to minimize the overall walking distance between employee homes and the closest bus stop\nYou should write an algorithm that returns the best 10 stops in your opinion. Also, please explain the rationale behind the algorithm.\n\nimport pandas as pd\n\n\nbus = pd.read_csv('Bus_Stops.csv')\nbus.head()\n\n\n\n\n\n\n\n\nStreet_One\nStreet_Two\n\n\n\n\n0\nMISSION ST\nITALY AVE\n\n\n1\nMISSION ST\nNEW MONTGOMERY ST\n\n\n2\nMISSION ST\n01ST ST\n\n\n3\nMISSION ST\n20TH ST\n\n\n4\nMISSION ST\nFREMONT ST\n\n\n\n\n\n\n\n\nbus.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 119 entries, 0 to 118\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Street_One  119 non-null    object\n 1   Street_Two  119 non-null    object\ndtypes: object(2)\nmemory usage: 2.0+ KB\n\n\n\nemployee = pd.read_csv('Employee_Addresses.csv')\nemployee.head()\n\n\n\n\n\n\n\n\naddress\nemployee_id\n\n\n\n\n0\n98 Edinburgh St, San Francisco, CA 94112, USA\n206\n\n\n1\n237 Accacia St, Daly City, CA 94014, USA\n2081\n\n\n2\n1835 Folsom St, San Francisco, CA 94103, USA\n178\n\n\n3\n170 Cambridge St, San Francisco, CA 94134, USA\n50\n\n\n4\n16 Roanoke St, San Francisco, CA 94131, USA\n1863\n\n\n\n\n\n\n\n\nemployee.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2191 entries, 0 to 2190\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   address      2191 non-null   object\n 1   employee_id  2191 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 34.4+ KB\n\n\nFind out all the possible bus stops Find 10 stops, minimize all employees’ distances to 1 of closest 10 stops\n\nMonte Carlo?\n10 clusters? centroid of cluster?\n\nhttps://medium.com/@bekterra25/optimization-of-employee-shuttle-stops-in-r-29c883c59ac0"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html",
    "title": "Data challenge - 1. conversion rate",
    "section": "",
    "text": "References\nWe have data about users who hit our site: whether they converted or not as well as some of their characteristics such as their country, the marketing channel, their age, whether they are repeat users and the number of pages visited during that session (as a proxy for site activity/time spent on site).\nYour project is to: - Predict conversion rate - Come up with recommendations for the product team and the marketing team to improve conversion rate\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#remove-outlier",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#remove-outlier",
    "title": "Data challenge - 1. conversion rate",
    "section": "Remove outlier",
    "text": "Remove outlier\n\n# if we remove 2 outliers, what is age?\ndf.age[df.age &lt; 80].describe()\n\ncount    316198.000000\nmean         30.569311\nstd           8.268958\nmin          17.000000\n25%          24.000000\n50%          30.000000\n75%          36.000000\nmax          79.000000\nName: age, dtype: float64\n\n\nhere, min=17, max=79, which makes sense. since we have 316200 samples, but just 2 outliers, we can remove these 2 outliers.\n\ndf = df.loc[df.age &lt; 100, :]"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#split-into-train-and-test-data",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#split-into-train-and-test-data",
    "title": "Data challenge - 1. conversion rate",
    "section": "Split into train and test data",
    "text": "Split into train and test data\n\n# assume no outlier in testing dataset\ndata = df.sample(frac=0.8, axis=0, random_state=10)\ntest_data = df.drop(index=data.index)\ntarget = 'converted'\n\n\ndata[target].value_counts()\n\n0    244803\n1      8155\nName: converted, dtype: int64\n\n\nThe target is not balanced, so the evaluation metric should be ROC."
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#country",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#country",
    "title": "Data challenge - 1. conversion rate",
    "section": "country",
    "text": "country\n\ndef plot_discrete(df, feature, target, orientation='v', figsize=(14, 4)):\n    \"\"\"Plot target mean and counts for unique values in feature.\n    Parameters\n    ----------\n    df: A pandas DataFrame to use\n    feature: A string specifying the name of the feature column\n    target: A string specifying the name of the target column\n    orientation: (optional) 'h' for horizontal and 'v' for  orientation of bars\n    figsize: (optional) A tuple specifying the shape of the plot\n    Returns\n    -------\n    A plot containing 2 subplots. Left subplot shows counts of categories. Right\n    subplot shows target mean value for each category.\n    \"\"\"\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    if orientation=='v':\n        sns.countplot(data=df, x=feature, ax=ax[0])\n        sns.countplot(data=df, x=feature, hue=target, ax=ax[1])\n        sns.barplot(data=df, x=feature, y=target, ax=ax[2])\n        # ax[2].set_ylim([0,1])\n\n    elif orientation=='h':\n        sns.countplot(data=df, y=feature, ax=ax[0])\n        sns.countplot(data=df, y=feature, hue=target, ax=ax[1])\n        sns.barplot(data=df, x=target, y=feature, orient='h', ax=ax[2])\n        # ax[2].set_xlim([0,1])\n\n    ax[0].set_title(f\"Category counts in {feature}\")\n    ax[1].set_title(f\"Category counts in {feature} by {target}\")\n    ax[2].set_title(f\"Mean target by category in {feature}\")\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\nfeature = 'country'\nplot_discrete(data, feature, target)\n\n\n\n\n\ndata[['country', 'converted']].groupby('country').mean().reset_index().sort_values('converted', ascending=False)\n\n\n\n\n\n\n\n\ncountry\nconverted\n\n\n\n\n1\nGermany\n0.062625\n\n\n2\nUK\n0.052754\n\n\n3\nUS\n0.037673\n\n\n0\nChina\n0.001453"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#age",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#age",
    "title": "Data challenge - 1. conversion rate",
    "section": "age",
    "text": "age\n\ndef plot_continuous(df, feature, target, bins=30, figsize=(14, 5)):\n    \"\"\"Plot histogram, density plot, box plot and swarm plot for feature colour\n    coded by target.\n    Parameters\n    ----------\n    df: A pandas DataFrame to use\n    feature: A string specifying the name of the feature column\n    target: A string specifying the name of the target column\n    bins: (optional) An integer for number of bins in histogram\n    figsize: (optional) A tuple specifying the shape of the plot\n    Returns\n    -------\n    A plot containing 4 subplots. Top left subplot shows number of histogram.\n    Top right subplot shows density plot. Bottom left subplot shows box plot.\n    Bottom right subplot shows swarm plot. Each contains overlaying graphs for\n    each class in target.\n    \"\"\"\n    fig, ax = plt.subplots(2, 2, figsize=(14,8))\n\n    sns.histplot(data=df, x=feature, hue=target, bins=bins, ax=ax[0,0])\n    ax[0,0].set_title(f'Histogram of {feature} by {target}')\n\n    sns.kdeplot(data=df, x=feature, hue=target, fill=True, common_norm=False, ax=ax[0,1])\n    ax[0,1].set_title(f'Density plot of {feature} by {target}')\n\n    sns.violinplot(data=df, y=feature, x=target, ax=ax[1,0])\n    ax[1,0].set_title(f'Violin plot of {feature} by {target}')\n\n    grouped = df[[feature, target]].groupby(feature).mean().reset_index().sort_values(feature)\n    sns.lineplot(data=grouped, x=feature, y=target, ax=ax[1,1])\n    # sns.violinplot(data=df, y=feature, x=target, ax=ax[1,1])\n    ax[1,1].set_title(f'Plot of {target} rate vs. {feature}')\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\nplot_continuous(data, 'age', target)\n\n\n\n\n\ngrouped = data[['age', 'converted']].groupby('age').mean().reset_index().sort_values('age')\ngrouped\n\n\n\n\n\n\n\n\nage\nconverted\n\n\n\n\n0\n17\n0.072466\n\n\n1\n18\n0.065036\n\n\n2\n19\n0.061021\n\n\n3\n20\n0.058485\n\n\n4\n21\n0.051748\n\n\n5\n22\n0.052592\n\n\n6\n23\n0.048104\n\n\n7\n24\n0.044835\n\n\n8\n25\n0.039826\n\n\n9\n26\n0.041423\n\n\n10\n27\n0.031747\n\n\n11\n28\n0.032115\n\n\n12\n29\n0.033275\n\n\n13\n30\n0.029050\n\n\n14\n31\n0.026804\n\n\n15\n32\n0.025028\n\n\n16\n33\n0.023706\n\n\n17\n34\n0.021808\n\n\n18\n35\n0.018928\n\n\n19\n36\n0.018971\n\n\n20\n37\n0.019409\n\n\n21\n38\n0.014026\n\n\n22\n39\n0.018149\n\n\n23\n40\n0.015069\n\n\n24\n41\n0.011328\n\n\n25\n42\n0.012078\n\n\n26\n43\n0.012333\n\n\n27\n44\n0.008870\n\n\n28\n45\n0.012551\n\n\n29\n46\n0.010897\n\n\n30\n47\n0.007530\n\n\n31\n48\n0.008824\n\n\n32\n49\n0.008462\n\n\n33\n50\n0.005535\n\n\n34\n51\n0.004751\n\n\n35\n52\n0.004065\n\n\n36\n53\n0.003929\n\n\n37\n54\n0.000000\n\n\n38\n55\n0.006536\n\n\n39\n56\n0.000000\n\n\n40\n57\n0.000000\n\n\n41\n58\n0.007634\n\n\n42\n59\n0.000000\n\n\n43\n60\n0.024390\n\n\n44\n61\n0.017857\n\n\n45\n62\n0.000000\n\n\n46\n63\n0.000000\n\n\n47\n64\n0.000000\n\n\n48\n65\n0.000000\n\n\n49\n66\n0.000000\n\n\n50\n67\n0.000000\n\n\n51\n68\n0.000000\n\n\n52\n69\n0.000000\n\n\n53\n70\n0.000000\n\n\n54\n72\n0.000000\n\n\n55\n73\n0.000000\n\n\n56\n77\n0.000000\n\n\n57\n79\n0.000000"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#user-type",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#user-type",
    "title": "Data challenge - 1. conversion rate",
    "section": "user type",
    "text": "user type\n\nplot_discrete(data, 'new_user', target)\n\n\n\n\n\ndata[['new_user', 'converted']].groupby('new_user').mean().reset_index().sort_values('converted', ascending=False)\n\n\n\n\n\n\n\n\nnew_user\nconverted\n\n\n\n\n0\n0\n0.072247\n\n\n1\n1\n0.013900"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#source",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#source",
    "title": "Data challenge - 1. conversion rate",
    "section": "source",
    "text": "source\n\nplot_discrete(data, 'source', target)\n\n\n\n\n\ndata[['source', 'converted']].groupby('source').mean().reset_index().sort_values('converted', ascending=False)\n\n\n\n\n\n\n\n\nsource\nconverted\n\n\n\n\n0\nAds\n0.034833\n\n\n2\nSeo\n0.032702\n\n\n1\nDirect\n0.028070"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#total-page-visited",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#total-page-visited",
    "title": "Data challenge - 1. conversion rate",
    "section": "total page visited",
    "text": "total page visited\n\nplot_continuous(data, 'total_pages_visited', target)\n\n\n\n\n\ngrouped = data[['total_pages_visited', 'converted']].groupby('total_pages_visited').mean().reset_index().sort_values('total_pages_visited')\ngrouped\n\n\n\n\n\n\n\n\ntotal_pages_visited\nconverted\n\n\n\n\n0\n1\n0.000000\n\n\n1\n2\n0.000256\n\n\n2\n3\n0.000200\n\n\n3\n4\n0.000668\n\n\n4\n5\n0.001655\n\n\n5\n6\n0.003295\n\n\n6\n7\n0.006563\n\n\n7\n8\n0.015708\n\n\n8\n9\n0.032441\n\n\n9\n10\n0.060648\n\n\n10\n11\n0.127162\n\n\n11\n12\n0.246875\n\n\n12\n13\n0.398757\n\n\n13\n14\n0.595833\n\n\n14\n15\n0.745247\n\n\n15\n16\n0.877574\n\n\n16\n17\n0.922741\n\n\n17\n18\n0.967014\n\n\n18\n19\n0.991031\n\n\n19\n20\n0.996914\n\n\n20\n21\n1.000000\n\n\n21\n22\n1.000000\n\n\n22\n23\n1.000000\n\n\n23\n24\n1.000000\n\n\n24\n25\n1.000000\n\n\n25\n26\n1.000000\n\n\n26\n27\n1.000000\n\n\n27\n28\n1.000000\n\n\n28\n29\n1.000000\n\n\n\n\n\n\n\nFor customers who visited more than 20 pages, they are all converted."
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#split-train-val-data",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#split-train-val-data",
    "title": "Data challenge - 1. conversion rate",
    "section": "Split train val data",
    "text": "Split train val data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\n\ncategorical_features = ['country', 'new_user', 'source']\nnumerical_features = ['age', 'total_pages_visited']\n\nX_train, X_val, y_train, y_val = train_test_split(data[categorical_features + numerical_features], data[target], test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#build-model",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#build-model",
    "title": "Data challenge - 1. conversion rate",
    "section": "Build model",
    "text": "Build model\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_features)], remainder = 'passthrough', verbose_feature_names_out=False)\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestClassifier(n_jobs=-1, random_state=0))\n])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['country', 'new_user', 'source'])],\n                  verbose_feature_names_out=False)cat['country', 'new_user', 'source']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['age', 'total_pages_visited']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=0)"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#evaluation",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#evaluation",
    "title": "Data challenge - 1. conversion rate",
    "section": "Evaluation",
    "text": "Evaluation\n\npipe.score(X_train, y_train)\n\n0.9885652728225097\n\n\n\npipe.score(X_val, y_val)\n\n0.9845825426944972\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef calculate_roc_auc(pipe, X, y):\n    \"\"\"Calculate roc auc score.\n\n    Parameters:\n    ===========\n    model_pipe: sklearn model or pipeline\n    X: features\n    y: true target\n    \"\"\"\n    y_proba = pipe.predict_proba(X)[:,1]\n    return roc_auc_score(y, y_proba)\n\n\ndef evaluate_model(pipe, X_train, y_train, X_val, y_val):\n    print(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\n    print(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_val, y_val):.4f}\")\n\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nTrain ROC-AUC: 0.9943\nTest ROC-AUC: 0.9530"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#tune-hyperparameters",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#tune-hyperparameters",
    "title": "Data challenge - 1. conversion rate",
    "section": "Tune hyperparameters",
    "text": "Tune hyperparameters\n\n# from sklearn.model_selection import RandomizedSearchCV\n\n# n_estimators = [10, 25, 50, 100]  # number of trees in the random forest\n# max_depth = [3, 5, 7]  # maximum number of levels allowed in each decision tree\n# min_samples_split = [2, 6, 10]  # minimum sample number to split a node\n# min_samples_leaf = [1, 3, 4]  # minimum sample number that can be stored in a leaf node\n\n# random_grid = {'model__n_estimators': n_estimators,\n#                'model__max_depth': max_depth,\n#                'model__min_samples_split': min_samples_split,\n#                'model__min_samples_leaf': min_samples_leaf}\n\n\n# random_search = RandomizedSearchCV(pipe, random_grid, n_iter=20, n_jobs=-1)\n# random_search.fit(X_train, y_train)\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [10, 25, 50, 100]  # number of trees in the random forest\nmax_depth = [3, 5, 7]  # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10]  # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4]  # minimum sample number that can be stored in a leaf node\n\nrandom_grid = {'model__n_estimators': n_estimators,\n               'model__max_depth': max_depth,\n               'model__min_samples_split': min_samples_split,\n               'model__min_samples_leaf': min_samples_leaf}\n\n\nrandom_search = RandomizedSearchCV(pipe, random_grid, scoring='roc_auc', n_iter=20, n_jobs=-1)\nrandom_search.fit(X_train, y_train)\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['country',\n                                                                                'new_user',\n                                                                                'source'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestClassifier(n_jobs=-1,\n                                                                     random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [3, 5, 7],\n                                        'model__min_samples_leaf': [1, 3, 4],\n                                        'model__min_samples_split': [2, 6, 10],\n                                        'model__n_estimators': [10, 25, 50,\n                                                                100]},\n                   scoring='roc_auc')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['country',\n                                                                                'new_user',\n                                                                                'source'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestClassifier(n_jobs=-1,\n                                                                     random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [3, 5, 7],\n                                        'model__min_samples_leaf': [1, 3, 4],\n                                        'model__min_samples_split': [2, 6, 10],\n                                        'model__n_estimators': [10, 25, 50,\n                                                                100]},\n                   scoring='roc_auc')estimator: PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['country', 'new_user', 'source'])],\n                  verbose_feature_names_out=False)cat['country', 'new_user', 'source']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['age', 'total_pages_visited']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=0)\n\n\n\nmodel_pipe = random_search.best_estimator_\nprint(random_search.best_params_)\nevaluate_model(model_pipe, X_train, y_train, X_val, y_val)\n\n{'model__n_estimators': 100, 'model__min_samples_split': 2, 'model__min_samples_leaf': 3, 'model__max_depth': 7}\nTrain ROC-AUC: 0.9833\nTest ROC-AUC: 0.9841"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#roc",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#roc",
    "title": "Data challenge - 1. conversion rate",
    "section": "ROC",
    "text": "ROC\n\nfrom sklearn.metrics import RocCurveDisplay\n\ndef plot_roc(model_pipe, X_train, y_train, X_val, y_val):\n    fig, ax = plt.subplots()\n    RocCurveDisplay.from_estimator(model_pipe, X_train, y_train, ax=ax, name='Train')\n    RocCurveDisplay.from_estimator(model_pipe, X_val, y_val, ax=ax, name='Validation')\n    ax.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('ROC curve')\n    ax.axis(\"square\")\n    ax.legend(loc=\"lower right\")\n    plt.show()\n\n\nfig, ax = plt.subplots()\nRocCurveDisplay.from_estimator(model_pipe, X_train, y_train, ax=ax, name='Train')\nRocCurveDisplay.from_estimator(model_pipe, X_val, y_val, ax=ax, name='Validation')\nax.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve')\nax.axis(\"square\")\nax.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#gini-importance",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#gini-importance",
    "title": "Data challenge - 1. conversion rate",
    "section": "Gini importance",
    "text": "Gini importance\n\nfeature_names = model_pipe[:-1].get_feature_names_out()\nimportances = model_pipe[-1].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_pipe[-1].estimators_], axis=0)\n\nfeature_importances = pd.Series(importances, index=feature_names)\nfeature_importances\n\ncountry                0.033805\nnew_user               0.041653\nsource                 0.001591\nage                    0.022513\ntotal_pages_visited    0.900438\ndtype: float64\n\n\n\nfig, ax = plt.subplots()\n\nfeature_importances.plot.barh(xerr=std, ax=ax)\nax.set_title(\"Feature importances using Gini impurity\")\nax.set_ylabel(\"Mean decrease in impurity\")\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#permutation-importance",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#permutation-importance",
    "title": "Data challenge - 1. conversion rate",
    "section": "Permutation importance",
    "text": "Permutation importance\n\nfrom sklearn.inspection import permutation_importance\n\nfeature_names = model_pipe[:-1].get_feature_names_out()\nresult = permutation_importance(\n    model_pipe, X_val, y_val, scoring='roc_auc', n_repeats=30, random_state=42, n_jobs=-1\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\n\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=feature_names\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (validation set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in AUC score\")\nax.figure.tight_layout()\n\n\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": -1,\n    \"random_state\": 0\n}\n\nfeatures_info = {\n    \"features\": categorical_features + numerical_features,\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": categorical_features,\n}\n\n# fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(12, 4), constrained_layout=True)\n\ndisplay = PartialDependenceDisplay.from_estimator(\n    model_pipe,\n    X_val,\n    **features_info,\n    # ax=ax,\n    **common_params,\n)\n\n# _ = display.figure_.suptitle(\"Partial dependence (validation set)\")\nfig = plt.gcf()\nfig.suptitle('Partial dependence (validation set)')\n# fig.figure(figsize=(12, 4)),fig.set_constrained_layout(True),fig.set_layout_engine('constrained'),fig.set_size_inches(8, 6)\nfig.set(dpi=100, size_inches=(8, 6), layout_engine = 'constrained')\nplt.show()\n\n\n\n\n\nX_val.age = X_val.age.astype('float')\n\n\nX_val.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 50592 entries, 80044 to 44778\nData columns (total 5 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   country              50592 non-null  object \n 1   age                  50592 non-null  float64\n 2   new_user             50592 non-null  int64  \n 3   source               50592 non-null  object \n 4   total_pages_visited  50592 non-null  int64  \ndtypes: float64(1), int64(2), object(2)\nmemory usage: 2.3+ MB\n\n\n\nPartialDependenceDisplay.from_estimator(model_pipe, X_val, [(1, 4)], n_jobs=-1)\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#split-train-val-data-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#split-train-val-data-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "Split train val data",
    "text": "Split train val data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\n\ncategorical_features = ['country', 'new_user', 'source']\nnumerical_features = ['age']\n\nX_train, X_val, y_train, y_val = train_test_split(data[categorical_features + numerical_features], data[target], test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#build-model-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#build-model-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "Build model",
    "text": "Build model\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_features)], remainder = 'passthrough', verbose_feature_names_out=False)\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestClassifier(n_jobs=-1, random_state=0))\n])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['country', 'new_user', 'source'])],\n                  verbose_feature_names_out=False)cat['country', 'new_user', 'source']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['age']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=0)"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#evaluation-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#evaluation-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "Evaluation",
    "text": "Evaluation\n\npipe.score(X_train, y_train)\n\n0.967647727385035\n\n\n\npipe.score(X_val, y_val)\n\n0.9682163187855788\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef calculate_roc_auc(pipe, X, y):\n    \"\"\"Calculate roc auc score.\n\n    Parameters:\n    ===========\n    model_pipe: sklearn model or pipeline\n    X: features\n    y: true target\n    \"\"\"\n    y_proba = pipe.predict_proba(X)[:,1]\n    return roc_auc_score(y, y_proba)\n\n\ndef evaluate_model(pipe, X_train, y_train, X_val, y_val):\n    print(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\n    print(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_val, y_val):.4f}\")\n\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nTrain ROC-AUC: 0.8288\nTest ROC-AUC: 0.8167"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#tune-hyperparameters-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#tune-hyperparameters-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "Tune hyperparameters",
    "text": "Tune hyperparameters\n\n# from sklearn.model_selection import RandomizedSearchCV\n\n# n_estimators = [10, 25, 50, 100]  # number of trees in the random forest\n# max_depth = [3, 5, 7]  # maximum number of levels allowed in each decision tree\n# min_samples_split = [2, 6, 10]  # minimum sample number to split a node\n# min_samples_leaf = [1, 3, 4]  # minimum sample number that can be stored in a leaf node\n\n# random_grid = {'model__n_estimators': n_estimators,\n#                'model__max_depth': max_depth,\n#                'model__min_samples_split': min_samples_split,\n#                'model__min_samples_leaf': min_samples_leaf}\n\n\n# random_search = RandomizedSearchCV(pipe, random_grid, n_iter=20, n_jobs=-1)\n# random_search.fit(X_train, y_train)\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [10, 25, 50, 100]  # number of trees in the random forest\nmax_depth = [3, 5, 7]  # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10]  # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4]  # minimum sample number that can be stored in a leaf node\n\nrandom_grid = {'model__n_estimators': n_estimators,\n               'model__max_depth': max_depth,\n               'model__min_samples_split': min_samples_split,\n               'model__min_samples_leaf': min_samples_leaf}\n\n\nrandom_search = RandomizedSearchCV(pipe, random_grid, scoring='roc_auc', n_iter=20, n_jobs=-1)\nrandom_search.fit(X_train, y_train)\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['country',\n                                                                                'new_user',\n                                                                                'source'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestClassifier(n_jobs=-1,\n                                                                     random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [3, 5, 7],\n                                        'model__min_samples_leaf': [1, 3, 4],\n                                        'model__min_samples_split': [2, 6, 10],\n                                        'model__n_estimators': [10, 25, 50,\n                                                                100]},\n                   scoring='roc_auc')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['country',\n                                                                                'new_user',\n                                                                                'source'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestClassifier(n_jobs=-1,\n                                                                     random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [3, 5, 7],\n                                        'model__min_samples_leaf': [1, 3, 4],\n                                        'model__min_samples_split': [2, 6, 10],\n                                        'model__n_estimators': [10, 25, 50,\n                                                                100]},\n                   scoring='roc_auc')estimator: PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['country', 'new_user', 'source'])],\n                  verbose_feature_names_out=False)cat['country', 'new_user', 'source']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['age']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=0)\n\n\n\nmodel_pipe = random_search.best_estimator_\nprint(random_search.best_params_)\nevaluate_model(model_pipe, X_train, y_train, X_val, y_val)\n\n{'model__n_estimators': 25, 'model__min_samples_split': 10, 'model__min_samples_leaf': 1, 'model__max_depth': 5}\nTrain ROC-AUC: 0.8185\nTest ROC-AUC: 0.8250"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#roc-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#roc-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "ROC",
    "text": "ROC\n\nfrom sklearn.metrics import RocCurveDisplay\n\ndef plot_roc(model_pipe, X_train, y_train, X_val, y_val):\n    fig, ax = plt.subplots()\n    RocCurveDisplay.from_estimator(model_pipe, X_train, y_train, ax=ax, name='Train')\n    RocCurveDisplay.from_estimator(model_pipe, X_val, y_val, ax=ax, name='Validation')\n    ax.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('ROC curve')\n    ax.axis(\"square\")\n    ax.legend(loc=\"lower right\")\n    plt.show()\n\n\nfig, ax = plt.subplots()\nRocCurveDisplay.from_estimator(model_pipe, X_train, y_train, ax=ax, name='Train')\nRocCurveDisplay.from_estimator(model_pipe, X_val, y_val, ax=ax, name='Validation')\nax.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve')\nax.axis(\"square\")\nax.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#gini-importance-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#gini-importance-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "Gini importance",
    "text": "Gini importance\n\nfeature_names = model_pipe[:-1].get_feature_names_out()\nimportances = model_pipe[-1].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_pipe[-1].estimators_], axis=0)\n\nfeature_importances = pd.Series(importances, index=feature_names)\nfeature_importances\n\ncountry     0.284219\nnew_user    0.471137\nsource      0.004888\nage         0.239756\ndtype: float64\n\n\n\nfig, ax = plt.subplots()\n\nfeature_importances.plot.barh(xerr=std, ax=ax)\nax.set_title(\"Feature importances using Gini impurity\")\nax.set_ylabel(\"Mean decrease in impurity\")\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#permutation-importance-1",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#permutation-importance-1",
    "title": "Data challenge - 1. conversion rate",
    "section": "Permutation importance",
    "text": "Permutation importance\n\nfrom sklearn.inspection import permutation_importance\n\nfeature_names = model_pipe[:-1].get_feature_names_out()\nresult = permutation_importance(\n    model_pipe, X_val, y_val, scoring='roc_auc', n_repeats=30, random_state=42, n_jobs=-1\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\n\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=feature_names\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (validation set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in AUC score\")\nax.figure.tight_layout()\n\n\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": -1,\n    \"random_state\": 0\n}\n\nfeatures_info = {\n    \"features\": categorical_features + numerical_features,\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": categorical_features,\n}\n\n# fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(12, 4), constrained_layout=True)\n\ndisplay = PartialDependenceDisplay.from_estimator(\n    model_pipe,\n    X_val,\n    **features_info,\n    # ax=ax,\n    **common_params,\n)\n\n# _ = display.figure_.suptitle(\"Partial dependence (validation set)\")\nfig = plt.gcf()\nfig.suptitle('Partial dependence (validation set)')\n# fig.figure(figsize=(12, 4)),fig.set_constrained_layout(True),fig.set_layout_engine('constrained'),fig.set_size_inches(8, 6)\nfig.set(dpi=100, size_inches=(8, 6), layout_engine = 'constrained')\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#build-model-2",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#build-model-2",
    "title": "Data challenge - 1. conversion rate",
    "section": "Build model",
    "text": "Build model\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegressionCV\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_features)], remainder = 'passthrough', verbose_feature_names_out=False)\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', LogisticRegressionCV(\n        Cs = np.logspace(-3, 3, 7),\n        scoring='roc_auc',\n        max_iter=1000,\n        n_jobs=-1))\n])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 LogisticRegressionCV(Cs=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                                      max_iter=1000, n_jobs=-1,\n                                      scoring='roc_auc'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['country', 'new_user',\n                                                   'source'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 LogisticRegressionCV(Cs=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                                      max_iter=1000, n_jobs=-1,\n                                      scoring='roc_auc'))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['country', 'new_user', 'source'])],\n                  verbose_feature_names_out=False)cat['country', 'new_user', 'source']OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['age', 'total_pages_visited']passthroughpassthroughLogisticRegressionCVLogisticRegressionCV(Cs=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                     max_iter=1000, n_jobs=-1, scoring='roc_auc')"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#evaluation-2",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#evaluation-2",
    "title": "Data challenge - 1. conversion rate",
    "section": "Evaluation",
    "text": "Evaluation\n\npipe.score(X_train, y_train)\n\n0.9860819802567138\n\n\n\npipe.score(X_val, y_val)\n\n0.9873551207298474\n\n\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nTrain ROC-AUC: 0.9861\nTest ROC-AUC: 0.9874\n\n\nLogistic regression performs better than random forest."
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#roc-2",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#roc-2",
    "title": "Data challenge - 1. conversion rate",
    "section": "ROC",
    "text": "ROC\n\nplot_roc(pipe, X_train, y_train, X_val, y_val)"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#confusion-matrix",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#confusion-matrix",
    "title": "Data challenge - 1. conversion rate",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(pipe, X_val, y_val)\nplt.show()\n\n\n\n\nHow to interpret classification report?\nIf we care more about not losing a valued customer, we should not misclassify converted to not converted, we should increase recall, lower threshold."
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#feature-importance",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#feature-importance",
    "title": "Data challenge - 1. conversion rate",
    "section": "Feature importance",
    "text": "Feature importance\n\nfeat_importances = pd.DataFrame({\"name\":pipe[:-1].get_feature_names_out(), \"coef\":pipe[-1].coef_[0]})\nfeat_importances['importances'] = np.abs(feat_importances['coef'])\nfeat_importances = feat_importances.sort_values(by=\"importances\",ascending=False)\nfeat_importances\n\n\n\n\n\n\n\n\nname\ncoef\nimportances\n\n\n\n\n0\ncountry_China\n-2.595908\n2.595908\n\n\n1\ncountry_Germany\n1.066157\n1.066157\n\n\n2\ncountry_UK\n0.948609\n0.948609\n\n\n5\nnew_user_1\n-0.888335\n0.888335\n\n\n4\nnew_user_0\n0.829132\n0.829132\n\n\n10\ntotal_pages_visited\n0.762460\n0.762460\n\n\n3\ncountry_US\n0.521939\n0.521939\n\n\n7\nsource_Direct\n-0.129854\n0.129854\n\n\n9\nage\n-0.074395\n0.074395\n\n\n6\nsource_Ads\n0.057305\n0.057305\n\n\n8\nsource_Seo\n0.013346\n0.013346\n\n\n\n\n\n\n\n\nsns.barplot(data=feat_importances, x='coef', y='name')\nplt.plot()\n\n[]\n\n\n\n\n\n\nconversion rate in China is too low. Considering China’s huge population, target at China, eg, good Chinese translation, culture requirement.\nconversion rate in Germany is very good, although not a lot customers in Germany. Target at German users, eg. put more advertisement.\nold user has better conversion rate. We should keep in touch with them, for example, send promotion email/coupons.\ntotal_pages_visited is positive factor. The more pages visited, the more likely the customer will be converted. We should try to increase pages visited by customer. for example, if a customer visited a lot of pages but did not buy anything, we can send email to remind them.\nage is negative factor. The older, the less likely. We should focus on young people. Figure out why they like it and strangthen it. Figure out why older people doesn’t like it and try to correct it."
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#tree-plot",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#tree-plot",
    "title": "Data challenge - 1. conversion rate",
    "section": "Tree plot",
    "text": "Tree plot\n\nfrom sklearn import tree\n\ntree.plot_tree(pipe[-1], feature_names=X_train.columns, filled=True)\nfig = plt.gcf()\nfig.set(dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#what-features-in-the-data-did-the-model-think-are-most-important",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#what-features-in-the-data-did-the-model-think-are-most-important",
    "title": "Data challenge - 1. conversion rate",
    "section": "What features in the data did the model think are most important?",
    "text": "What features in the data did the model think are most important?\n\nGet a trained model.\nShuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That performance deterioration measures the importance of the variable you just shuffled.\nReturn the data to the original order (undoing the shuffle from step 2). Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column.\n\npermutation importance - eli5 https://www.kaggle.com/code/dansbecker/permutation-importance/tutorial\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Make a small change to the code below to use in this problem.\nperm = PermutationImportance(first_model, random_state=1).fit(val_X, val_y)\n\n# uncomment the following line to visualize your results\neli5.show_weights(perm, feature_names = val_X.columns.tolist())"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#for-any-single-prediction-from-a-model-how-did-each-feature-in-the-data-affect-that-particular-prediction",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#for-any-single-prediction-from-a-model-how-did-each-feature-in-the-data-affect-that-particular-prediction",
    "title": "Data challenge - 1. conversion rate",
    "section": "For any single prediction from a model, how did each feature in the data affect that particular prediction?",
    "text": "For any single prediction from a model, how did each feature in the data affect that particular prediction?\nhttps://www.kaggle.com/code/dansbecker/partial-plots/tutorial\ncan install pdpbox, but KeyError: ‘Patch’\nfrom pdpbox import pdp\n\ntree_model = clf\nfeature_names = data.drop(columns=['converted']).columns.to_list()\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=X_val, model_features=feature_names, feature='total_pages_visited')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'total_pages_visited')\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-04-data1-conversion/conversion1.html#shap",
    "href": "posts/2022-12-04-data1-conversion/conversion1.html#shap",
    "title": "Data challenge - 1. conversion rate",
    "section": "SHAP",
    "text": "SHAP\nhttps://www.kaggle.com/code/dansbecker/shap-values/tutorial How does each feature affect the model’s predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html",
    "href": "posts/2022-05-31-python-oop/index.html",
    "title": "Python oop",
    "section": "",
    "text": "Source\n类(Class)是用来描述具有相同属性(Attribute)和方法(Method)对象的集合。\n对象(Object)是类(Class)的具体实例。\n属性(Attribute): 类里面用于描述所有对象共同特征的变量或数据。比如学生的名字和分数。\n方法(Method): 类里面的函数，用来区别类外面的函数, 用来实现某些功能。比如打印出学生的名字和分数。\n\n# 创建一个学生类\nclass Student:\n    \n    # 定义学生属性，初始化方法\n    def __init__(self, name, score):\n        self.name = name\n        self.score = score\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.score))\n\n\nstudent1 = Student(\"John\", 100)\nstudent2 = Student(\"Lucy\", 99)\n\n\nstudent1.name\n\n'John'\n\n\n\nstudent1.show()\n\nName: John. Score: 100\n\n\n\n\n假设我们需要在Student类里增加一个计数器number，每当一个新的学生对象(Object)被创建时，这个计数器就自动加1。由于这个计数器不属于某个具体学生，而属于Student类的，所以被称为类变量(class variables)。而姓名和分数属于每个学生对象的，所以属于实例变量(instance variables)，也被称为对象变量(object variables)。\n类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。\n\n访问或调用类变量的正确方式是类名.变量名或者self.__class__.变量名。self.__class__自动返回每个对象的类名。\n\n实例变量：定义在方法中的变量，属于某个具体的对象。\n\n访问或调用实例变量的正确方式是对象名.变量名或者self.变量名.\n\n\n# 创建一个学生类\nclass Student:\n\n    # number属于类变量，不属于某个具体的学生实例\n    number = 0\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量\n    def __init__(self, name, score):\n        self.name = name\n        self.score = score\n        Student.number = Student.number + 1\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.score))\n\n# 实例化，创建对象\nstudent1 = Student(\"John\", 100)\nstudent2 = Student(\"Lucy\", 99)\n\nprint(Student.number)  # 打印2\nprint(student1.__class__.number) # 打印2\n\n2\n2\n\n\n\n\n\n正如同有些变量只属于类，有些方法也只属于类，不属于具体的对象。你有没有注意到属于对象的方法里面都有一个self参数, 比如__init__(self), show(self)?\nself是指对象本身。属于类的方法不使用self参数， 而使用参数cls，代表类本身。另外习惯上对类方法我们会加上@classmethod的修饰符做说明。\n调用类方法：类名.类方法名\n\nclass Student:\n\n    # number属于类变量，不属于某个具体的学生实例\n    number = 0\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量\n    def __init__(self, name, score):\n        self.name = name\n        self.score = score\n        Student.number = Student.number + 1\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.score))\n\n    # 定义类方法，打印学生的数量\n    @classmethod\n    def total(cls):\n        print(\"Total: {0}\".format(cls.number))\n\n\n# 实例化，创建对象\nstudent1 = Student(\"John\", 100)\nstudent2 = Student(\"Lucy\", 99)\n\nStudent.total()  # 打印 Total: 2\n\nTotal: 2\n\n\n\n\n\n类里面的私有属性和私有方法以双下划线__开头。私有属性或方法不能在类的外部被使用或直接访问。\n在面向对象的编程中,通常情况下很少让外部类直接访问类内部的属性和方法，而是向外部类提供一些按钮,对其内部的成员进行访问,以保证程序的安全性，这就是封装。\n\n# 创建一个学生类\nclass Student:\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量, 其中__score属于私有变量\n    def __init__(self, name, score):\n        self.name = name\n        self.__score = score\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.__score))\n\n# 实例化，创建对象\nstudent1 = Student(\"John\", 100)\n\nstudent1.show()  # 打印 Name: John, Score: 100\nstudent1.__score  # 打印出错，该属性不能从外部访问。\n\nName: John. Score: 100\n\n\nAttributeError: 'Student' object has no attribute '__score'\n\n\n\n\n\nclass instance and object instance\n\n# Python Object Oriented Programming by Joe Marini course example\n# Using class-level and static methods\n\n\nclass Book:\n    # 类变量\n    BOOK_TYPES=(\"HARDCOVER\",\"PAPERBACK\",\"EBOOK\")\n    # 类的私有变量\n    __booklist = None\n\n    # 类方法\n    @classmethod\n    def getbooktypes(cls):\n        return cls.BOOK_TYPES\n\n    # 类的静态方法\n    @staticmethod\n    def getbooklist():\n        if Book.__booklist==None:\n            Book.__booklist=[]\n        return Book.__booklist\n\n    # instance methods receive a specific object instance as an argument\n    # and operate on data specific to that object instance\n    \n    # 对象的方法\n    def setTitle(self, newtitle):\n        self.title = newtitle\n\n    def __init__(self, title, booktype):\n        self.title = title\n        if (not booktype in Book.BOOK_TYPES):\n            raise ValueError(f\"{booktype} is not a valid book type\")\n        else:\n            self.booktype=booktype\n\n\n# 用静态方法调用类的私有变量\nprint(\"Book types: \", Book.getbooktypes())\n\n# TODO: Create some book instances\nb1 = Book(\"Title 1\",\"HARDCOVER\")\nb2 = Book(\"Title 2\",'PAPERBACK')\n\n# TODO: Use the static method to access a singleton object\nthebooks=Book.getbooklist()\nthebooks.append(b1)\nthebooks.append(b2)\nprint(thebooks)\n# 结果中含两个instances\n\nBook types:  ('HARDCOVER', 'PAPERBACK', 'EBOOK')\n[&lt;__main__.Book object at 0x7fd5787cebe0&gt;, &lt;__main__.Book object at 0x7fd578e49a30&gt;]\n\n\n\n\n\n我们有没有一种方法让用户通过student1.score来访问学生分数而继续保持__score私有变量的属性呢？这时我们就可以借助python的@property装饰器了。\n我们可以先定义一个方法score(), 然后利用@property把这个函数伪装成属性。\n\n# 创建一个学生类\nclass Student:\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量, 其中score属于私有变量\n    def __init__(self, name, score):\n        self.name = name\n        self.__score = score\n\n    # 利用property装饰器把函数伪装成属性\n    @property\n    def score(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.__score))\n\n# 实例化，创建对象\n\nstudent1 = Student(\"John\", 100)\n\n# 一旦给函数加上一个装饰器@property,调用函数的时候不用加括号就可以直接调用函数了 \nstudent1.score  # 打印 Name: John. Score: 100\n\nName: John. Score: 100\n\n\n\n\n\n子类就可以从父类那里获得其已有的属性与方法，这种现象叫做类的继承。\n我们再看另一个例子，老师和学生同属学校成员，都有姓名和年龄的属性，然而老师有工资这个专有属性，学生有分数这个专有属性。这时我们就可以定义1一个学校成员父类，2个子类。\n\n# 创建父类学校成员SchoolMember\nclass SchoolMember:\n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def tell(self):\n        # 打印个人信息\n        print('Name:\"{}\" Age:\"{}\"'.format(self.name, self.age), end=\" \")\n\n\n# 创建子类老师 Teacher\nclass Teacher(SchoolMember):\n\n    def __init__(self, name, age, salary):\n        SchoolMember.__init__(self, name, age) # 利用父类进行初始化，手动调用父类的构造函数__init__来完成子类的构造\n        self.salary = salary\n\n    # 方法重写\n    def tell(self):\n        SchoolMember.tell(self) # 调用父类方法，打印name，age\n                                # 在子类中调用父类的方法时，需要加上父类的类名前缀，且需要带上self参数变量。比如SchoolMember.tell(self), 这个可以通过使用super关键词简化代码。\n                                # 如果子类调用了某个方法(如tell())或属性，Python会先在子类中找，如果找到了会直接调用。如果找不到才会去父类找。这为方法重写带来了便利。\n        print('Salary: {}'.format(self.salary))\n\n\n# 创建子类学生Student\nclass Student(SchoolMember):\n\n    def __init__(self, name, age, score):\n        SchoolMember.__init__(self, name, age)\n        self.score = score\n\n    def tell(self):\n        SchoolMember.tell(self)\n        print('score: {}'.format(self.score))\n\n\nteacher1 = Teacher(\"John\", 44, \"$60000\")\nstudent1 = Student(\"Mary\", 12, 99)\n\nteacher1.tell()  # 打印 Name:\"John\" Age:\"44\" Salary: $60000\nstudent1.tell()  # Name:\"Mary\" Age:\"12\" score: 99\n\nName:\"John\" Age:\"44\" Salary: $60000\nName:\"Mary\" Age:\"12\" score: 99\n\n\n\n\n在子类当中可以通过使用super关键字来直接调用父类的中相应的方法，简化代码。\n\n# 创建子类学生Student\nclass Student(SchoolMember):\n\n    def __init__(self, name, age, score):\n        SchoolMember.__init__(self, name, age)\n        self.score = score\n\n    def tell(self):\n        super().tell() # super().tell()等同于 SchoolMember.tell(self)\n                       # 当你使用Python super()关键字调用父类方法时时，注意去掉括号里self这个参数。\n        print('score: {}'.format(self.score))"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html#类变量class-variables与实例变量instance-variables",
    "href": "posts/2022-05-31-python-oop/index.html#类变量class-variables与实例变量instance-variables",
    "title": "Python oop",
    "section": "",
    "text": "假设我们需要在Student类里增加一个计数器number，每当一个新的学生对象(Object)被创建时，这个计数器就自动加1。由于这个计数器不属于某个具体学生，而属于Student类的，所以被称为类变量(class variables)。而姓名和分数属于每个学生对象的，所以属于实例变量(instance variables)，也被称为对象变量(object variables)。\n类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。\n\n访问或调用类变量的正确方式是类名.变量名或者self.__class__.变量名。self.__class__自动返回每个对象的类名。\n\n实例变量：定义在方法中的变量，属于某个具体的对象。\n\n访问或调用实例变量的正确方式是对象名.变量名或者self.变量名.\n\n\n# 创建一个学生类\nclass Student:\n\n    # number属于类变量，不属于某个具体的学生实例\n    number = 0\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量\n    def __init__(self, name, score):\n        self.name = name\n        self.score = score\n        Student.number = Student.number + 1\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.score))\n\n# 实例化，创建对象\nstudent1 = Student(\"John\", 100)\nstudent2 = Student(\"Lucy\", 99)\n\nprint(Student.number)  # 打印2\nprint(student1.__class__.number) # 打印2\n\n2\n2"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html#类方法class-method",
    "href": "posts/2022-05-31-python-oop/index.html#类方法class-method",
    "title": "Python oop",
    "section": "",
    "text": "正如同有些变量只属于类，有些方法也只属于类，不属于具体的对象。你有没有注意到属于对象的方法里面都有一个self参数, 比如__init__(self), show(self)?\nself是指对象本身。属于类的方法不使用self参数， 而使用参数cls，代表类本身。另外习惯上对类方法我们会加上@classmethod的修饰符做说明。\n调用类方法：类名.类方法名\n\nclass Student:\n\n    # number属于类变量，不属于某个具体的学生实例\n    number = 0\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量\n    def __init__(self, name, score):\n        self.name = name\n        self.score = score\n        Student.number = Student.number + 1\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.score))\n\n    # 定义类方法，打印学生的数量\n    @classmethod\n    def total(cls):\n        print(\"Total: {0}\".format(cls.number))\n\n\n# 实例化，创建对象\nstudent1 = Student(\"John\", 100)\nstudent2 = Student(\"Lucy\", 99)\n\nStudent.total()  # 打印 Total: 2\n\nTotal: 2"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html#类的私有属性private-attribute和私有方法private-method",
    "href": "posts/2022-05-31-python-oop/index.html#类的私有属性private-attribute和私有方法private-method",
    "title": "Python oop",
    "section": "",
    "text": "类里面的私有属性和私有方法以双下划线__开头。私有属性或方法不能在类的外部被使用或直接访问。\n在面向对象的编程中,通常情况下很少让外部类直接访问类内部的属性和方法，而是向外部类提供一些按钮,对其内部的成员进行访问,以保证程序的安全性，这就是封装。\n\n# 创建一个学生类\nclass Student:\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量, 其中__score属于私有变量\n    def __init__(self, name, score):\n        self.name = name\n        self.__score = score\n\n    # 定义打印学生信息的方法\n    def show(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.__score))\n\n# 实例化，创建对象\nstudent1 = Student(\"John\", 100)\n\nstudent1.show()  # 打印 Name: John, Score: 100\nstudent1.__score  # 打印出错，该属性不能从外部访问。\n\nName: John. Score: 100\n\n\nAttributeError: 'Student' object has no attribute '__score'"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html#类的静态方法",
    "href": "posts/2022-05-31-python-oop/index.html#类的静态方法",
    "title": "Python oop",
    "section": "",
    "text": "class instance and object instance\n\n# Python Object Oriented Programming by Joe Marini course example\n# Using class-level and static methods\n\n\nclass Book:\n    # 类变量\n    BOOK_TYPES=(\"HARDCOVER\",\"PAPERBACK\",\"EBOOK\")\n    # 类的私有变量\n    __booklist = None\n\n    # 类方法\n    @classmethod\n    def getbooktypes(cls):\n        return cls.BOOK_TYPES\n\n    # 类的静态方法\n    @staticmethod\n    def getbooklist():\n        if Book.__booklist==None:\n            Book.__booklist=[]\n        return Book.__booklist\n\n    # instance methods receive a specific object instance as an argument\n    # and operate on data specific to that object instance\n    \n    # 对象的方法\n    def setTitle(self, newtitle):\n        self.title = newtitle\n\n    def __init__(self, title, booktype):\n        self.title = title\n        if (not booktype in Book.BOOK_TYPES):\n            raise ValueError(f\"{booktype} is not a valid book type\")\n        else:\n            self.booktype=booktype\n\n\n# 用静态方法调用类的私有变量\nprint(\"Book types: \", Book.getbooktypes())\n\n# TODO: Create some book instances\nb1 = Book(\"Title 1\",\"HARDCOVER\")\nb2 = Book(\"Title 2\",'PAPERBACK')\n\n# TODO: Use the static method to access a singleton object\nthebooks=Book.getbooklist()\nthebooks.append(b1)\nthebooks.append(b2)\nprint(thebooks)\n# 结果中含两个instances\n\nBook types:  ('HARDCOVER', 'PAPERBACK', 'EBOOK')\n[&lt;__main__.Book object at 0x7fd5787cebe0&gt;, &lt;__main__.Book object at 0x7fd578e49a30&gt;]"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html#property的用法与神奇之处",
    "href": "posts/2022-05-31-python-oop/index.html#property的用法与神奇之处",
    "title": "Python oop",
    "section": "",
    "text": "我们有没有一种方法让用户通过student1.score来访问学生分数而继续保持__score私有变量的属性呢？这时我们就可以借助python的@property装饰器了。\n我们可以先定义一个方法score(), 然后利用@property把这个函数伪装成属性。\n\n# 创建一个学生类\nclass Student:\n\n    # 定义学生属性，初始化方法\n    # name和score属于实例变量, 其中score属于私有变量\n    def __init__(self, name, score):\n        self.name = name\n        self.__score = score\n\n    # 利用property装饰器把函数伪装成属性\n    @property\n    def score(self):\n        print(\"Name: {}. Score: {}\".format(self.name, self.__score))\n\n# 实例化，创建对象\n\nstudent1 = Student(\"John\", 100)\n\n# 一旦给函数加上一个装饰器@property,调用函数的时候不用加括号就可以直接调用函数了 \nstudent1.score  # 打印 Name: John. Score: 100\n\nName: John. Score: 100"
  },
  {
    "objectID": "posts/2022-05-31-python-oop/index.html#类的继承inheritance",
    "href": "posts/2022-05-31-python-oop/index.html#类的继承inheritance",
    "title": "Python oop",
    "section": "",
    "text": "子类就可以从父类那里获得其已有的属性与方法，这种现象叫做类的继承。\n我们再看另一个例子，老师和学生同属学校成员，都有姓名和年龄的属性，然而老师有工资这个专有属性，学生有分数这个专有属性。这时我们就可以定义1一个学校成员父类，2个子类。\n\n# 创建父类学校成员SchoolMember\nclass SchoolMember:\n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def tell(self):\n        # 打印个人信息\n        print('Name:\"{}\" Age:\"{}\"'.format(self.name, self.age), end=\" \")\n\n\n# 创建子类老师 Teacher\nclass Teacher(SchoolMember):\n\n    def __init__(self, name, age, salary):\n        SchoolMember.__init__(self, name, age) # 利用父类进行初始化，手动调用父类的构造函数__init__来完成子类的构造\n        self.salary = salary\n\n    # 方法重写\n    def tell(self):\n        SchoolMember.tell(self) # 调用父类方法，打印name，age\n                                # 在子类中调用父类的方法时，需要加上父类的类名前缀，且需要带上self参数变量。比如SchoolMember.tell(self), 这个可以通过使用super关键词简化代码。\n                                # 如果子类调用了某个方法(如tell())或属性，Python会先在子类中找，如果找到了会直接调用。如果找不到才会去父类找。这为方法重写带来了便利。\n        print('Salary: {}'.format(self.salary))\n\n\n# 创建子类学生Student\nclass Student(SchoolMember):\n\n    def __init__(self, name, age, score):\n        SchoolMember.__init__(self, name, age)\n        self.score = score\n\n    def tell(self):\n        SchoolMember.tell(self)\n        print('score: {}'.format(self.score))\n\n\nteacher1 = Teacher(\"John\", 44, \"$60000\")\nstudent1 = Student(\"Mary\", 12, 99)\n\nteacher1.tell()  # 打印 Name:\"John\" Age:\"44\" Salary: $60000\nstudent1.tell()  # Name:\"Mary\" Age:\"12\" score: 99\n\nName:\"John\" Age:\"44\" Salary: $60000\nName:\"Mary\" Age:\"12\" score: 99\n\n\n\n\n在子类当中可以通过使用super关键字来直接调用父类的中相应的方法，简化代码。\n\n# 创建子类学生Student\nclass Student(SchoolMember):\n\n    def __init__(self, name, age, score):\n        SchoolMember.__init__(self, name, age)\n        self.score = score\n\n    def tell(self):\n        super().tell() # super().tell()等同于 SchoolMember.tell(self)\n                       # 当你使用Python super()关键字调用父类方法时时，注意去掉括号里self这个参数。\n        print('score: {}'.format(self.score))"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html",
    "href": "posts/2022-12-06-data7-email/email7.html",
    "title": "Data challenge - 7. email",
    "section": "",
    "text": "From the marketing team perspective, a success is if the user clicks on the link inside of the email.\n&lt;span style='color:orange;font-weight:bold;font-size:1.5em'&gt;&lt;/span&gt;\n&lt;span style='color:red'&gt;&lt;/span&gt;"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#target-variable",
    "href": "posts/2022-12-06-data7-email/email7.html#target-variable",
    "title": "Data challenge - 7. email",
    "section": "Target variable",
    "text": "Target variable\n\ndef find_frequency(series):\n    # Provide summary on frequency counts and proportions.\n\n    columns = ['p_frequency', 'n_frequency']\n    frequency = pd.concat([series.value_counts(normalize=True),\n                           series.value_counts()], keys=columns, axis=1)\n    return frequency\n\nfind_frequency(df.clicked)\n\n\n\n\n\n\n\n\np_frequency\nn_frequency\n\n\n\n\n0\n0.97881\n97881\n\n\n1\n0.02119\n2119\n\n\n\n\n\n\n\nThe target variable is highly imbalanced."
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#check-missing-values",
    "href": "posts/2022-12-06-data7-email/email7.html#check-missing-values",
    "title": "Data challenge - 7. email",
    "section": "Check missing values",
    "text": "Check missing values\n\ndef summarise(df):\n    # Provide summary on missing values, unique values and data type.\n    columns = ['n_missing', 'p_missing', 'n_unique', 'dtype']\n    summary = pd.concat([df.isnull().sum(),\n                         df.isnull().mean(),\n                         df.nunique(),\n                         df.dtypes], keys=columns, axis=1)\n    return summary.sort_values(by='n_missing', ascending=False)\n\nsummarise(df)\n\n\n\n\n\n\n\n\nn_missing\np_missing\nn_unique\ndtype\n\n\n\n\nemail_text\n0\n0.0\n2\nobject\n\n\nemail_version\n0\n0.0\n2\nobject\n\n\nhour\n0\n0.0\n24\nint64\n\n\nweekday\n0\n0.0\n7\nobject\n\n\nuser_country\n0\n0.0\n4\nobject\n\n\nuser_past_purchases\n0\n0.0\n23\nint64\n\n\nopened\n0\n0.0\n2\nint64\n\n\nclicked\n0\n0.0\n2\nint64\n\n\n\n\n\n\n\nNo missing values. Data looks good."
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#numerical-features",
    "href": "posts/2022-12-06-data7-email/email7.html#numerical-features",
    "title": "Data challenge - 7. email",
    "section": "Numerical features",
    "text": "Numerical features\n\nnumerical_feats = ['user_past_purchases']\ntarget = 'clicked'\n\ndf[numerical_feats].describe()\n\n\n\n\n\n\n\n\nuser_past_purchases\n\n\n\n\ncount\n100000.00000\n\n\nmean\n3.87845\n\n\nstd\n3.19611\n\n\nmin\n0.00000\n\n\n25%\n1.00000\n\n\n50%\n3.00000\n\n\n75%\n6.00000\n\n\nmax\n22.00000\n\n\n\n\n\n\n\n\ndef find_outlier(series, k=1.5):\n    \"\"\"Find outlier using first and third quartiles and interquartile range.\n    Parameters\n    ----------\n    series: A pandas Series to find outlier in\n    k: (optional) An integer indicating threshold of outlier in IQR from Q1/Q3\n    Returns\n    -------\n    A pandas Series containing boolean values where True indicates an outlier.\n    \"\"\"\n    q1 = series.quantile(.25)\n    q3 = series.quantile(.75)\n    iqr = q3-q1\n    lower_bound = q1 - k*iqr\n    upper_bound = q3 + k*iqr\n    is_outlier = (series&lt;lower_bound) | (series&gt;upper_bound)\n    return is_outlier\n\n\ndef describe_more(df, features, k=1.5):\n    #Provide descriptive statistics and outlier summary for numerical features.\n\n    descriptives = df[features].describe()\n    outliers = df[features].apply(find_outlier)\n    descriptives.loc['n_outliers']= outliers.sum()\n    descriptives.loc['p_outliers']= outliers.mean()\n    return descriptives\n\ndescribe_more(df, numerical_feats)\n\n\n\n\n\n\n\n\nuser_past_purchases\n\n\n\n\ncount\n100000.00000\n\n\nmean\n3.87845\n\n\nstd\n3.19611\n\n\nmin\n0.00000\n\n\n25%\n1.00000\n\n\n50%\n3.00000\n\n\n75%\n6.00000\n\n\nmax\n22.00000\n\n\nn_outliers\n778.00000\n\n\np_outliers\n0.00778\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\ndef plot_continuous(df, feature, target, bins=30):\n    \"\"\"Plot histogram, density plot, box plot and rate for for each class in target.\n\n    A plot containing 4 subplots.\n    Top left subplot shows histogram.\n    Top right subplot shows density plot.\n    Bottom left subplot shows box plot.\n    Bottom right subplot shows feature rate vs. target.\n    \"\"\"\n    fig, ax = plt.subplots(2, 2, figsize=(14,8))\n\n    sns.histplot(data=df, x=feature, hue=target, bins=bins, ax=ax[0,0])\n    ax[0,0].set_title(f'Histogram of {feature} by {target}')\n\n    sns.kdeplot(data=df, x=feature, hue=target, fill=True, common_norm=False, ax=ax[0,1])\n    ax[0,1].set_title(f'Density plot of {feature} by {target}')\n\n    sns.violinplot(data=df, y=feature, x=target, ax=ax[1,0])\n    ax[1,0].set_title(f'Violin plot of {feature} by {target}')\n\n    grouped = df[[feature, target]].groupby(feature).mean().reset_index().sort_values(feature)\n    sns.lineplot(data=grouped, x=feature, y=target, ax=ax[1,1])\n    # sns.violinplot(data=df, y=feature, x=target, ax=ax[1,1])\n    ax[1,1].set_title(f'Plot of {target} rate vs. {feature}')\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\nfor feature in numerical_feats:\n    plot_continuous(df, feature, target)\n\n\n\n\n\nuser_past_purchases\n\nFrom histogram and density plot, we can see for users who did not click, most have few past purchases.\nFrom violin plot, we can see the median of past purchases of user who clicked is larger than that of who did not click.\nFrom clicked rate vs. past purchases, we can see a trend that the more past purchases, the more past purchases, the more likely the user clicked. This is reasonable that more past purchased indicates more favor of the company and thus likely to click the link.\n\n&lt;span style='color:orange;font-weight:bold;font-size:1.5em'&gt;&lt;/span&gt;"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#categorical-features",
    "href": "posts/2022-12-06-data7-email/email7.html#categorical-features",
    "title": "Data challenge - 7. email",
    "section": "Categorical features",
    "text": "Categorical features\n\ndf.columns\n\nIndex(['email_text', 'email_version', 'hour', 'weekday', 'user_country',\n       'user_past_purchases', 'opened', 'clicked'],\n      dtype='object')\n\n\n\ncategorical_feats = ['email_text', 'email_version', 'hour', 'weekday', 'user_country']\n\n\ndef plot_discrete(df, feature, target, orientation='v', figsize=(18, 4)):\n    \"\"\"Plot target mean and counts for unique values in feature.\n    Parameters\n    ----------\n    df: A pandas DataFrame to use\n    feature: A string specifying the name of the feature column\n    target: A string specifying the name of the target column\n    orientation: (optional) 'h' for horizontal and 'v' for  orientation of bars\n    figsize: (optional) A tuple specifying the shape of the plot\n    Returns\n    -------\n    A plot containing 2 subplots. Left subplot shows counts of categories. Right\n    subplot shows target mean value for each category.\n    \"\"\"\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    if orientation=='v':\n        sns.countplot(data=df, x=feature, ax=ax[0])\n        sns.countplot(data=df, x=feature, hue=target, ax=ax[1])\n        sns.barplot(data=df, x=feature, y=target, ax=ax[2])\n        # ax[2].set_ylim([0,1])\n\n    elif orientation=='h':\n        sns.countplot(data=df, y=feature, ax=ax[0])\n        sns.countplot(data=df, y=feature, hue=target, ax=ax[1])\n        sns.barplot(data=df, x=target, y=feature, orient='h', ax=ax[2])\n        # ax[2].set_xlim([0,1])\n\n    ax[0].set_title(f\"Category counts in {feature}\")\n    ax[1].set_title(f\"Category counts in {feature} by {target}\")\n    ax[2].set_title(f\"Mean {target} rate by category in {feature}\")\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\n# Visualise\n# for feature in categorical_feats:\n#     plot_discrete(df, feature, target)\n\n\nplot_discrete(df, categorical_feats[0], target)\n\n\n\n\n\nemail_text\n\nThe numbers of sent short emails and sent long emails are close.\nBoth short and long emails have low clicked rate and more users clicked on short email.\n\n\nplot_discrete(df, categorical_feats[1], target)\n\n\n\n\n\n\nemail_version\n\nThe number of sent personalized and generic emails are close.\nBoth personalized and generic have low clicked rate and more users clicked on personalized email.\n\n\nplot_discrete(df, categorical_feats[2], target)\n\n\n\n\n\n\nhours\n\nMost emails were sent in daytime (6 - 16)\nIt seems that sending hour has some impact on click rate (the click rate is higher at hour 23, 24). However, the error bar is large and this could be caused by few samples during such hours. Further analysis, like ANOVA could be used to test whether there is difference among them.\n\n&lt;span style='color:red'&gt;&lt;/span&gt;\n\nplot_discrete(df, categorical_feats[3], target)\n\n\n\n\n\n\nweekday\n\nFriday, Saturday, Sunday have lower click rate than Monday to Thursday. People normally do not process emails on weekends.\n\n\nplot_discrete(df, categorical_feats[4], target)\n\n\n\n\n\n\nuser_country\n\nMost emails were sent to US.\nUS, UK have higher click rate while FR, ES have lower click rate. This could be the translation issue. I guess the email was written in English, which is unreadable to some non-English users, which cause low click rate."
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#feature-importance-base-model",
    "href": "posts/2022-12-06-data7-email/email7.html#feature-importance-base-model",
    "title": "Data challenge - 7. email",
    "section": "Feature importance: base model",
    "text": "Feature importance: base model\n\npipe_base.feature_names_in_\n\narray(['email_text', 'email_version', 'hour', 'weekday', 'user_country',\n       'user_past_purchases'], dtype=object)\n\n\n\npipe_base[:-1].get_feature_names_out()\n\narray(['cat__email_text_long_email', 'cat__email_text_short_email',\n       'cat__email_version_generic', 'cat__email_version_personalized',\n       'cat__hour_1', 'cat__hour_2', 'cat__hour_3', 'cat__hour_4',\n       'cat__hour_5', 'cat__hour_6', 'cat__hour_7', 'cat__hour_8',\n       'cat__hour_9', 'cat__hour_10', 'cat__hour_11', 'cat__hour_12',\n       'cat__hour_13', 'cat__hour_14', 'cat__hour_15', 'cat__hour_16',\n       'cat__hour_17', 'cat__hour_18', 'cat__hour_19', 'cat__hour_20',\n       'cat__hour_21', 'cat__hour_22', 'cat__hour_23', 'cat__hour_24',\n       'cat__weekday_Friday', 'cat__weekday_Monday',\n       'cat__weekday_Saturday', 'cat__weekday_Sunday',\n       'cat__weekday_Thursday', 'cat__weekday_Tuesday',\n       'cat__weekday_Wednesday', 'cat__user_country_ES',\n       'cat__user_country_FR', 'cat__user_country_UK',\n       'cat__user_country_US', 'remainder__user_past_purchases'],\n      dtype=object)\n\n\n\npipe_base[-1].coef_[0]\n\narray([-0.12563961,  0.13677691, -0.30528676,  0.31642406,  0.02746371,\n       -0.26113829, -0.06110822, -0.33445855, -0.1041927 , -0.1873496 ,\n       -0.11525458, -0.01876223,  0.21080367,  0.36645922,  0.31071168,\n        0.32373923, -0.07248329,  0.00281335,  0.14399457,  0.03122329,\n       -0.01627106, -0.12409123, -0.25234426, -0.3709864 , -0.7049287 ,\n        0.16449357,  0.70240016,  0.35040398, -0.43145013,  0.12806989,\n       -0.11114658, -0.19495462,  0.19525258,  0.13343291,  0.29193325,\n       -0.52571853, -0.62455879,  0.6029985 ,  0.55841612,  0.18815361])\n\n\n\nimport numpy as np\n\nfeat_names = pipe_base[:-1].get_feature_names_out()\nfeat_coefficients = pipe_base[-1].coef_[0]\n\nfeat_importances = pd.DataFrame({\"name\":feat_names,\"coef\":feat_coefficients})\nfeat_importances = feat_importances[['name','coef']]# reorder the columns\nfeat_importances['importances'] = np.abs(feat_importances['coef'])\nfeat_importances.sort_values(by=\"importances\",inplace=True,ascending=False)\nfeat_importances\n\n\n\n\n\n\n\n\nname\ncoef\nimportances\n\n\n\n\n24\ncat__hour_21\n-0.704929\n0.704929\n\n\n26\ncat__hour_23\n0.702400\n0.702400\n\n\n36\ncat__user_country_FR\n-0.624559\n0.624559\n\n\n37\ncat__user_country_UK\n0.602999\n0.602999\n\n\n38\ncat__user_country_US\n0.558416\n0.558416\n\n\n35\ncat__user_country_ES\n-0.525719\n0.525719\n\n\n28\ncat__weekday_Friday\n-0.431450\n0.431450\n\n\n23\ncat__hour_20\n-0.370986\n0.370986\n\n\n13\ncat__hour_10\n0.366459\n0.366459\n\n\n27\ncat__hour_24\n0.350404\n0.350404\n\n\n7\ncat__hour_4\n-0.334459\n0.334459\n\n\n15\ncat__hour_12\n0.323739\n0.323739\n\n\n3\ncat__email_version_personalized\n0.316424\n0.316424\n\n\n14\ncat__hour_11\n0.310712\n0.310712\n\n\n2\ncat__email_version_generic\n-0.305287\n0.305287\n\n\n34\ncat__weekday_Wednesday\n0.291933\n0.291933\n\n\n5\ncat__hour_2\n-0.261138\n0.261138\n\n\n22\ncat__hour_19\n-0.252344\n0.252344\n\n\n12\ncat__hour_9\n0.210804\n0.210804\n\n\n32\ncat__weekday_Thursday\n0.195253\n0.195253\n\n\n31\ncat__weekday_Sunday\n-0.194955\n0.194955\n\n\n39\nremainder__user_past_purchases\n0.188154\n0.188154\n\n\n9\ncat__hour_6\n-0.187350\n0.187350\n\n\n25\ncat__hour_22\n0.164494\n0.164494\n\n\n18\ncat__hour_15\n0.143995\n0.143995\n\n\n1\ncat__email_text_short_email\n0.136777\n0.136777\n\n\n33\ncat__weekday_Tuesday\n0.133433\n0.133433\n\n\n29\ncat__weekday_Monday\n0.128070\n0.128070\n\n\n0\ncat__email_text_long_email\n-0.125640\n0.125640\n\n\n21\ncat__hour_18\n-0.124091\n0.124091\n\n\n10\ncat__hour_7\n-0.115255\n0.115255\n\n\n30\ncat__weekday_Saturday\n-0.111147\n0.111147\n\n\n8\ncat__hour_5\n-0.104193\n0.104193\n\n\n16\ncat__hour_13\n-0.072483\n0.072483\n\n\n6\ncat__hour_3\n-0.061108\n0.061108\n\n\n19\ncat__hour_16\n0.031223\n0.031223\n\n\n4\ncat__hour_1\n0.027464\n0.027464\n\n\n11\ncat__hour_8\n-0.018762\n0.018762\n\n\n20\ncat__hour_17\n-0.016271\n0.016271\n\n\n17\ncat__hour_14\n0.002813\n0.002813\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 10))\nsns.barplot(data=feat_importances, x='importances', y='name', ax=ax)\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#feature-importance-random-forest",
    "href": "posts/2022-12-06-data7-email/email7.html#feature-importance-random-forest",
    "title": "Data challenge - 7. email",
    "section": "Feature importance: random forest",
    "text": "Feature importance: random forest\n\nfeat_names = X_val.columns\nfeat_coefficients = search.best_estimator_[-1].feature_importances_\n\nfeat_importances = pd.DataFrame({\"name\":feat_names,\"coef\":feat_coefficients})\nfeat_importances = feat_importances[['name','coef']]# reorder the columns\nfeat_importances['importances'] = np.abs(feat_importances['coef'])\nfeat_importances.sort_values(by=\"importances\",inplace=True,ascending=False)\nfeat_importances\n\n\n\n\n\n\n\n\nname\ncoef\nimportances\n\n\n\n\n5\nuser_past_purchases\n0.606479\n0.606479\n\n\n4\nuser_country\n0.114434\n0.114434\n\n\n1\nemail_version\n0.095657\n0.095657\n\n\n2\nhour\n0.093785\n0.093785\n\n\n3\nweekday\n0.068505\n0.068505\n\n\n0\nemail_text\n0.021139\n0.021139\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 8))\nsns.barplot(data=feat_importances, x='importances', y='name', ax=ax)\nplt.show()\n\n\n\n\n\n# import eli5\n# from eli5.sklearn import PermutationImportance\n#\n# # Make a small change to the code below to use in this problem.\n# perm = PermutationImportance(search.best_estimator_, random_state=1).fit(X_val, y_val)\n#\n# # uncomment the following line to visualize your results\n# eli5.show_weights(perm, feature_names = X_val.columns.tolist())\n\n\n# import shap  # package used to calculate Shap values\n#\n# # Create object that can calculate shap values\n# explainer = shap.TreeExplainer(search.best_estimator_)\n#\n# # Calculate Shap values\n# shap_values = explainer.shap_values(val_X)\n# shap.initjs()\n# shap.force_plot(explainer.expected_value[1], shap_values[1], val_X)"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#featuer-selection",
    "href": "posts/2022-12-06-data7-email/email7.html#featuer-selection",
    "title": "Data challenge - 7. email",
    "section": "Featuer selection",
    "text": "Featuer selection\n\nfrom scipy.stats import f_oneway  # not recommended\n\nsamples = [group[1]['clicked'].tolist() for group in df.groupby('hour')]\n\nf_val, p_val = f_oneway(*samples)\nprint(f'f_val: {f_val}; p_val: {p_val}')\n\nf_val: 3.9104974606797516; p_val: 7.358968701228666e-10\n\n\n\nfrom sklearn.feature_selection import chi2,f_classif\n\nchi2scores,_ = chi2(OrdinalEncoder().fit_transform(X_train),y_train)\nfscores,_ = f_classif(OrdinalEncoder().fit_transform(X_train),y_train)\n\n\nfeat_scores = pd.DataFrame({\"chi2scores\":chi2scores,\"fscores\":fscores},index=X_train.columns)\n\n\nfeat_scores.sort_values(by='chi2scores',ascending=False)\n\n\n\n\n\n\n\n\nchi2scores\nfscores\n\n\n\n\nuser_past_purchases\n2240.138349\n862.838404\n\n\nemail_version\n68.465083\n136.745883\n\n\nweekday\n51.794282\n38.929001\n\n\nuser_country\n49.664494\n114.161066\n\n\nhour\n22.095027\n9.051159\n\n\nemail_text\n11.373098\n22.635954\n\n\n\n\n\n\n\nhigher the Chi-Square value the feature is more dependent on the response and it can be selected for model training\n\nfeat_scores.sort_values(by=\"fscores\",ascending=False)\n\n\n\n\n\n\n\n\nchi2scores\nfscores\n\n\n\n\nuser_past_purchases\n2240.138349\n862.838404\n\n\nemail_version\n68.465083\n136.745883\n\n\nuser_country\n49.664494\n114.161066\n\n\nweekday\n51.794282\n38.929001\n\n\nemail_text\n11.373098\n22.635954\n\n\nhour\n22.095027\n9.051159\n\n\n\n\n\n\n\nFurther the means and smaller the within variances (big f score), better the feature is.\n\ncategorical_feats\n\n['email_text', 'email_version', 'hour', 'weekday', 'user_country']"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#logistic-regression",
    "href": "posts/2022-12-06-data7-email/email7.html#logistic-regression",
    "title": "Data challenge - 7. email",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nX_train, X_val, y_train, y_val = train_test_split(df.drop(columns=['opened', 'clicked', 'hour']), df['clicked'], random_state=42)\n\ncategorical_feats = ['email_text', 'email_version', 'weekday', 'user_country']\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe_base = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', LogisticRegression(max_iter=10000, tol=0.01))\n])\n\npipe_base.fit(X_train, y_train)\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(pipe_base, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(pipe_base, X_val, y_val):.4f}\")\n\nTrain ROC-AUC: 0.7405\nTest ROC-AUC: 0.7271"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#random-forest-1",
    "href": "posts/2022-12-06-data7-email/email7.html#random-forest-1",
    "title": "Data challenge - 7. email",
    "section": "Random forest",
    "text": "Random forest\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestClassifier())\n])\n\n\nparam_grid = {\n    \"model__n_estimators\": [25, 50, 100],\n    'model__max_depth': [6]\n}\n\nsearch = GridSearchCV(pipe, param_grid, n_jobs=-1, scoring='roc_auc')\nsearch.fit(X_train, y_train)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\nprint(f\"Train ROC-AUC: {calculate_roc_auc(search.best_estimator_, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(search.best_estimator_, X_val, y_val):.4f}\")\n\nBest parameter (CV score=0.736):\n{'model__max_depth': 6, 'model__n_estimators': 100}\nTrain ROC-AUC: 0.7557\nTest ROC-AUC: 0.7176\n\n\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestClassifier())\n])\n\n\nparam_grid = {\n    \"model__n_estimators\": [25, 50, 100],\n    'model__max_depth': [6, 7, 9]\n}\n\nsearch = GridSearchCV(pipe, param_grid, n_jobs=-1, scoring='roc_auc')\nsearch.fit(X_train, y_train)\n\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\nprint(f\"Train ROC-AUC: {calculate_roc_auc(search.best_estimator_, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(search.best_estimator_, X_val, y_val):.4f}\")\n\nBest parameter (CV score=0.741):\n{'model__max_depth': 6, 'model__n_estimators': 25}\nTrain ROC-AUC: 0.7564\nTest ROC-AUC: 0.7292"
  },
  {
    "objectID": "posts/2022-12-06-data7-email/email7.html#feature-engineering-does-not-help-much",
    "href": "posts/2022-12-06-data7-email/email7.html#feature-engineering-does-not-help-much",
    "title": "Data challenge - 7. email",
    "section": "Feature engineering does not help much",
    "text": "Feature engineering does not help much\n\nfrom sklearn.metrics import f1_score\n\nprint(f'f1 score of base model: {f1_score(y_val, pipe_base.predict(X_val))}')\nprint(f'f1 score of random forest model: {f1_score(y_val, search.best_estimator_.predict(X_val))}')\n\nf1 score of base model: 0.0\nf1 score of random forest model: 0.0"
  },
  {
    "objectID": "posts/2022-12-07-data10-credit-card/credit_card10.html",
    "href": "posts/2022-12-07-data10-credit-card/credit_card10.html",
    "title": "Data challenge - 10. credit card",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\ncc_info = pd.read_csv('cc_info.csv')\ncc_info.head()\n\n\n\n\n\n\n\n\ncredit_card\ncity\nstate\nzipcode\ncredit_card_limit\n\n\n\n\n0\n1280981422329509\nDallas\nPA\n18612\n6000\n\n\n1\n9737219864179988\nHouston\nPA\n15342\n16000\n\n\n2\n4749889059323202\nAuburn\nMA\n1501\n14000\n\n\n3\n9591503562024072\nOrlando\nWV\n26412\n18000\n\n\n4\n2095640259001271\nNew York\nNY\n10001\n20000\ntransactions = pd.read_csv('transactions.csv')\ntransactions.head()\n\n\n\n\n\n\n\n\ncredit_card\ndate\ntransaction_dollar_amount\nLong\nLat\n\n\n\n\n0\n1003715054175576\n2015-09-11 00:32:40\n43.78\n-80.174132\n40.267370\n\n\n1\n1003715054175576\n2015-10-24 22:23:08\n103.15\n-80.194240\n40.180114\n\n\n2\n1003715054175576\n2015-10-26 18:19:36\n48.55\n-80.211033\n40.313004\n\n\n3\n1003715054175576\n2015-10-22 19:41:10\n136.18\n-80.174138\n40.290895\n\n\n4\n1003715054175576\n2015-10-26 20:08:22\n71.82\n-80.238720\n40.166719\ntransactions.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 294588 entries, 0 to 294587\nData columns (total 5 columns):\n #   Column                     Non-Null Count   Dtype  \n---  ------                     --------------   -----  \n 0   credit_card                294588 non-null  int64  \n 1   date                       294588 non-null  object \n 2   transaction_dollar_amount  294588 non-null  float64\n 3   Long                       294588 non-null  float64\n 4   Lat                        294588 non-null  float64\ndtypes: float64(3), int64(1), object(1)\nmemory usage: 11.2+ MB"
  },
  {
    "objectID": "posts/2022-12-07-data10-credit-card/credit_card10.html#feature-of-unusual-transaction",
    "href": "posts/2022-12-07-data10-credit-card/credit_card10.html#feature-of-unusual-transaction",
    "title": "Data challenge - 10. credit card",
    "section": "Feature of unusual transaction",
    "text": "Feature of unusual transaction\n\ntransaction_dollar_amount_stat = transactions.groupby('credit_card')['transaction_dollar_amount'].quantile([0.25, 0.5, 0.75]).unstack().reset_index()  # unstack hierarchical index\ntransaction_dollar_amount_stat\n\n\n\n\n\n\n\n\ncredit_card\n0.25\n0.5\n0.75\n\n\n\n\n0\n1003715054175576\n59.0100\n93.730\n124.630\n\n\n1\n1013870087888817\n58.0900\n87.670\n128.020\n\n\n2\n1023820165155391\n55.4300\n85.780\n122.830\n\n\n3\n1073931538936472\n108.0800\n147.190\n194.830\n\n\n4\n1077622576192810\n122.9300\n148.180\n160.110\n\n\n...\n...\n...\n...\n...\n\n\n979\n9958678964376192\n91.3100\n145.300\n201.560\n\n\n980\n9961694231875562\n43.5500\n70.750\n104.100\n\n\n981\n9981251982982618\n75.5375\n132.615\n186.445\n\n\n982\n9986135779184360\n31.6250\n51.320\n72.060\n\n\n983\n9999757432802760\n83.7400\n130.940\n177.905\n\n\n\n\n984 rows × 4 columns\n\n\n\n\ntemp = pd.merge(cc_info, transaction_dollar_amount_stat, how='left', on='credit_card')\nq3 = pd.merge(temp, transactions, how='left', on='credit_card')\n\n\nq3\n\n\n\n\n\n\n\n\ncredit_card\ncity\nstate\nzipcode\ncredit_card_limit\n0.25\n0.5\n0.75\ndate\ntransaction_dollar_amount\nLong\nLat\nmonth\nday\n\n\n\n\n0\n1280981422329509\nDallas\nPA\n18612\n6000\n23.6700\n34.37\n45.3150\n2015-08-05 00:59:19\n11.94\n-75.964527\n41.353578\n8\n5\n\n\n1\n1280981422329509\nDallas\nPA\n18612\n6000\n23.6700\n34.37\n45.3150\n2015-10-29 18:23:04\n5.76\n-76.019703\n41.311467\n10\n29\n\n\n2\n1280981422329509\nDallas\nPA\n18612\n6000\n23.6700\n34.37\n45.3150\n2015-10-25 17:50:48\n25.84\n-76.043031\n41.291053\n10\n25\n\n\n3\n1280981422329509\nDallas\nPA\n18612\n6000\n23.6700\n34.37\n45.3150\n2015-09-05 17:39:43\n68.89\n-75.944299\n41.327282\n9\n5\n\n\n4\n1280981422329509\nDallas\nPA\n18612\n6000\n23.6700\n34.37\n45.3150\n2015-09-04 14:12:59\n27.01\n-75.997259\n41.352099\n9\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n294583\n1409322756311484\nHouston\nPA\n15342\n15000\n40.4575\n58.73\n84.4525\n2015-09-24 19:18:40\n98.47\n-80.238676\n40.228136\n9\n24\n\n\n294584\n1409322756311484\nHouston\nPA\n15342\n15000\n40.4575\n58.73\n84.4525\n2015-08-24 12:36:52\n38.07\n-80.178443\n40.247585\n8\n24\n\n\n294585\n1409322756311484\nHouston\nPA\n15342\n15000\n40.4575\n58.73\n84.4525\n2015-09-04 19:04:01\n56.20\n-80.211523\n40.222771\n9\n4\n\n\n294586\n1409322756311484\nHouston\nPA\n15342\n15000\n40.4575\n58.73\n84.4525\n2015-08-15 00:52:23\n60.44\n-80.145914\n40.251028\n8\n15\n\n\n294587\n1409322756311484\nHouston\nPA\n15342\n15000\n40.4575\n58.73\n84.4525\n2015-10-03 20:26:57\n58.84\n-80.148290\n40.224969\n10\n3\n\n\n\n\n294588 rows × 14 columns\n\n\n\n\nq3 = q3[['credit_card_limit', 0.25, 0.5, 0.75, 'transaction_dollar_amount']]\nq3\n\n\n\n\n\n\n\n\ncredit_card_limit\n0.25\n0.5\n0.75\ntransaction_dollar_amount\n\n\n\n\n0\n6000\n23.6700\n34.37\n45.3150\n11.94\n\n\n1\n6000\n23.6700\n34.37\n45.3150\n5.76\n\n\n2\n6000\n23.6700\n34.37\n45.3150\n25.84\n\n\n3\n6000\n23.6700\n34.37\n45.3150\n68.89\n\n\n4\n6000\n23.6700\n34.37\n45.3150\n27.01\n\n\n...\n...\n...\n...\n...\n...\n\n\n294583\n15000\n40.4575\n58.73\n84.4525\n98.47\n\n\n294584\n15000\n40.4575\n58.73\n84.4525\n38.07\n\n\n294585\n15000\n40.4575\n58.73\n84.4525\n56.20\n\n\n294586\n15000\n40.4575\n58.73\n84.4525\n60.44\n\n\n294587\n15000\n40.4575\n58.73\n84.4525\n58.84\n\n\n\n\n294588 rows × 5 columns"
  },
  {
    "objectID": "posts/2022-12-07-data10-credit-card/credit_card10.html#reduce-dimensions-and-visualize",
    "href": "posts/2022-12-07-data10-credit-card/credit_card10.html#reduce-dimensions-and-visualize",
    "title": "Data challenge - 10. credit card",
    "section": "reduce dimensions and visualize",
    "text": "reduce dimensions and visualize\n\nq3.describe()\n\n\n\n\n\n\n\n\ncredit_card_limit\n0.25\n0.5\n0.75\ntransaction_dollar_amount\n\n\n\n\ncount\n294588.000000\n294588.000000\n294588.000000\n294588.000000\n294588.000000\n\n\nmean\n15502.053716\n43.265113\n69.081697\n97.282645\n86.008036\n\n\nstd\n7956.291556\n21.471896\n35.160028\n49.919646\n124.655954\n\n\nmin\n2000.000000\n6.150000\n7.320000\n9.120000\n0.010000\n\n\n25%\n10000.000000\n27.960000\n43.590000\n60.812500\n29.970000\n\n\n50%\n15000.000000\n40.595000\n65.190000\n91.832500\n58.470000\n\n\n75%\n20000.000000\n56.215000\n89.710000\n127.345000\n100.400000\n\n\nmax\n55000.000000\n137.690000\n222.290000\n318.730000\n999.970000\n\n\n\n\n\n\n\nI will use PCA to reduce dimensions to visualize, the first step is to standardize data.\n\n# standardize data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = scaler.fit_transform(q3)\n\n/Users/haoqiwang/.conda/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['float', 'str']. An error will be raised in 1.2.\n  warnings.warn(\n/Users/haoqiwang/.conda/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['float', 'str']. An error will be raised in 1.2.\n  warnings.warn(\n\n\n\n# use PCA to reduce dimensions\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n#pca.fit(X)\nX_pca = pca.fit_transform(X)\nX_pca = pd.DataFrame(X_pca, columns=['pc1', 'pc2'])\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X_pca.pc1, X_pca.pc2)\n\n&lt;matplotlib.collections.PathCollection at 0x7f9f25ab24d0&gt;"
  },
  {
    "objectID": "posts/2022-12-07-data10-credit-card/credit_card10.html#cluster",
    "href": "posts/2022-12-07-data10-credit-card/credit_card10.html#cluster",
    "title": "Data challenge - 10. credit card",
    "section": "Cluster",
    "text": "Cluster\nabove plot shows data is well seperated. also it shows the data may be grouped to 6 clusters. try k-means clustering\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=6)\nkmeans.fit(X)\n# kmeans not working, do not know why\n\n# from sklearn.cluster import SpectralClustering\n#\n# clustering = SpectralClustering(n_clusters=6, assign_labels='discretize', random_state=0).fit(X)\n\nKMeans(n_clusters=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=6)\n\n\n\nimport numpy as np\n\n#Getting unique labels\nlabels = kmeans.labels_\nu_labels = np.unique(labels)\n\n#plotting the results:\n\nfor i in u_labels:\n    plt.scatter(X[labels == i , 0] , X[labels == i , 1] , label = i)\nplt.legend()\nplt.show()\n\n\n\n\n\nfor i in u_labels:\n    plt.scatter(X_pca.pc1[labels == i], X_pca.pc2[labels == i] , label = i)\nplt.legend()\nplt.show()\n\n\n\n\nlook at each cluster’s label, for the label with least examples, join them to %75 table, see it they are much higher\nlook at each cluster’s label, for the label with least examples, join them to %75 table, see it they are much higher\n\ncc_info.head()\n\n\n\n\n\n\n\n\ncredit_card\ncity\nstate\nzipcode\ncredit_card_limit\n\n\n\n\n0\n1280981422329509\nDallas\nPA\n18612\n6000\n\n\n1\n9737219864179988\nHouston\nPA\n15342\n16000\n\n\n2\n4749889059323202\nAuburn\nMA\n1501\n14000\n\n\n3\n9591503562024072\nOrlando\nWV\n26412\n18000\n\n\n4\n2095640259001271\nNew York\nNY\n10001\n20000\n\n\n\n\n\n\n\n\ntransactions.head()\n\n\n\n\n\n\n\n\ncredit_card\ndate\ntransaction_dollar_amount\nLong\nLat\nmonth\nday\n\n\n\n\n0\n1003715054175576\n2015-09-11 00:32:40\n43.78\n-80.174132\n40.267370\n9\n11\n\n\n1\n1003715054175576\n2015-10-24 22:23:08\n103.15\n-80.194240\n40.180114\n10\n24\n\n\n2\n1003715054175576\n2015-10-26 18:19:36\n48.55\n-80.211033\n40.313004\n10\n26\n\n\n3\n1003715054175576\n2015-10-22 19:41:10\n136.18\n-80.174138\n40.290895\n10\n22\n\n\n4\n1003715054175576\n2015-10-26 20:08:22\n71.82\n-80.238720\n40.166719\n10\n26\n\n\n\n\n\n\n\n\ntransactions.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 294588 entries, 0 to 294587\nData columns (total 7 columns):\n #   Column                     Non-Null Count   Dtype         \n---  ------                     --------------   -----         \n 0   credit_card                294588 non-null  int64         \n 1   date                       294588 non-null  datetime64[ns]\n 2   transaction_dollar_amount  294588 non-null  float64       \n 3   Long                       294588 non-null  float64       \n 4   Lat                        294588 non-null  float64       \n 5   month                      294588 non-null  int64         \n 6   day                        294588 non-null  int64         \ndtypes: datetime64[ns](1), float64(3), int64(3)\nmemory usage: 15.7 MB\n\n\n\ntransactions.head()\n\n\n\n\n\n\n\n\ncredit_card\ndate\ntransaction_dollar_amount\nLong\nLat\nmonth\nday\n\n\n\n\n0\n1003715054175576\n2015-09-11 00:32:40\n43.78\n-80.174132\n40.267370\n9\n11\n\n\n1\n1003715054175576\n2015-10-24 22:23:08\n103.15\n-80.194240\n40.180114\n10\n24\n\n\n2\n1003715054175576\n2015-10-26 18:19:36\n48.55\n-80.211033\n40.313004\n10\n26\n\n\n3\n1003715054175576\n2015-10-22 19:41:10\n136.18\n-80.174138\n40.290895\n10\n22\n\n\n4\n1003715054175576\n2015-10-26 20:08:22\n71.82\n-80.238720\n40.166719\n10\n26"
  },
  {
    "objectID": "posts/2021-08-15-bash/index.html",
    "href": "posts/2021-08-15-bash/index.html",
    "title": "Bash",
    "section": "",
    "text": "2021-11-01 update\nUT Advanced Bash Scripting: https://wikis.utexas.edu/display/CbbShortBashScript/Advanced+Bash+Scripting+home\n\n\nTerminal shortcuts\nCtrl - C, cancel\nCtrl - A, move to beginning line\nCtrl - E, move to end line\nCtrl - left arrow, move to left one word\nCtrl - right arrow, move to right one word\n() parentheses\n{} braces\n[] brackets\n\n\nCommands\nopen folder/file\necho\nman\ncd # go back to home folder\nls -R # look at folder structure\nls -l list more details\nmkdir\nrmdir\ncp\ncp –r patient21 patient22 # copy folder and contents\nmv\nrm\nrm -r # recurrence, remove all\ncat # print content of file\ntouch # create blank file\nnano # open in text editor, Ctrl - O, save file, Ctrl - X, exit file\nchown # change ownership\nln -s # soft link\nwc # word count\ngrep # search for text\nawk, sed # extract data, change data\ntar # tape archive\nzip, unzip\ndeclare -r # cannot be modified\ndeclare -l # lowercase string\ndeclare -u # uppercase string\n~ tilde expansion\n\necho ~\necho ~-\n\n/Users/haoqiwang\n~-\n\n\n$((...)) arithmetic expansion, return results\n\necho $((4+3))\necho $((4 * 3)) # spaces do not matter\n\n7\n12\n\n\n$(...) command substitution, use output of one command inside another\n\nuname -r\necho \"The kernel is $(uname -r).\"\n\n21.6.0\nThe kernel is 21.6.0.\n\n\n${...} parameter expansion, retrieve stored values\n\nanimal=\"dog\"\necho animal # treat as string\necho \"animal\"\necho $animal\necho ${animal} # ${ } parameter expansion, retrieve stored values\necho ${animal:2}\n\nanimal\nanimal\ndog\ndog\ng\n\n\n{...} brace expansion, create set of ranges\n\necho {1,2,3}\necho {1, 2, 3} # cannot exist spaces\n\n1 2 3\n{1, 2, 3}\n\n\n\necho {1..10}\necho {1..20..3}\necho {10..2}\necho {a..z}\n\n1 2 3 4 5 6 7 8 9 10\n{1..20..3}\n10 9 8 7 6 5 4 3 2\na b c d e f g h i j k l m n o p q r s t u v w x y z\n\n\n\necho {cat,dog}_{1..3}\n\ncat_1 cat_2 cat_3 dog_1 dog_2 dog_3\n\n\n[...], [[...]] test\n[\"cat\" = \"dog\"]; echo $?\n\n\nBash script\nfirst line - shebang, hash and bang\n\n#!/usr/bin/env bash\n\n2 types of arrays\n\n# indexed array\ndeclare -a snacks=(\"apple\" \"banana\" \"orange\")\necho ${snacks[2]}\necho ${snacks[@]}\n\norange\napple banana orange\n\n\n# associative array / dictionary\ndeclare -A \n\necho -e \"\\033[31mHello\\033[0m\" # The output is supposed to be red\n\n\u001b[31mHello\u001b[0m\n\n\nvariable is special case of parameter substitution\nsingle quotes prevent Bash from performing the command substitution\n\necho The kernel release is $(uname -r).\necho \"The kernel release is $(uname -r)\".\necho 'The kernel release is $(uname -r).'\n\nThe kernel release is 21.6.0.\nThe kernel release is 21.6.0.\nThe kernel release is $(uname -r).\n\n\nif\n\ndeclare -i a=3\n\nif (($a &gt; 4)); then # ((...)), arithmetic evaluation, perform calculation\n    echo \"$a is greater than 4.\" \nelif (($a &gt; 2)); then\n    echo \"$a is greater than 2.\"\nelse\n    echo \"$a is not greater than 4.\"\nfi\n\n3 is greater than 2.\n\n\nwhile, until loop\n\ndeclare -i m=0 # no spaces before and after \"=\"\nuntil ((m == 10)) # if the condition is not true\ndo\n    echo \"m:$m\"\n    ((m++))\n    sleep 1\ndone\n\nm:0\nm:1\nm:2\nm:3\nm:4\nm:5\nm:6\nm:7\nm:8\nm:9\n\n\nfor loop\n\nfor i in 1 2 3\ndo\n    echo $i\ndone\n\nfor i in 1 2 3; do echo $i; done\n\n1\n2\n3\n1\n2\n3\n\n\n\nfor i in {1..9} # { }, brace expansion, create set of ranges\ndo\n    echo $i\ndone\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\nfor ((i=1; i&lt;=9; i++))\ndo\n    echo $i\ndone\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\ndeclare -a fruits=(\"apple\", \"banana\", \"cherry\")\nfor i in ${fruits[@]}\ndo\n    echo \"Today's fruit is: $i\"\ndone\n\nToday's fruit is: apple,\nToday's fruit is: banana,\nToday's fruit is: cherry\n\n\n\nfor i in $(ls)\ndo\n    echo \"Found a file: $i\"\ndone\n\nFound a file: index.Rmd\nFound a file: index.rmarkdown\n\n\n\nanimal=\"dog\"\n\ncase $animal in\n    cat) echo \"Feline\";;\n    dog|puppy) echo \"Canine\";;\n    *) echo \"No match!\"\nesac\n\nCanine\n\n\nfunction\n\ngreet() {\n    echo \"Hello there, $1. Good $2.\" # $1, first argument passed\n}\n\necho \"And now, a greeting...\"\ngreet Haoqi Morning\n\nAnd now, a greeting...\nHello there, Haoqi. Good Morning.\n\n\n\nnumberthing() {\n    declare -i i=1\n    for f in $@; do # $@ is the list of arguments given to function\n        echo \"$i: $f\"\n        ((i++))\n    done\n}\n\nnumberthing $(ls /)\n\n1: Applications\n2: Library\n3: System\n4: Users\n5: Volumes\n6: bin\n7: cores\n8: dev\n9: etc\n10: home\n11: opt\n12: private\n13: sbin\n14: tmp\n15: usr\n16: var\n\n\n\nvar1=\"I'm variable 1\" # all variables in bash are global\n\nmyfunction() {\n    var2=\"I'm variable 2\"\n    local var3=\"I'm variable 3\" # local variable, only exist inside a function\n}\nmyfunction\necho $var1\necho $var2\necho $var3\n\nI'm variable 1\nI'm variable 2\n\n\narguments and options\nwhile getopts :u:p:ab option; do\n    case $option in\n        u) user=$OPTARG;;\n        p) pass=$OPTARG;;\n        a) echo \"got the A flag\";;\n        b) echo got the B flag;;\n        ?) echo \"I don't know what $OPTARG is!\";;\n    esac\ndone\n\necho \"user: $user / pass: $pass\"\nget inputs, another option is select\necho \"What is your name?\"\nread name\necho \"What is your password?\"\nread -s pass\nread -p \"What's your favorite animal? \" animal\n\necho name: $name, pass: $pass, animal: $animal\n\nread -ep \"Favorite color? \" -i \"Blue\" favcolor # set default"
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html",
    "title": "Data challenge - 4. fraud",
    "section": "",
    "text": "The goal of this challenge is to build a machine learning model that predicts the probability that the first transaction of a new user is fraudulent.\nYou only have information about the user first transaction on the site and based on that you have to make your classification (“fraud/no fraud”)."
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html#feature-engineering",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html#feature-engineering",
    "title": "Data challenge - 4. fraud",
    "section": "Feature engineering",
    "text": "Feature engineering\n\ndf = df_fraud.copy()\n\n\nTime\n\ndf.signup_time = pd.to_datetime(df.signup_time)\ndf.purchase_time = pd.to_datetime(df.purchase_time)\ndf['interval_after_signup'] = df.purchase_time - df.signup_time\ndf['interval_after_signup'] = df['interval_after_signup'].dt.total_seconds()\n\n\ndf\n\n\n\n\n\n\n\n\nuser_id\nsignup_time\npurchase_time\npurchase_value\ndevice_id\nsource\nbrowser\nsex\nage\nip_address\nclass\ncountry\ninterval_after_signup\n\n\n\n\n1\n22058\n2015-02-24 22:55:49\n2015-04-18 02:47:11\n34\nQVPSPJUOCKZAR\nSEO\nChrome\nM\n39\n732758368\n0\nJapan\n4506682.0\n\n\n2\n333320\n2015-06-07 20:39:50\n2015-06-08 01:38:54\n16\nEOGFQPIZPYXFZ\nAds\nChrome\nF\n53\n350311387\n0\nUnited States\n17944.0\n\n\n3\n1359\n2015-01-01 18:52:44\n2015-01-01 18:52:45\n15\nYSSKYOSJHPPLJ\nSEO\nOpera\nM\n53\n2621473820\n1\nUnited States\n1.0\n\n\n4\n150084\n2015-04-28 21:13:25\n2015-05-04 13:54:50\n44\nATGTXKYKUDUQN\nSEO\nSafari\nM\n41\n3840542443\n0\nUnknown\n492085.0\n\n\n5\n221365\n2015-07-21 07:09:52\n2015-09-09 18:40:53\n39\nNAUITBZFJKHWW\nAds\nSafari\nM\n45\n415583117\n0\nUnited States\n4361461.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n151108\n345170\n2015-01-27 03:03:34\n2015-03-29 00:30:47\n43\nXPSKTWGPWINLR\nSEO\nChrome\nM\n28\n3451154527\n1\nUnited States\n5261233.0\n\n\n151109\n274471\n2015-05-15 17:43:29\n2015-05-26 12:24:39\n35\nLYSFABUCPCGBA\nSEO\nSafari\nM\n32\n2439047221\n0\nNetherlands\n931270.0\n\n\n151110\n368416\n2015-03-03 23:07:31\n2015-05-20 07:07:47\n40\nMEQHCSJUBRBFE\nSEO\nIE\nF\n26\n2748470524\n0\nJapan\n6681616.0\n\n\n151111\n207709\n2015-07-09 20:06:07\n2015-09-07 09:34:46\n46\nCMCXFGRHYSTVJ\nSEO\nChrome\nM\n37\n3601174708\n0\nUnited States\n5146119.0\n\n\n151112\n138208\n2015-06-10 07:02:20\n2015-07-21 02:03:53\n20\nZINIADFCLHYPG\nDirect\nIE\nM\n38\n4103824511\n0\nUnknown\n3524493.0\n\n\n\n\n151112 rows × 13 columns\n\n\n\n\n\nDevice\n\ndevice_count = df.device_id.value_counts().reset_index()\ndevice_count = device_count.rename(columns={'index': 'device_id', 'device_id': 'device_id_count'})\ndf = pd.merge(df, device_count, how='left', on='device_id')\n\n\ndf\n\n\n\n\n\n\n\n\nuser_id\nsignup_time\npurchase_time\npurchase_value\ndevice_id\nsource\nbrowser\nsex\nage\nip_address\nclass\ncountry\ninterval_after_signup\ndevice_id_count\n\n\n\n\n0\n22058\n2015-02-24 22:55:49\n2015-04-18 02:47:11\n34\nQVPSPJUOCKZAR\nSEO\nChrome\nM\n39\n732758368\n0\nJapan\n4506682.0\n1\n\n\n1\n333320\n2015-06-07 20:39:50\n2015-06-08 01:38:54\n16\nEOGFQPIZPYXFZ\nAds\nChrome\nF\n53\n350311387\n0\nUnited States\n17944.0\n1\n\n\n2\n1359\n2015-01-01 18:52:44\n2015-01-01 18:52:45\n15\nYSSKYOSJHPPLJ\nSEO\nOpera\nM\n53\n2621473820\n1\nUnited States\n1.0\n12\n\n\n3\n150084\n2015-04-28 21:13:25\n2015-05-04 13:54:50\n44\nATGTXKYKUDUQN\nSEO\nSafari\nM\n41\n3840542443\n0\nUnknown\n492085.0\n1\n\n\n4\n221365\n2015-07-21 07:09:52\n2015-09-09 18:40:53\n39\nNAUITBZFJKHWW\nAds\nSafari\nM\n45\n415583117\n0\nUnited States\n4361461.0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n151107\n345170\n2015-01-27 03:03:34\n2015-03-29 00:30:47\n43\nXPSKTWGPWINLR\nSEO\nChrome\nM\n28\n3451154527\n1\nUnited States\n5261233.0\n2\n\n\n151108\n274471\n2015-05-15 17:43:29\n2015-05-26 12:24:39\n35\nLYSFABUCPCGBA\nSEO\nSafari\nM\n32\n2439047221\n0\nNetherlands\n931270.0\n1\n\n\n151109\n368416\n2015-03-03 23:07:31\n2015-05-20 07:07:47\n40\nMEQHCSJUBRBFE\nSEO\nIE\nF\n26\n2748470524\n0\nJapan\n6681616.0\n1\n\n\n151110\n207709\n2015-07-09 20:06:07\n2015-09-07 09:34:46\n46\nCMCXFGRHYSTVJ\nSEO\nChrome\nM\n37\n3601174708\n0\nUnited States\n5146119.0\n2\n\n\n151111\n138208\n2015-06-10 07:02:20\n2015-07-21 02:03:53\n20\nZINIADFCLHYPG\nDirect\nIE\nM\n38\n4103824511\n0\nUnknown\n3524493.0\n1\n\n\n\n\n151112 rows × 14 columns\n\n\n\n\n\nIP\n\nip_count = df.ip_address.value_counts().reset_index()\nip_count = ip_count.rename(columns={'index': 'ip_address', 'ip_address': 'ip_address_count'})\ndf = pd.merge(df, ip_count, how='left', on='ip_address')\n\n\ndf\n\n\n\n\n\n\n\n\nuser_id\nsignup_time\npurchase_time\npurchase_value\ndevice_id\nsource\nbrowser\nsex\nage\nip_address\nclass\ncountry\ninterval_after_signup\ndevice_id_count\nip_address_count\n\n\n\n\n0\n22058\n2015-02-24 22:55:49\n2015-04-18 02:47:11\n34\nQVPSPJUOCKZAR\nSEO\nChrome\nM\n39\n732758368\n0\nJapan\n4506682.0\n1\n1\n\n\n1\n333320\n2015-06-07 20:39:50\n2015-06-08 01:38:54\n16\nEOGFQPIZPYXFZ\nAds\nChrome\nF\n53\n350311387\n0\nUnited States\n17944.0\n1\n1\n\n\n2\n1359\n2015-01-01 18:52:44\n2015-01-01 18:52:45\n15\nYSSKYOSJHPPLJ\nSEO\nOpera\nM\n53\n2621473820\n1\nUnited States\n1.0\n12\n12\n\n\n3\n150084\n2015-04-28 21:13:25\n2015-05-04 13:54:50\n44\nATGTXKYKUDUQN\nSEO\nSafari\nM\n41\n3840542443\n0\nUnknown\n492085.0\n1\n1\n\n\n4\n221365\n2015-07-21 07:09:52\n2015-09-09 18:40:53\n39\nNAUITBZFJKHWW\nAds\nSafari\nM\n45\n415583117\n0\nUnited States\n4361461.0\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n151107\n345170\n2015-01-27 03:03:34\n2015-03-29 00:30:47\n43\nXPSKTWGPWINLR\nSEO\nChrome\nM\n28\n3451154527\n1\nUnited States\n5261233.0\n2\n1\n\n\n151108\n274471\n2015-05-15 17:43:29\n2015-05-26 12:24:39\n35\nLYSFABUCPCGBA\nSEO\nSafari\nM\n32\n2439047221\n0\nNetherlands\n931270.0\n1\n1\n\n\n151109\n368416\n2015-03-03 23:07:31\n2015-05-20 07:07:47\n40\nMEQHCSJUBRBFE\nSEO\nIE\nF\n26\n2748470524\n0\nJapan\n6681616.0\n1\n1\n\n\n151110\n207709\n2015-07-09 20:06:07\n2015-09-07 09:34:46\n46\nCMCXFGRHYSTVJ\nSEO\nChrome\nM\n37\n3601174708\n0\nUnited States\n5146119.0\n2\n1\n\n\n151111\n138208\n2015-06-10 07:02:20\n2015-07-21 02:03:53\n20\nZINIADFCLHYPG\nDirect\nIE\nM\n38\n4103824511\n0\nUnknown\n3524493.0\n1\n1\n\n\n\n\n151112 rows × 15 columns\n\n\n\n\ndf.columns\n\nIndex(['user_id', 'signup_time', 'purchase_time', 'purchase_value',\n       'device_id', 'source', 'browser', 'sex', 'age', 'ip_address', 'class',\n       'country', 'interval_after_signup', 'device_id_count',\n       'ip_address_count'],\n      dtype='object')\n\n\n\ndf.value_counts('class')\n\nclass\n0    136961\n1     14151\ndtype: int64"
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html#split-train-val",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html#split-train-val",
    "title": "Data challenge - 4. fraud",
    "section": "Split train, val",
    "text": "Split train, val\n\ncolumns_to_drop = ['user_id', 'signup_time', 'purchase_time', 'device_id', 'ip_address']\ntarget = 'class'\ndf = df.drop(columns=columns_to_drop)"
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html#todo-deal-with-rare-countries-class-balanced",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html#todo-deal-with-rare-countries-class-balanced",
    "title": "Data challenge - 4. fraud",
    "section": "todo: deal with rare countries, class balanced?",
    "text": "todo: deal with rare countries, class balanced?\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(df.drop(columns=[target]), df[target], random_state=42)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\n\ncategorical_columns = ['source', 'browser', 'sex', 'country']\n\n# encoder\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_columns)], remainder = 'passthrough', verbose_feature_names_out=False)\n\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestClassifier(n_jobs=-1, random_state=0))\n])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['source', 'browser', 'sex',\n                                                   'country'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['source', 'browser', 'sex',\n                                                   'country'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['source', 'browser', 'sex', 'country'])],\n                  verbose_feature_names_out=False)cat['source', 'browser', 'sex', 'country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['purchase_value', 'age', 'interval_after_signup', 'device_id_count', 'ip_address_count']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=0)\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef calculate_roc_auc(model_pipe, X, y):\n    \"\"\"Calculate roc auc score.\n\n    Parameters:\n    ===========\n    model_pipe: sklearn model or pipeline\n    X: features\n    y: true target\n    \"\"\"\n    y_proba = model_pipe.predict_proba(X)[:,1]\n    return roc_auc_score(y, y_proba)\n\n\nprint(f\"Train ROC-AUC: {calculate_roc_auc(pipe, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(pipe, X_val, y_val):.4f}\")\n\nTrain ROC-AUC: 1.0000\nTest ROC-AUC: 0.8407\n\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [10, 25, 50, 100] # number of trees in the random forest\nmax_depth = [3, 5, 7]#[int(x) for x in np.linspace(1, 19, num = 10)] # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10] # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\n\nrandom_grid = {'model__n_estimators': n_estimators,\n               'model__max_depth': max_depth,\n               'model__min_samples_split': min_samples_split,\n               'model__min_samples_leaf': min_samples_leaf}\n\n\nrandom_search = RandomizedSearchCV(pipe, random_grid, n_iter=20, n_jobs=-1)\nrandom_search.fit(X_train, y_train)\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['source',\n                                                                                'browser',\n                                                                                'sex',\n                                                                                'country'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestClassifier(n_jobs=-1,\n                                                                     random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [3, 5, 7],\n                                        'model__min_samples_leaf': [1, 3, 4],\n                                        'model__min_samples_split': [2, 6, 10],\n                                        'model__n_estimators': [10, 25, 50,\n                                                                100]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['source',\n                                                                                'browser',\n                                                                                'sex',\n                                                                                'country'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestClassifier(n_jobs=-1,\n                                                                     random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [3, 5, 7],\n                                        'model__min_samples_leaf': [1, 3, 4],\n                                        'model__min_samples_split': [2, 6, 10],\n                                        'model__n_estimators': [10, 25, 50,\n                                                                100]})estimator: PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['source', 'browser', 'sex',\n                                                   'country'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestClassifier(n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['source', 'browser', 'sex', 'country'])],\n                  verbose_feature_names_out=False)cat['source', 'browser', 'sex', 'country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['purchase_value', 'age', 'interval_after_signup', 'device_id_count', 'ip_address_count']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=0)\n\n\n\nprint(random_search.best_params_)\nmodel_pipe = random_search.best_estimator_\nprint(f\"Train ROC-AUC: {calculate_roc_auc(model_pipe, X_train, y_train):.4f}\")\nprint(f\"Test ROC-AUC: {calculate_roc_auc(model_pipe, X_val, y_val):.4f}\")\n\n{'model__n_estimators': 100, 'model__min_samples_split': 10, 'model__min_samples_leaf': 1, 'model__max_depth': 5}\nTrain ROC-AUC: 0.8569\nTest ROC-AUC: 0.8475\n\n\n\npipe.get_params()\n\n{'memory': None,\n 'steps': [('preprocessors',\n   ColumnTransformer(remainder='passthrough',\n                     transformers=[('cat',\n                                    Pipeline(steps=[('encoder',\n                                                     OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                    unknown_value=-1))]),\n                                    ['source', 'browser', 'sex', 'country'])],\n                     verbose_feature_names_out=False)),\n  ('model', RandomForestClassifier(n_jobs=-1, random_state=0))],\n 'verbose': False,\n 'preprocessors': ColumnTransformer(remainder='passthrough',\n                   transformers=[('cat',\n                                  Pipeline(steps=[('encoder',\n                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                  unknown_value=-1))]),\n                                  ['source', 'browser', 'sex', 'country'])],\n                   verbose_feature_names_out=False),\n 'model': RandomForestClassifier(n_jobs=-1, random_state=0),\n 'preprocessors__n_jobs': None,\n 'preprocessors__remainder': 'passthrough',\n 'preprocessors__sparse_threshold': 0.3,\n 'preprocessors__transformer_weights': None,\n 'preprocessors__transformers': [('cat',\n   Pipeline(steps=[('encoder',\n                    OrdinalEncoder(handle_unknown='use_encoded_value',\n                                   unknown_value=-1))]),\n   ['source', 'browser', 'sex', 'country'])],\n 'preprocessors__verbose': False,\n 'preprocessors__verbose_feature_names_out': False,\n 'preprocessors__cat': Pipeline(steps=[('encoder',\n                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                 unknown_value=-1))]),\n 'preprocessors__cat__memory': None,\n 'preprocessors__cat__steps': [('encoder',\n   OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))],\n 'preprocessors__cat__verbose': False,\n 'preprocessors__cat__encoder': OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n 'preprocessors__cat__encoder__categories': 'auto',\n 'preprocessors__cat__encoder__dtype': numpy.float64,\n 'preprocessors__cat__encoder__encoded_missing_value': nan,\n 'preprocessors__cat__encoder__handle_unknown': 'use_encoded_value',\n 'preprocessors__cat__encoder__unknown_value': -1,\n 'model__bootstrap': True,\n 'model__ccp_alpha': 0.0,\n 'model__class_weight': None,\n 'model__criterion': 'gini',\n 'model__max_depth': None,\n 'model__max_features': 'sqrt',\n 'model__max_leaf_nodes': None,\n 'model__max_samples': None,\n 'model__min_impurity_decrease': 0.0,\n 'model__min_samples_leaf': 1,\n 'model__min_samples_split': 2,\n 'model__min_weight_fraction_leaf': 0.0,\n 'model__n_estimators': 100,\n 'model__n_jobs': -1,\n 'model__oob_score': False,\n 'model__random_state': 0,\n 'model__verbose': 0,\n 'model__warm_start': False}"
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html#roc",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html#roc",
    "title": "Data challenge - 4. fraud",
    "section": "ROC",
    "text": "ROC\n\nfrom sklearn.metrics import RocCurveDisplay\n\nfig, ax = plt.subplots()\nRocCurveDisplay.from_estimator(model_pipe, X_train, y_train, ax=ax, name='Train')\nRocCurveDisplay.from_estimator(model_pipe, X_val, y_val, ax=ax, name='Validation')\nax.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve')\nax.axis(\"square\")\nax.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nBased on ROC,\n\nIf false positive (不是fraud, 预测fraud) rate cost higher, we need to minimize false positive rate, we should increase probability threshold (往左下角走), but the true positive rate decreases. choose a cut-off that would give us ture positive rate of ~0.5 and false positive rate almost 0.\nIf false negative (是fraud, 预测not fraud) rate cost higher, we need to maximize true positive rate, we should decrease probability threshold (往右上角走), but the false positive rate increases.\n\nin this case, because normally this “Fraud Detection Model” is often used in a pre-screening step, whose result will be further investigated by expert, so\nif ‘Not Fraud’ is classified as ‘Fraud’, human expert can still have method to fix the problem but if ‘Fraud’ is classified as ‘Not Fraud’, the company will lose money directly. so in this case, “false negative” cost much higher, so we should choose a relatively smaller threshold."
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html#feature-importance",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html#feature-importance",
    "title": "Data challenge - 4. fraud",
    "section": "Feature importance",
    "text": "Feature importance\n\nGini importance\n\nmodel_pipe\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['source', 'browser', 'sex',\n                                                   'country'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 RandomForestClassifier(max_depth=7, min_samples_leaf=4,\n                                        min_samples_split=10, n_estimators=25,\n                                        n_jobs=-1, random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['source', 'browser', 'sex',\n                                                   'country'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 RandomForestClassifier(max_depth=7, min_samples_leaf=4,\n                                        min_samples_split=10, n_estimators=25,\n                                        n_jobs=-1, random_state=0))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['source', 'browser', 'sex', 'country'])],\n                  verbose_feature_names_out=False)cat['source', 'browser', 'sex', 'country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainder['purchase_value', 'age', 'interval_after_signup', 'device_id_count', 'ip_address_count']passthroughpassthroughRandomForestClassifierRandomForestClassifier(max_depth=7, min_samples_leaf=4, min_samples_split=10,\n                       n_estimators=25, n_jobs=-1, random_state=0)\n\n\n\nfeature_names = model_pipe[:-1].get_feature_names_out()\nimportances = model_pipe[-1].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_pipe[-1].estimators_], axis=0)\n\nfeature_importances = pd.Series(importances, index=feature_names)\n# feature_importances = pd.DataFrame({\"name\": feature_names,\"importance\": importances, \"importance_std\": std})\n\n# feature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances\n\nsource                   0.002694\nbrowser                  0.000378\nsex                      0.000234\ncountry                  0.000880\npurchase_value           0.001125\nage                      0.001109\ninterval_after_signup    0.553868\ndevice_id_count          0.151445\nip_address_count         0.288267\ndtype: float64\n\n\n\nfig, ax = plt.subplots(figsize=(5, 8))\n\nfeature_importances.plot.barh(xerr=std, ax=ax)\nax.set_title(\"Feature importances using Gini impurity\")\nax.set_ylabel(\"Mean decrease in impurity\")\nplt.show()\n# fig.tight_layout()\n\n\n\n\n\n# fig, ax = plt.subplots(figsize=(5, 8))\n# sns.barplot(data=feature_importances, x='importance', y='name',\n#             errorbar=(feature_importances.importance-feature_importances.importance_std,\n#                       feature_importances.importance+feature_importances.importance_std), ax=ax)\n# # feature_importances.plot.barh(ax=ax)\n#\n# plt.show()\n\n\n\nPermutation importance\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    model_pipe, X_train, y_train, scoring='roc_auc', n_repeats=30, random_state=42, n_jobs=-1\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\n\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=feature_names\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in score\")\nax.figure.tight_layout()\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    model_pipe, X_val, y_val, scoring='roc_auc', n_repeats=30, random_state=42, n_jobs=-1\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\n\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=feature_names\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (validation set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in score\")\nax.figure.tight_layout()"
  },
  {
    "objectID": "posts/2022-12-11-data4-fraud/fraud4.html#partial-dependence",
    "href": "posts/2022-12-11-data4-fraud/fraud4.html#partial-dependence",
    "title": "Data challenge - 4. fraud",
    "section": "Partial dependence",
    "text": "Partial dependence\n\nX_train.columns\n\nIndex(['purchase_value', 'source', 'browser', 'sex', 'age', 'country',\n       'interval_after_signup', 'device_id_count', 'ip_address_count'],\n      dtype='object')\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": -1,\n    \"grid_resolution\": 20,\n    \"random_state\": 0,\n}\n\nprint(\"Computing partial dependence plots...\")\nfeatures_info = {\n    # features of interest\n    \"features\": ['purchase_value', 'source', 'browser', 'sex', 'age', 'interval_after_signup', 'device_id_count', 'ip_address_count'],\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": categorical_columns,\n}\n\n_, ax = plt.subplots(ncols=4, nrows=2, figsize=(10, 6), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(\n    model_pipe,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n\n_ = display.figure_.suptitle(\n    \"Partial dependence of the fraud\\n\"\n    \"for the random forest\",\n    fontsize=16,\n)\n\nComputing partial dependence plots...\n\n\n\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": -1,\n    \"grid_resolution\": 20,\n    \"random_state\": 0,\n}\n\nprint(\"Computing partial dependence plots...\")\nfeatures_info = {\n    # features of interest\n    \"features\": ['purchase_value', 'source', 'browser', 'sex', 'age', 'interval_after_signup', 'device_id_count', 'ip_address_count'],\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": categorical_columns,\n}\n\n_, ax = plt.subplots(ncols=4, nrows=2, figsize=(10, 6), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(\n    model_pipe,\n    X_val,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n\n_ = display.figure_.suptitle(\n    \"Partial dependence of the fraud\\n\"\n    \"for the random forest\",\n    fontsize=16,\n)\n\nComputing partial dependence plots..."
  },
  {
    "objectID": "posts/2022-11-24-data5-funnel-analysis/funnel_analysis5.html",
    "href": "posts/2022-11-24-data5-funnel-analysis/funnel_analysis5.html",
    "title": "Data challenge - 5. funnel analysis",
    "section": "",
    "text": "Funnel Analysis\nFunnel analysis allows to understand where/when our users abandon the website.\nhome page -&gt; search page -&gt; payment page -&gt; confirmation page\nhow conversion rate can be improved\nimport pandas as pd\nuser = pd.read_csv(\"user_table.csv\")\nuser.head()\n\n\n\n\n\n\n\n\nuser_id\ndate\ndevice\nsex\n\n\n\n\n0\n450007\n2015-02-28\nDesktop\nFemale\n\n\n1\n756838\n2015-01-13\nDesktop\nMale\n\n\n2\n568983\n2015-04-09\nDesktop\nMale\n\n\n3\n190794\n2015-02-18\nDesktop\nFemale\n\n\n4\n537909\n2015-01-15\nDesktop\nMale\nuser.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 90400 entries, 0 to 90399\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   user_id  90400 non-null  int64 \n 1   date     90400 non-null  object\n 2   device   90400 non-null  object\n 3   sex      90400 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 2.8+ MB\nhome_page = pd.read_csv(\"home_page_table.csv\").rename(columns={'page': 'home_page'})\nhome_page.head()\n\n\n\n\n\n\n\n\nuser_id\nhome_page\n\n\n\n\n0\n313593\nhome_page\n\n\n1\n468315\nhome_page\n\n\n2\n264005\nhome_page\n\n\n3\n290784\nhome_page\n\n\n4\n639104\nhome_page\nhome_page.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 90400 entries, 0 to 90399\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   user_id    90400 non-null  int64 \n 1   home_page  90400 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 1.4+ MB\nsearch_page = pd.read_csv(\"search_page_table.csv\").rename(columns={'page': 'search_page'})\nsearch_page.head()\n\n\n\n\n\n\n\n\nuser_id\nsearch_page\n\n\n\n\n0\n15866\nsearch_page\n\n\n1\n347058\nsearch_page\n\n\n2\n577020\nsearch_page\n\n\n3\n780347\nsearch_page\n\n\n4\n383739\nsearch_page\nsearch_page.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 45200 entries, 0 to 45199\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   user_id      45200 non-null  int64 \n 1   search_page  45200 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 706.4+ KB\npayment_page = pd.read_csv(\"payment_page_table.csv\").rename(columns={'page': 'payment_page'})\npayment_page.head()\n\n\n\n\n\n\n\n\nuser_id\npayment_page\n\n\n\n\n0\n253019\npayment_page\n\n\n1\n310478\npayment_page\n\n\n2\n304081\npayment_page\n\n\n3\n901286\npayment_page\n\n\n4\n195052\npayment_page\npayment_page.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6030 entries, 0 to 6029\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   user_id       6030 non-null   int64 \n 1   payment_page  6030 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 94.3+ KB\npayment_confirmation = pd.read_csv(\"payment_confirmation_table.csv\").rename(columns={'page': 'payment_confirmation_page'})\npayment_confirmation.head()\n\n\n\n\n\n\n\n\nuser_id\npayment_confirmation_page\n\n\n\n\n0\n123100\npayment_confirmation_page\n\n\n1\n704999\npayment_confirmation_page\n\n\n2\n407188\npayment_confirmation_page\n\n\n3\n538348\npayment_confirmation_page\n\n\n4\n841681\npayment_confirmation_page\npayment_confirmation.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 452 entries, 0 to 451\nData columns (total 2 columns):\n #   Column                     Non-Null Count  Dtype \n---  ------                     --------------  ----- \n 0   user_id                    452 non-null    int64 \n 1   payment_confirmation_page  452 non-null    object\ndtypes: int64(1), object(1)\nmemory usage: 7.2+ KB\nNo missing values."
  },
  {
    "objectID": "posts/2022-11-24-data5-funnel-analysis/funnel_analysis5.html#sex",
    "href": "posts/2022-11-24-data5-funnel-analysis/funnel_analysis5.html#sex",
    "title": "Data challenge - 5. funnel analysis",
    "section": "sex",
    "text": "sex\n\n# stages = ['home_page', 'search_page', 'payment_page', 'payment_confirmation_page']\n# sexes = ['Male', 'Female']\n# number_mobile = [data[(data[stage] == stage) & (data['device'] == 'Mobile')].shape[0] for stage in stages]\n#\n# number_desktop = [data[(data[stage] == stage) & (data['device'] == 'Desktop')].shape[0] for stage in stages]\n#\n# df_mobile = pd.DataFrame(dict(number=number_mobile, stage=stages))\n# df_mobile['device'] = 'mobile'\n#\n# df_desktop = pd.DataFrame(dict(number=number_desktop, stage=stages))\n# df_desktop['device'] = 'desktop'\n#\n# df_mobile['n_drop'] = -df_mobile.number.diff()\n# df_mobile['drop_rate'] = -df_mobile.number.pct_change()\n# df_mobile['conversion_rate'] = 1 - df_mobile['drop_rate']\n# # df_mobile\n#\n# df_desktop['n_drop'] = -df_desktop.number.diff()\n# df_desktop['drop_rate'] = -df_desktop.number.pct_change()\n# df_desktop['conversion_rate'] = 1 - df_desktop['drop_rate']\n# # df_desktop\n#\n# df = pd.concat([df_mobile, df_desktop], axis=0)\n\n\n# data.groupby(by=['device']).count().reset_index()\n\n\n# data.groupby(by=['sex']).count().reset_index().drop(columns=['user_id', 'date', 'device'])\n\n\ndf_sex = (data.groupby(by=['sex']).count().\n reset_index().\n drop(columns=['user_id', 'date', 'device']).\n melt(id_vars=['sex']).\n sort_values(by=['sex', 'value'], ascending=False))\n\ndf_sex\n\n\n\n\n\n\n\n\nsex\nvariable\nvalue\n\n\n\n\n1\nMale\nhome_page\n45325\n\n\n3\nMale\nsearch_page\n22524\n\n\n5\nMale\npayment_page\n2930\n\n\n7\nMale\npayment_confirmation_page\n211\n\n\n0\nFemale\nhome_page\n45075\n\n\n2\nFemale\nsearch_page\n22676\n\n\n4\nFemale\npayment_page\n3100\n\n\n6\nFemale\npayment_confirmation_page\n241\n\n\n\n\n\n\n\n\ndef calc_conversion(df):\n    return pd.DataFrame({\"sex\": df['sex'],\n                         \"stage\": df['variable'],\n                         \"n_conversion\": df['value'],\n                         \"conversion_rate\": 1 + df['value'].pct_change()})\n    #return 1 + df['value'].pct_change()\n\n\ndf_sex = df_sex.groupby(by='sex').apply(calc_conversion)#.reset_index()\ndf_sex\n\n\n\n\n\n\n\n\nsex\nstage\nn_conversion\nconversion_rate\n\n\n\n\n1\nMale\nhome_page\n45325\nNaN\n\n\n3\nMale\nsearch_page\n22524\n0.496944\n\n\n5\nMale\npayment_page\n2930\n0.130083\n\n\n7\nMale\npayment_confirmation_page\n211\n0.072014\n\n\n0\nFemale\nhome_page\n45075\nNaN\n\n\n2\nFemale\nsearch_page\n22676\n0.503073\n\n\n4\nFemale\npayment_page\n3100\n0.136708\n\n\n6\nFemale\npayment_confirmation_page\n241\n0.077742\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\nsns.barplot(data=df_sex.dropna(), x='stage', y='conversion_rate', hue='sex', ax=ax[0])\nsns.barplot(data=df_sex.dropna(), x='stage', y='n_conversion', hue='sex', ax=ax[1])\n\nplt.show()\n\n\n\n\nno obvious difference between sexes\n\n(data.groupby(by=['device']).count().\n reset_index().\n drop(columns=['user_id', 'date', 'sex']).\n melt(id_vars=['device']).\n sort_values(by=['device', 'value'], ascending=False))\n\n\n\n\n\n\n\n\ndevice\nvariable\nvalue\n\n\n\n\n1\nMobile\nhome_page\n30200\n\n\n3\nMobile\nsearch_page\n15100\n\n\n5\nMobile\npayment_page\n3020\n\n\n7\nMobile\npayment_confirmation_page\n302\n\n\n0\nDesktop\nhome_page\n60200\n\n\n2\nDesktop\nsearch_page\n30100\n\n\n4\nDesktop\npayment_page\n3010\n\n\n6\nDesktop\npayment_confirmation_page\n150"
  },
  {
    "objectID": "posts/2022-11-24-data5-funnel-analysis/funnel_analysis5.html#date",
    "href": "posts/2022-11-24-data5-funnel-analysis/funnel_analysis5.html#date",
    "title": "Data challenge - 5. funnel analysis",
    "section": "date",
    "text": "date\n\nday of week\n\ndata['month'] = pd.to_datetime(data['date']).dt.month\ndata['dayofweek'] = pd.to_datetime(data['date']).dt.isocalendar().day\n# data['year'] = pd.to_datetime(data['date']).dt.year  # all year is 2015\n\n\ndata\n\n\n\n\n\n\n\n\nuser_id\ndate\ndevice\nsex\nhome_page\nsearch_page\npayment_page\npayment_confirmation_page\nmonth\ndayofweek\n\n\n\n\n0\n450007\n2015-02-28\nDesktop\nFemale\nhome_page\nNaN\nNaN\nNaN\n2\n6\n\n\n1\n756838\n2015-01-13\nDesktop\nMale\nhome_page\nNaN\nNaN\nNaN\n1\n2\n\n\n2\n568983\n2015-04-09\nDesktop\nMale\nhome_page\nsearch_page\nNaN\nNaN\n4\n4\n\n\n3\n190794\n2015-02-18\nDesktop\nFemale\nhome_page\nsearch_page\nNaN\nNaN\n2\n3\n\n\n4\n537909\n2015-01-15\nDesktop\nMale\nhome_page\nNaN\nNaN\nNaN\n1\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n90395\n307667\n2015-03-30\nDesktop\nFemale\nhome_page\nNaN\nNaN\nNaN\n3\n1\n\n\n90396\n642989\n2015-02-08\nDesktop\nFemale\nhome_page\nsearch_page\nNaN\nNaN\n2\n7\n\n\n90397\n659645\n2015-04-13\nDesktop\nMale\nhome_page\nsearch_page\nNaN\nNaN\n4\n1\n\n\n90398\n359779\n2015-03-23\nDesktop\nMale\nhome_page\nNaN\nNaN\nNaN\n3\n1\n\n\n90399\n438929\n2015-03-26\nMobile\nFemale\nhome_page\nNaN\nNaN\nNaN\n3\n4\n\n\n\n\n90400 rows × 10 columns\n\n\n\n\ndf_dayofweek = (data.groupby(by=['dayofweek']).\n count().\n reset_index().\n drop(columns=['user_id', 'date', 'device', 'sex', 'month']).\n melt(id_vars=['dayofweek']).\n sort_values(by=['dayofweek', 'value'], ascending=[True, False]))\n\n# conversion =\ndf_dayofweek['conversion_rate'] = 1 + df_dayofweek.groupby('dayofweek')['value'].apply(pd.DataFrame.pct_change)\n\ndf_dayofweek\n\n\n\n\n\n\n\n\ndayofweek\nvariable\nvalue\nconversion_rate\n\n\n\n\n0\n1\nhome_page\n12955\nNaN\n\n\n7\n1\nsearch_page\n6483\n0.500425\n\n\n14\n1\npayment_page\n869\n0.134043\n\n\n21\n1\npayment_confirmation_page\n77\n0.088608\n\n\n1\n2\nhome_page\n12697\nNaN\n\n\n8\n2\nsearch_page\n6252\n0.492400\n\n\n15\n2\npayment_page\n853\n0.136436\n\n\n22\n2\npayment_confirmation_page\n69\n0.080891\n\n\n2\n3\nhome_page\n12831\nNaN\n\n\n9\n3\nsearch_page\n6335\n0.493726\n\n\n16\n3\npayment_page\n838\n0.132281\n\n\n23\n3\npayment_confirmation_page\n54\n0.064439\n\n\n3\n4\nhome_page\n13444\nNaN\n\n\n10\n4\nsearch_page\n6717\n0.499628\n\n\n17\n4\npayment_page\n899\n0.133840\n\n\n24\n4\npayment_confirmation_page\n66\n0.073415\n\n\n4\n5\nhome_page\n12750\nNaN\n\n\n11\n5\nsearch_page\n6406\n0.502431\n\n\n18\n5\npayment_page\n871\n0.135966\n\n\n25\n5\npayment_confirmation_page\n54\n0.061998\n\n\n5\n6\nhome_page\n12801\nNaN\n\n\n12\n6\nsearch_page\n6545\n0.511288\n\n\n19\n6\npayment_page\n845\n0.129106\n\n\n26\n6\npayment_confirmation_page\n70\n0.082840\n\n\n6\n7\nhome_page\n12922\nNaN\n\n\n13\n7\nsearch_page\n6462\n0.500077\n\n\n20\n7\npayment_page\n855\n0.132312\n\n\n27\n7\npayment_confirmation_page\n62\n0.072515\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\nsns.barplot(data=df_dayofweek.dropna(), x='variable', y='conversion_rate', hue='dayofweek', ax=ax[0])\nsns.barplot(data=df_dayofweek.dropna(), x='variable', y='value', hue='dayofweek', ax=ax[1])\n\nplt.show()\n\n\n\n\nno difference for different day of week\n\n\nmonth\n\ndf_month = (data.groupby(by=['month']).\n count().\n reset_index().\n drop(columns=['user_id', 'date', 'device', 'sex', 'dayofweek']).\n melt(id_vars=['month']).\n sort_values(by=['month', 'value'], ascending=[True, False]))\n\n# conversion =\ndf_dayofweek['conversion_rate'] = 1 + df_dayofweek.groupby('dayofweek')['value'].apply(pd.DataFrame.pct_change)\n\ndf_dayofweek\n\n\n\n\n\n\n\n\ndayofweek\nvariable\nvalue\nconversion_rate\n\n\n\n\n0\n1\nhome_page\n12955\nNaN\n\n\n7\n1\nsearch_page\n6483\n0.500425\n\n\n14\n1\npayment_page\n869\n0.134043\n\n\n21\n1\npayment_confirmation_page\n77\n0.088608\n\n\n1\n2\nhome_page\n12697\nNaN\n\n\n8\n2\nsearch_page\n6252\n0.492400\n\n\n15\n2\npayment_page\n853\n0.136436\n\n\n22\n2\npayment_confirmation_page\n69\n0.080891\n\n\n2\n3\nhome_page\n12831\nNaN\n\n\n9\n3\nsearch_page\n6335\n0.493726\n\n\n16\n3\npayment_page\n838\n0.132281\n\n\n23\n3\npayment_confirmation_page\n54\n0.064439\n\n\n3\n4\nhome_page\n13444\nNaN\n\n\n10\n4\nsearch_page\n6717\n0.499628\n\n\n17\n4\npayment_page\n899\n0.133840\n\n\n24\n4\npayment_confirmation_page\n66\n0.073415\n\n\n4\n5\nhome_page\n12750\nNaN\n\n\n11\n5\nsearch_page\n6406\n0.502431\n\n\n18\n5\npayment_page\n871\n0.135966\n\n\n25\n5\npayment_confirmation_page\n54\n0.061998\n\n\n5\n6\nhome_page\n12801\nNaN\n\n\n12\n6\nsearch_page\n6545\n0.511288\n\n\n19\n6\npayment_page\n845\n0.129106\n\n\n26\n6\npayment_confirmation_page\n70\n0.082840\n\n\n6\n7\nhome_page\n12922\nNaN\n\n\n13\n7\nsearch_page\n6462\n0.500077\n\n\n20\n7\npayment_page\n855\n0.132312\n\n\n27\n7\npayment_confirmation_page\n62\n0.072515\n\n\n\n\n\n\n\n\ndf_month = (data.groupby(by=['month']).\n count().\n reset_index().\n drop(columns=['user_id', 'date', 'device', 'sex', 'dayofweek']).\n melt(id_vars=['month']).\n sort_values(by=['month', 'value'], ascending=[True, False]))\n\ndf_month['conversion_rate'] = 1 + df_month.groupby('month')['value'].apply(pd.DataFrame.pct_change)\n\ndf_month\n\n\n\n\n\n\n\n\nmonth\nvariable\nvalue\nconversion_rate\n\n\n\n\n0\n1\nhome_page\n22600\nNaN\n\n\n4\n1\nsearch_page\n13554\n0.599735\n\n\n8\n1\npayment_page\n2390\n0.176332\n\n\n12\n1\npayment_confirmation_page\n189\n0.079079\n\n\n1\n2\nhome_page\n22600\nNaN\n\n\n5\n2\nsearch_page\n13687\n0.605619\n\n\n9\n2\npayment_page\n2412\n0.176226\n\n\n13\n2\npayment_confirmation_page\n173\n0.071725\n\n\n2\n3\nhome_page\n22600\nNaN\n\n\n6\n3\nsearch_page\n8879\n0.392876\n\n\n10\n3\npayment_page\n631\n0.071067\n\n\n14\n3\npayment_confirmation_page\n44\n0.069731\n\n\n3\n4\nhome_page\n22600\nNaN\n\n\n7\n4\nsearch_page\n9080\n0.401770\n\n\n11\n4\npayment_page\n597\n0.065749\n\n\n15\n4\npayment_confirmation_page\n46\n0.077052\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\nsns.barplot(data=df_month.dropna(), x='variable', y='conversion_rate', hue='month', ax=ax[0])\nsns.barplot(data=df_month.dropna(), x='variable', y='value', hue='month', ax=ax[1])\n\nplt.show()\n\n\n\n\nfirst 2 months, conversion rate is higher than the last 2 months, related to season?"
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html",
    "title": "Data challenge - Snowflake 2023",
    "section": "",
    "text": "# If you'd like to install packages that aren't installed by default, uncomment the last two lines of this cell and replace &lt;package list&gt; with a list of your packages.\n# This will ensure your notebook has all the dependencies and works everywhere\n\n#import sys\n#!{sys.executable} -m pip install &lt;package list&gt;\nimport sys\n!{sys.executable} -m pip install -U scikit-learn\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (1.2.1)\nRequirement already satisfied: scipy&gt;=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.8.1)\nRequirement already satisfied: numpy&gt;=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.21.6)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\nimport sklearn\n\nprint(sklearn.__version__)\n1.2.1\n#Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option(\"display.max_columns\", 101)\n\n\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nid\nThe unique id assigned to every record.\n\n\nDate\nThe date on which grocery item is sold.\n\n\nStore Number\nThe unique number assigned to each store.\n\n\nStore Grade\nThe grade assigned to each store. Values are between A - D, where A indicates a huge hypermarket and D indicates a small store.\n\n\nCity\nThe city in which the store is located.\n\n\nState\nThe state in which the store is located.\n\n\nItem Number\nThe unique number assigned to each item.\n\n\nItem Family\nThe family to which the item belongs.\n\n\nPromotion\nIndicates if the item is held under promotions (or discounts). Values are True and False.\n\n\nPerishable\nIndicates if the item is perishable or not. Values are 0 and 1.\n\n\nHoliday\nIndicates if the chosen date is a holiday or not.\n\n\nHoliday Extent\nIndicates the extent of a holiday. Values are ‘Local’, ‘Regional’, ‘National’.\n\n\nHoliday Transferred\nIndicates if the holiday is transferred or not. Values are True and False. A transferred holiday is like a normal working day.\n\n\nQuantity\nThe number of units of an item sold.\n\n\n\n\n\n\n# Dataset is already loaded below\ndata = pd.read_csv(\"train.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\n\n\n\nid\n\n\n\n\nDate\n\n\n\n\nStore Number\n\n\n\n\nStore Grade\n\n\n\n\nCity\n\n\n\n\nState\n\n\n\n\nItem Number\n\n\n\n\nItem Family\n\n\n\n\nPromotion\n\n\n\n\nPerishable\n\n\n\n\nHoliday\n\n\n\n\nHoliday Extent\n\n\n\n\nHoliday Transferred\n\n\n\n\nQuantity\n\n\n\n\n\n\n\n\n0\n\n\n\n\n66779748\n\n\n\n\n2019-01-05\n\n\n\n\n11\n\n\n\n\nA\n\n\n\n\nNew York\n\n\n\n\nNew York\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n1\n\n\n\n\n66784022\n\n\n\n\n2019-01-05\n\n\n\n\n14\n\n\n\n\nB\n\n\n\n\nLos Angeles\n\n\n\n\nCalifornia\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n2\n\n\n\n\n66785409\n\n\n\n\n2019-01-05\n\n\n\n\n15\n\n\n\n\nB\n\n\n\n\nHouston\n\n\n\n\nTexas\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n3\n\n\n\n\n66791247\n\n\n\n\n2019-01-05\n\n\n\n\n19\n\n\n\n\nB\n\n\n\n\nPhiladelphia\n\n\n\n\nPennsylvania\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n4\n\n\n\n\n66796208\n\n\n\n\n2019-01-05\n\n\n\n\n22\n\n\n\n\nB\n\n\n\n\nChicago\n\n\n\n\nIllinois\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n\n#Explore columns\ndata.columns\nIndex(['id', 'Date', 'Store Number', 'Store Grade', 'City', 'State',\n       'Item Number', 'Item Family', 'Promotion', 'Perishable', 'Holiday',\n       'Holiday Extent', 'Holiday Transferred', 'Quantity'],\n      dtype='object')\n#Description\ndata.describe()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nStore Number\nItem Number\nPerishable\nQuantity\n\n\n\n\ncount\n2.411000e+03\n2411.000000\n2411.0\n2411.0\n2411.000000\n\n\nmean\n7.815180e+07\n27.083783\n1146795.0\n0.0\n6.203650\n\n\nstd\n6.503956e+06\n13.192087\n0.0\n0.0\n1.830018\n\n\nmin\n6.677975e+07\n11.000000\n1146795.0\n0.0\n2.000000\n\n\n25%\n7.256662e+07\n15.000000\n1146795.0\n0.0\n5.000000\n\n\n50%\n7.824733e+07\n22.000000\n1146795.0\n0.0\n6.000000\n\n\n75%\n8.362051e+07\n35.000000\n1146795.0\n0.0\n8.000000\n\n\nmax\n8.965620e+07\n54.000000\n1146795.0\n0.0\n14.000000\n\n\n\n\ndata.describe(exclude=[np.number])\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\n\nStore Grade\n\n\n\n\nCity\n\n\n\n\nState\n\n\n\n\nItem Family\n\n\n\n\nPromotion\n\n\n\n\nHoliday\n\n\n\n\nHoliday Extent\n\n\n\n\nHoliday Transferred\n\n\n\n\n\n\n\n\ncount\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n587\n\n\n\n\n587\n\n\n\n\n587\n\n\n\n\n\n\nunique\n\n\n\n\n239\n\n\n\n\n4\n\n\n\n\n10\n\n\n\n\n8\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n1\n\n\n\n\n3\n\n\n\n\n2\n\n\n\n\n\n\ntop\n\n\n\n\n2019-06-25\n\n\n\n\nB\n\n\n\n\nHouston\n\n\n\n\nTexas\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\nHoliday\n\n\n\n\nNational\n\n\n\n\nFalse\n\n\n\n\n\n\nfreq\n\n\n\n\n27\n\n\n\n\n1686\n\n\n\n\n249\n\n\n\n\n497\n\n\n\n\n2411\n\n\n\n\n2117\n\n\n\n\n587\n\n\n\n\n403\n\n\n\n\n557\n\n\n\n\n\n\n\nOnly 1 item family, not helpful\ndata.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2411 entries, 0 to 2410\nData columns (total 14 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   id                   2411 non-null   int64  \n 1   Date                 2411 non-null   object \n 2   Store Number         2411 non-null   int64  \n 3   Store Grade          2411 non-null   object \n 4   City                 2411 non-null   object \n 5   State                2411 non-null   object \n 6   Item Number          2411 non-null   int64  \n 7   Item Family          2411 non-null   object \n 8   Promotion            2411 non-null   bool   \n 9   Perishable           2411 non-null   int64  \n 10  Holiday              587 non-null    object \n 11  Holiday Extent       587 non-null    object \n 12  Holiday Transferred  587 non-null    object \n 13  Quantity             2411 non-null   float64\ndtypes: bool(1), float64(1), int64(4), object(8)\nmemory usage: 247.3+ KB\nNull in Holiday related features refers to not holiday, no missing values."
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html#data-description",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html#data-description",
    "title": "Data challenge - Snowflake 2023",
    "section": "",
    "text": "Column\nDescription\n\n\n\n\nid\nThe unique id assigned to every record.\n\n\nDate\nThe date on which grocery item is sold.\n\n\nStore Number\nThe unique number assigned to each store.\n\n\nStore Grade\nThe grade assigned to each store. Values are between A - D, where A indicates a huge hypermarket and D indicates a small store.\n\n\nCity\nThe city in which the store is located.\n\n\nState\nThe state in which the store is located.\n\n\nItem Number\nThe unique number assigned to each item.\n\n\nItem Family\nThe family to which the item belongs.\n\n\nPromotion\nIndicates if the item is held under promotions (or discounts). Values are True and False.\n\n\nPerishable\nIndicates if the item is perishable or not. Values are 0 and 1.\n\n\nHoliday\nIndicates if the chosen date is a holiday or not.\n\n\nHoliday Extent\nIndicates the extent of a holiday. Values are ‘Local’, ‘Regional’, ‘National’.\n\n\nHoliday Transferred\nIndicates if the holiday is transferred or not. Values are True and False. A transferred holiday is like a normal working day.\n\n\nQuantity\nThe number of units of an item sold."
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html#data-wrangling-visualization",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html#data-wrangling-visualization",
    "title": "Data challenge - Snowflake 2023",
    "section": "",
    "text": "# Dataset is already loaded below\ndata = pd.read_csv(\"train.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\n\n\n\nid\n\n\n\n\nDate\n\n\n\n\nStore Number\n\n\n\n\nStore Grade\n\n\n\n\nCity\n\n\n\n\nState\n\n\n\n\nItem Number\n\n\n\n\nItem Family\n\n\n\n\nPromotion\n\n\n\n\nPerishable\n\n\n\n\nHoliday\n\n\n\n\nHoliday Extent\n\n\n\n\nHoliday Transferred\n\n\n\n\nQuantity\n\n\n\n\n\n\n\n\n0\n\n\n\n\n66779748\n\n\n\n\n2019-01-05\n\n\n\n\n11\n\n\n\n\nA\n\n\n\n\nNew York\n\n\n\n\nNew York\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n1\n\n\n\n\n66784022\n\n\n\n\n2019-01-05\n\n\n\n\n14\n\n\n\n\nB\n\n\n\n\nLos Angeles\n\n\n\n\nCalifornia\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n2\n\n\n\n\n66785409\n\n\n\n\n2019-01-05\n\n\n\n\n15\n\n\n\n\nB\n\n\n\n\nHouston\n\n\n\n\nTexas\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n3\n\n\n\n\n66791247\n\n\n\n\n2019-01-05\n\n\n\n\n19\n\n\n\n\nB\n\n\n\n\nPhiladelphia\n\n\n\n\nPennsylvania\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n4\n\n\n\n\n66796208\n\n\n\n\n2019-01-05\n\n\n\n\n22\n\n\n\n\nB\n\n\n\n\nChicago\n\n\n\n\nIllinois\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n6.0\n\n\n\n\n\n\n\n#Explore columns\ndata.columns\nIndex(['id', 'Date', 'Store Number', 'Store Grade', 'City', 'State',\n       'Item Number', 'Item Family', 'Promotion', 'Perishable', 'Holiday',\n       'Holiday Extent', 'Holiday Transferred', 'Quantity'],\n      dtype='object')\n#Description\ndata.describe()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nStore Number\nItem Number\nPerishable\nQuantity\n\n\n\n\ncount\n2.411000e+03\n2411.000000\n2411.0\n2411.0\n2411.000000\n\n\nmean\n7.815180e+07\n27.083783\n1146795.0\n0.0\n6.203650\n\n\nstd\n6.503956e+06\n13.192087\n0.0\n0.0\n1.830018\n\n\nmin\n6.677975e+07\n11.000000\n1146795.0\n0.0\n2.000000\n\n\n25%\n7.256662e+07\n15.000000\n1146795.0\n0.0\n5.000000\n\n\n50%\n7.824733e+07\n22.000000\n1146795.0\n0.0\n6.000000\n\n\n75%\n8.362051e+07\n35.000000\n1146795.0\n0.0\n8.000000\n\n\nmax\n8.965620e+07\n54.000000\n1146795.0\n0.0\n14.000000\n\n\n\n\ndata.describe(exclude=[np.number])\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\n\nStore Grade\n\n\n\n\nCity\n\n\n\n\nState\n\n\n\n\nItem Family\n\n\n\n\nPromotion\n\n\n\n\nHoliday\n\n\n\n\nHoliday Extent\n\n\n\n\nHoliday Transferred\n\n\n\n\n\n\n\n\ncount\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n2411\n\n\n\n\n587\n\n\n\n\n587\n\n\n\n\n587\n\n\n\n\n\n\nunique\n\n\n\n\n239\n\n\n\n\n4\n\n\n\n\n10\n\n\n\n\n8\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n1\n\n\n\n\n3\n\n\n\n\n2\n\n\n\n\n\n\ntop\n\n\n\n\n2019-06-25\n\n\n\n\nB\n\n\n\n\nHouston\n\n\n\n\nTexas\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\nHoliday\n\n\n\n\nNational\n\n\n\n\nFalse\n\n\n\n\n\n\nfreq\n\n\n\n\n27\n\n\n\n\n1686\n\n\n\n\n249\n\n\n\n\n497\n\n\n\n\n2411\n\n\n\n\n2117\n\n\n\n\n587\n\n\n\n\n403\n\n\n\n\n557\n\n\n\n\n\n\n\nOnly 1 item family, not helpful\ndata.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2411 entries, 0 to 2410\nData columns (total 14 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   id                   2411 non-null   int64  \n 1   Date                 2411 non-null   object \n 2   Store Number         2411 non-null   int64  \n 3   Store Grade          2411 non-null   object \n 4   City                 2411 non-null   object \n 5   State                2411 non-null   object \n 6   Item Number          2411 non-null   int64  \n 7   Item Family          2411 non-null   object \n 8   Promotion            2411 non-null   bool   \n 9   Perishable           2411 non-null   int64  \n 10  Holiday              587 non-null    object \n 11  Holiday Extent       587 non-null    object \n 12  Holiday Transferred  587 non-null    object \n 13  Quantity             2411 non-null   float64\ndtypes: bool(1), float64(1), int64(4), object(8)\nmemory usage: 247.3+ KB\nNull in Holiday related features refers to not holiday, no missing values."
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html#target",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html#target",
    "title": "Data challenge - Snowflake 2023",
    "section": "Target",
    "text": "Target\ntarget = 'Quantity'\ndata[target].hist()\n&lt;AxesSubplot:&gt;\n\n\n\npng\n\n\ndata[target].value_counts()\n6.0     514\n5.0     491\n7.0     360\n8.0     343\n4.0     341\n9.0     164\n3.0      82\n10.0     63\n11.0     22\n2.0      18\n12.0     11\n14.0      2\nName: Quantity, dtype: int64\nMost are less than 10 and seems a little left skewed. I will try to transform the target variable if I have more time."
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html#categorical-features",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html#categorical-features",
    "title": "Data challenge - Snowflake 2023",
    "section": "Categorical features",
    "text": "Categorical features\ndef plot_discrete(df, feature, target, figsize=(14, 8)):\n    # bins=30\n    fig, ax = plt.subplots(2, 2, figsize=figsize)\n    \n    sns.histplot(data=df, x=target, hue=feature, ax=ax[0,0])\n    ax[0,0].set_title(f'Histogram of {feature} by {target}')\n    \n    sns.kdeplot(data=df, x=target, hue=feature, fill=True, common_norm=False, ax=ax[0,1])\n    ax[0,1].set_title(f'Density plot of {feature} by {target}')\n    \n    sns.boxplot(data=df, y=target, x=feature, ax=ax[1,0])\n    ax[1,0].set_title(f'Box plot of {feature} by {target}')\n    \n    sns.violinplot(data=df, y=target, x=feature, ax=ax[1,1])\n    ax[1,1].set_title(f'Violin plot of {feature} by {target}')\n    plt.tight_layout()\nplot_discrete(data, 'Store Number', target)\n\n\n\npng\n\n\n\nNo obvious difference for different store numbers.\nStore number 33 has the largest meadian.\n\nplot_discrete(data, 'Store Grade', target)\n\n\n\npng\n\n\ndata['Store Grade'].value_counts()\nB    1686\nD     248\nC     239\nA     238\nName: Store Grade, dtype: int64\n\nMost store grade is B. This feature is unbalanced.\nNo obvious difference for different store grade.\n\nplot_discrete(data, 'City', target)\n\n\n\npng\n\n\n\nSan Antonio has the largest quantity\nNo obvious difference of different city\n\nplot_discrete(data, 'State', target)\n\n\n\npng\n\n\n\nNo obvious difference across states\n\nplot_discrete(data, 'Promotion', target)\n\n\n\npng\n\n\ndata.groupby('Promotion')['Quantity'].mean()\nPromotion\nFalse    6.152574\nTrue     6.571429\nName: Quantity, dtype: float64\n\nQuantity is a little bit larger for promotion\n\nplot_discrete(data, 'Holiday', target)\n\n\n\npng\n\n\ndata.groupby('Holiday')['Quantity'].mean()\nHoliday\nHoliday        6.172061\nNot holiday    6.213816\nName: Quantity, dtype: float64\n\nNo obvious difference for holiday or not\n\nplot_discrete(data, 'Holiday Extent', target)\n\n\n\npng\n\n\n\nRegional holiday seems has the most median quantity\nHoliday Extent contains the information of Holiday. I will drop Holiday\n\nplot_discrete(data, 'Holiday Transferred', target)\n\n\n\npng\n\n\n\nIf the holiday is transferred, median quantity is smaller. This makes sense, since a transferred holiday is like working day.\n\nplot_discrete(data, 'weekend', target)\n\n\n\npng\n\n\n\nNo obvious difference\n\nplot_discrete(data, 'dayofweek', target)\n\n\n\npng\n\n\n\nInterestingly, day of week == 2, 3, 4, has higher median quantity. Seems middle of the week has better sale.\nSince day of week contains the information of is weekend or not and shows more interesting pattern, I will keep day of week instead of is weekend.\nIf I have more time, I will look into the interactions between features\nNo numerical features"
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html#visualization-modeling-machine-learning",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html#visualization-modeling-machine-learning",
    "title": "Data challenge - Snowflake 2023",
    "section": "Visualization, Modeling, Machine Learning",
    "text": "Visualization, Modeling, Machine Learning\nCan you build a model that can forecast the unit sales and identify how different features influence it? Please explain your findings effectively to technical and non-technical audiences using comments and visualizations, if appropriate. - Build an optimized model that effectively solves the business problem. - The model would be evaluated on the basis of mean absolute percent error (MAPE). - Read the test.csv file and prepare features for testing.\ncategorical_features = ['Store Number', \n                        'Store Grade', \n                        'City', \n                        'State', \n                        'Promotion', \n                        'Holiday Extent',\n                        'Holiday Transferred',\n                        'dayofweek']\ndata.month.value_counts()\n5    344\n6    311\n7    309\n4    306\n3    304\n8    291\n2    281\n1    265\nName: month, dtype: int64\n\nSplit train, val\nUse first 6 months as training data, 7, 8 as validation data\nfor col in categorical_features:\n    data[col] = data[col].astype('str')\ntrain = data[data['month'].isin([1, 2, 3, 4, 5, 6])]\nval = data[data['month'].isin([7, 8])]\nX_train = train[categorical_features]\ny_train = train[target]\nX_val = val[categorical_features]\ny_val = val[target]\n\n\nModel\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\n# encoder\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_features)], remainder='passthrough', verbose_feature_names_out=False\n)\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestRegressor(n_jobs=-1, random_state=0))\n])\npipe.fit(X_train, y_train)\n\n\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['Store Number',\n                                                   'Store Grade', 'City',\n                                                   'State', 'Promotion',\n                                                   'Holiday Extent',\n                                                   'Holiday Transferred',\n                                                   'dayofweek'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestRegressor(n_jobs=-1, random_state=0))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\nPipeline\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['Store Number',\n                                                   'Store Grade', 'City',\n                                                   'State', 'Promotion',\n                                                   'Holiday Extent',\n                                                   'Holiday Transferred',\n                                                   'dayofweek'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestRegressor(n_jobs=-1, random_state=0))])\n\n\n\n\n\n\n\npreprocessors: ColumnTransformer\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['Store Number', 'Store Grade', 'City',\n                                  'State', 'Promotion', 'Holiday Extent',\n                                  'Holiday Transferred', 'dayofweek'])],\n                  verbose_feature_names_out=False)\n\n\n\n\n\n\n\n\ncat\n\n['Store Number', 'Store Grade', 'City', 'State', 'Promotion', 'Holiday Extent', 'Holiday Transferred', 'dayofweek']\n\n\n\n\n\n\n\n\nOrdinalEncoder\n\nOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n\n\n\n\n\n\n\n\n\n\n\n\nremainder\n\n[]\n\n\n\n\n\n\npassthrough\n\npassthrough\n\n\n\n\n\n\n\n\n\n\nRandomForestRegressor\n\nRandomForestRegressor(n_jobs=-1, random_state=0)\n\n\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_percentage_error\n\ndef evaluate_model(pipe, X_train, y_train, X_val, y_val):\n    print(f'MAPE train = {mean_absolute_percentage_error(pipe.predict(X_train), y_train): .4f}')\n    print(f'MAPE val = {mean_absolute_percentage_error(pipe.predict(X_val), y_val): .4f}')\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\nMAPE train =  0.1781\nMAPE val =  0.2693\nThe model is overtrained. I will tune hyperparameters.\n\n\nTune hyperparameters\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [10, 20, 50]\nmax_depth = [5, 7, 9]\nmin_samples_split = [3, 6, 10]\nmin_samples_leaf = [2, 3, 5]\n\nrandom_grid = {'model__n_estimators': n_estimators,\n               'model__max_depth': max_depth,\n               'model__min_samples_split': min_samples_split,\n               'model__min_samples_leaf': min_samples_leaf}\n\nrandom_search = RandomizedSearchCV(pipe, random_grid, n_iter=20, n_jobs=-1, scoring='neg_mean_absolute_percentage_error')\nrandom_search.fit(X_train, y_train)\n\n\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['Store '\n                                                                                'Number',\n                                                                                'Store '\n                                                                                'Grade',\n                                                                                'City',\n                                                                                'State',\n                                                                                'Promotion',\n                                                                                'Holiday '\n                                                                                'Extent',\n                                                                                'Holiday '\n                                                                                'Transferred',\n                                                                                'dayofweek'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestRegressor(n_jobs=-1,\n                                                                    random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [5, 7, 9],\n                                        'model__min_samples_leaf': [2, 3, 5],\n                                        'model__min_samples_split': [3, 6, 10],\n                                        'model__n_estimators': [10, 20, 50]},\n                   scoring='neg_mean_absolute_percentage_error')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\nRandomizedSearchCV\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                                               unknown_value=-1))]),\n                                                                               ['Store '\n                                                                                'Number',\n                                                                                'Store '\n                                                                                'Grade',\n                                                                                'City',\n                                                                                'State',\n                                                                                'Promotion',\n                                                                                'Holiday '\n                                                                                'Extent',\n                                                                                'Holiday '\n                                                                                'Transferred',\n                                                                                'dayofweek'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              RandomForestRegressor(n_jobs=-1,\n                                                                    random_state=0))]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'model__max_depth': [5, 7, 9],\n                                        'model__min_samples_leaf': [2, 3, 5],\n                                        'model__min_samples_split': [3, 6, 10],\n                                        'model__n_estimators': [10, 20, 50]},\n                   scoring='neg_mean_absolute_percentage_error')\n\n\n\n\n\n\n\n\nestimator: Pipeline\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['Store Number',\n                                                   'Store Grade', 'City',\n                                                   'State', 'Promotion',\n                                                   'Holiday Extent',\n                                                   'Holiday Transferred',\n                                                   'dayofweek'])],\n                                   verbose_feature_names_out=False)),\n                ('model', RandomForestRegressor(n_jobs=-1, random_state=0))])\n\n\n\n\n\n\n\n\n\npreprocessors: ColumnTransformer\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['Store Number', 'Store Grade', 'City',\n                                  'State', 'Promotion', 'Holiday Extent',\n                                  'Holiday Transferred', 'dayofweek'])],\n                  verbose_feature_names_out=False)\n\n\n\n\n\n\n\n\ncat\n\n['Store Number', 'Store Grade', 'City', 'State', 'Promotion', 'Holiday Extent', 'Holiday Transferred', 'dayofweek']\n\n\n\n\n\n\n\n\nOrdinalEncoder\n\nOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n\n\n\n\n\n\n\n\n\n\n\n\nremainder\n\n[]\n\n\n\n\n\n\npassthrough\n\npassthrough\n\n\n\n\n\n\n\n\n\n\nRandomForestRegressor\n\nRandomForestRegressor(n_jobs=-1, random_state=0)\n\n\n\n\n\n\n\n\n\n\n\n\nprint(random_search.best_params_)\nbest_pipe = random_search.best_estimator_\nevaluate_model(best_pipe, X_train, y_train, X_val, y_val)\n{'model__n_estimators': 10, 'model__min_samples_split': 10, 'model__min_samples_leaf': 3, 'model__max_depth': 9}\nMAPE train =  0.1843\nMAPE val =  0.2691\n\nThe improvement is quite limited.\nI will tune more hyperparameters if I have more time."
  },
  {
    "objectID": "posts/2023-02-25-data-snowflake/snowflake_oa.html#feature-importance",
    "href": "posts/2023-02-25-data-snowflake/snowflake_oa.html#feature-importance",
    "title": "Data challenge - Snowflake 2023",
    "section": "Feature Importance",
    "text": "Feature Importance\nThe management wants to know what are the most important features for your model. Can you tell them?\n\nTask:\n\nVisualize the top 20 features and their feature importance.\n\n\n\nGini importance\nfeature_names = best_pipe[:-1].get_feature_names_out()\nimportances = best_pipe[-1].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_pipe[-1].estimators_], axis=0)\n\nfeature_importances = pd.Series(importances, index=feature_names)\nfeature_importances\nStore Number           0.157598\nStore Grade            0.018390\nCity                   0.051446\nState                  0.035006\nPromotion              0.069175\nHoliday Extent         0.158827\nHoliday Transferred    0.033777\ndayofweek              0.475782\ndtype: float64\nfig, ax = plt.subplots(figsize=(5, 8))\n\nfeature_importances.plot.barh(xerr=std, ax=ax)\nax.set_title('Feature importances using Gini impurity')\nax.set_ylabel(\"Mean decrease in impurity\")\nplt.show()\n\n\n\npng\n\n\n\n\nPermutation importance\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    best_pipe, X_val, y_val, scoring='neg_mean_absolute_percentage_error',\n    n_repeats=30, random_state=42, n_jobs=-1)\n\nsorted_importances_idx = result.importances_mean.argsort()\n\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=feature_names)\n\nax = importances.plot.box(vert=False, whis=10)\nax.set_title('Permutation Importances (validation set)')\nax.axvline(x=0, color='k', linestyle='-')\nax.set_xlabel('Decrease in MAPE')\nax.figure.tight_layout()\n\n\n\npng\n\n\n\n\nPartial dependence\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    'subsample': 50,\n    'n_jobs': -1,\n    'random_state': 0\n}\n\nfeatures_info = {\n    'features': categorical_features,\n    'kind': 'average',\n    'categorical_features': categorical_features\n}\n\n_, ax = plt.subplots(ncols=4, nrows=2, figsize=(16, 8), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(\n    best_pipe, \n    X_val,\n    **features_info,\n    ax=ax,\n    **common_params\n)\n\n_ = display.figure_.suptitle('Partial dependence for the random forest', fontsize=16)\n\n\n\npng\n\n\n\nMost important features are\n\nDay of week, middle of the week has apprantly more sales\nPromotion helps with the sales\nHoliday transferred.\nHoliday extent. Local is a little bit better than national.\n\nOther comments\n\nCity, some cities, like San Antonio, San Diego, have more sales\nStore number, larger store number has more sales, perhaps larger store number is newer with better infrastructures\nStore grade. A has slightly more sale than other grades. Huge hypermarket has more sales.\n\n\n\nTask:\n\nSubmit the predictions on the test dataset using your optimized model  For each record in the test set (test.csv), you must predict the value of the Quantity variable. You should submit a CSV file with a header row and one row per test entry. The file (submissions.csv) should have exactly 2 columns:\n\n\nThe file (submissions.csv) should have exactly 2 columns: - id - Quantity\n#Loading Test data\ntest_data=pd.read_csv('test.csv')\ntest_data.head()\n\n\n\n\n\n\n\n\n\n\n\n\nid\n\n\n\n\nDate\n\n\n\n\nStore Number\n\n\n\n\nStore Grade\n\n\n\n\nCity\n\n\n\n\nState\n\n\n\n\nItem Number\n\n\n\n\nItem Family\n\n\n\n\nPromotion\n\n\n\n\nPerishable\n\n\n\n\nHoliday\n\n\n\n\nHoliday Extent\n\n\n\n\nHoliday Transferred\n\n\n\n\n\n\n\n\n0\n\n\n\n\n89679192\n\n\n\n\n2019-09-01\n\n\n\n\n11\n\n\n\n\nA\n\n\n\n\nNew York\n\n\n\n\nNew York\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n\n\n1\n\n\n\n\n89683748\n\n\n\n\n2019-09-01\n\n\n\n\n14\n\n\n\n\nB\n\n\n\n\nLos Angeles\n\n\n\n\nCalifornia\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n\n\n2\n\n\n\n\n89685203\n\n\n\n\n2019-09-01\n\n\n\n\n15\n\n\n\n\nB\n\n\n\n\nHouston\n\n\n\n\nTexas\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n\n\n3\n\n\n\n\n89689659\n\n\n\n\n2019-09-01\n\n\n\n\n19\n\n\n\n\nB\n\n\n\n\nPhiladelphia\n\n\n\n\nPennsylvania\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n\n\n4\n\n\n\n\n89694936\n\n\n\n\n2019-09-01\n\n\n\n\n22\n\n\n\n\nB\n\n\n\n\nChicago\n\n\n\n\nIllinois\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n\n\n\ntest_data.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1124 entries, 0 to 1123\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   id                   1124 non-null   int64 \n 1   Date                 1124 non-null   object\n 2   Store Number         1124 non-null   int64 \n 3   Store Grade          1124 non-null   object\n 4   City                 1124 non-null   object\n 5   State                1124 non-null   object\n 6   Item Number          1124 non-null   int64 \n 7   Item Family          1124 non-null   object\n 8   Promotion            1124 non-null   bool  \n 9   Perishable           1124 non-null   int64 \n 10  Holiday              208 non-null    object\n 11  Holiday Extent       218 non-null    object\n 12  Holiday Transferred  218 non-null    object\ndtypes: bool(1), int64(4), object(8)\nmemory usage: 106.6+ KB\n# prepare test data\ntest_data['month'] = pd.to_datetime(test_data.Date).dt.month\ntest_data['dayofweek'] = pd.to_datetime(test_data.Date).dt.dayofweek\ntest_data['weekend'] = (pd.to_datetime(test_data.Date).dt.dayofweek // 5 == 1).astype(int)\n\ntest_data = test_data.fillna('Not holiday')\n\nfor col in categorical_features:\n    test_data[col] = test_data[col].astype('str')\ntest_data.head()\n\n\n\n\n\n\n\n\n\n\n\n\nid\n\n\n\n\nDate\n\n\n\n\nStore Number\n\n\n\n\nStore Grade\n\n\n\n\nCity\n\n\n\n\nState\n\n\n\n\nItem Number\n\n\n\n\nItem Family\n\n\n\n\nPromotion\n\n\n\n\nPerishable\n\n\n\n\nHoliday\n\n\n\n\nHoliday Extent\n\n\n\n\nHoliday Transferred\n\n\n\n\nmonth\n\n\n\n\ndayofweek\n\n\n\n\nweekend\n\n\n\n\n\n\n\n\n0\n\n\n\n\n89679192\n\n\n\n\n2019-09-01\n\n\n\n\n11\n\n\n\n\nA\n\n\n\n\nNew York\n\n\n\n\nNew York\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\n9\n\n\n\n\n6\n\n\n\n\n1\n\n\n\n\n\n\n1\n\n\n\n\n89683748\n\n\n\n\n2019-09-01\n\n\n\n\n14\n\n\n\n\nB\n\n\n\n\nLos Angeles\n\n\n\n\nCalifornia\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\n9\n\n\n\n\n6\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n89685203\n\n\n\n\n2019-09-01\n\n\n\n\n15\n\n\n\n\nB\n\n\n\n\nHouston\n\n\n\n\nTexas\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\n9\n\n\n\n\n6\n\n\n\n\n1\n\n\n\n\n\n\n3\n\n\n\n\n89689659\n\n\n\n\n2019-09-01\n\n\n\n\n19\n\n\n\n\nB\n\n\n\n\nPhiladelphia\n\n\n\n\nPennsylvania\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\n9\n\n\n\n\n6\n\n\n\n\n1\n\n\n\n\n\n\n4\n\n\n\n\n89694936\n\n\n\n\n2019-09-01\n\n\n\n\n22\n\n\n\n\nB\n\n\n\n\nChicago\n\n\n\n\nIllinois\n\n\n\n\n1146795\n\n\n\n\nGROCERY\n\n\n\n\nFalse\n\n\n\n\n0\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\nNot holiday\n\n\n\n\n9\n\n\n\n\n6\n\n\n\n\n1\n\n\n\n\n\n\n\n# retrain model with all the data\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestRegressor(\n        n_jobs=-1, \n        random_state=0,\n        n_estimators=random_search.best_params_['model__n_estimators'],\n        min_samples_split=random_search.best_params_['model__min_samples_split'],\n        min_samples_leaf=random_search.best_params_['model__min_samples_leaf'],\n        max_depth=random_search.best_params_['model__max_depth']))\n])\n\npipe.fit(data[categorical_features], data[target])\n\n\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['Store Number',\n                                                   'Store Grade', 'City',\n                                                   'State', 'Promotion',\n                                                   'Holiday Extent',\n                                                   'Holiday Transferred',\n                                                   'dayofweek'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 RandomForestRegressor(max_depth=9, min_samples_leaf=3,\n                                       min_samples_split=10, n_estimators=10,\n                                       n_jobs=-1, random_state=0))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\nPipeline\n\nPipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=-1))]),\n                                                  ['Store Number',\n                                                   'Store Grade', 'City',\n                                                   'State', 'Promotion',\n                                                   'Holiday Extent',\n                                                   'Holiday Transferred',\n                                                   'dayofweek'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 RandomForestRegressor(max_depth=9, min_samples_leaf=3,\n                                       min_samples_split=10, n_estimators=10,\n                                       n_jobs=-1, random_state=0))])\n\n\n\n\n\n\n\npreprocessors: ColumnTransformer\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1))]),\n                                 ['Store Number', 'Store Grade', 'City',\n                                  'State', 'Promotion', 'Holiday Extent',\n                                  'Holiday Transferred', 'dayofweek'])],\n                  verbose_feature_names_out=False)\n\n\n\n\n\n\n\n\ncat\n\n['Store Number', 'Store Grade', 'City', 'State', 'Promotion', 'Holiday Extent', 'Holiday Transferred', 'dayofweek']\n\n\n\n\n\n\n\n\nOrdinalEncoder\n\nOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n\n\n\n\n\n\n\n\n\n\n\n\nremainder\n\n[]\n\n\n\n\n\n\npassthrough\n\npassthrough\n\n\n\n\n\n\n\n\n\n\nRandomForestRegressor\n\nRandomForestRegressor(max_depth=9, min_samples_leaf=3, min_samples_split=10,\n                      n_estimators=10, n_jobs=-1, random_state=0)\n\n\n\n\n\n\n\ntest_pred = pipe.predict(test_data[categorical_features])\nsubmission_df = pd.read_csv('sample_output.csv')\n\nsubmission_df.head()\n\n\n\n\n\n\n\n\n\n\n\nid\nQuantity\n\n\n\n\n0\n89679192\n3\n\n\n1\n89683748\n11\n\n\n2\n89685203\n5\n\n\n3\n89689659\n6\n\n\n4\n89694936\n3\n\n\n\n\nsubmission_df['Quantity'] = np.rint(test_pred)\n\nsubmission_df.head()\n\n\n\n\n\n\n\n\n\n\n\nid\nQuantity\n\n\n\n\n0\n89679192\n6.0\n\n\n1\n89683748\n5.0\n\n\n2\n89685203\n5.0\n\n\n3\n89689659\n5.0\n\n\n4\n89694936\n5.0\n\n\n\n\n#Submission\nsubmission_df.to_csv('submissions.csv',index=False)"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html",
    "title": "Data challenge - 15. diversity workplace",
    "section": "",
    "text": "see whether results suggest that the company is treating its employees fairly\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#target",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#target",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Target",
    "text": "Target\n\ndef find_outlier(series, k=1.5):\n    \"\"\"Find outlier using first and third quartiles and interquartile range.\n    Parameters\n    ----------\n    series: A pandas Series to find outlier in\n    k: (optional) An integer indicating threshold of outlier in IQR from Q1/Q3\n    Returns\n    -------\n    A pandas Series containing boolean values where True indicates an outlier.\n    \"\"\"\n    q1 = series.quantile(.25)\n    q3 = series.quantile(.75)\n    iqr = q3-q1\n    lower_bound = q1 - k*iqr\n    upper_bound = q3 + k*iqr\n    is_outlier = (series&lt;lower_bound) | (series&gt;upper_bound)\n    return is_outlier\n\n\ndef describe_more(df, features, k=1.5):\n    \"\"\"Provide descriptive statistics and outlier summary for numerical features.\n    Parameters\n    ----------\n    df: A pandas DataFrame to describe\n    features: A list of numerical feature column names to use\n    k: (optional) An integer indicating threshold of outlier in IQR from Q1/Q3\n    Returns\n    -------\n    A pandas DataFrame containing descriptive statistics and outlier summary.\n    \"\"\"\n    descriptives = df[features].describe()\n    outliers = df[features].apply(find_outlier)\n    descriptives.loc['n_outliers']= outliers.sum()\n    descriptives.loc['p_outliers']= outliers.mean()\n    return descriptives\n\n\ndescribe_more(data, ['salary'])\n\n\n\n\n\n\n\n\nsalary\n\n\n\n\ncount\n10000.000000\n\n\nmean\n189111.800000\n\n\nstd\n88973.796898\n\n\nmin\n60000.000000\n\n\n25%\n110000.000000\n\n\n50%\n182000.000000\n\n\n75%\n255000.000000\n\n\nmax\n700000.000000\n\n\nn_outliers\n5.000000\n\n\np_outliers\n0.000500\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(data=data, x=\"salary\", kde=True, ax=ax[0])\nsns.boxplot(data=data, x=\"salary\", ax=ax[1])\nplt.show()\n\n\n\n\n\nLog transform\n\nsns.histplot(x=np.log(data[\"salary\"]), kde=True)\nplt.show()\n\n\n\n\n\n\nQuantile transform\n\nfrom sklearn.preprocessing import quantile_transform\n\nplt.hist(x=quantile_transform(data[\"salary\"].values.reshape(-1, 1)))\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import quantile_transform\n\nplt.hist(x=quantile_transform(data[\"salary\"].values.reshape(-1, 1), output_distribution='normal'))\nplt.show()\n\n\n\n\nThe target variable is skewed and quite high percentage salaries are below 100000."
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#categorical-features",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#categorical-features",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Categorical features",
    "text": "Categorical features\n\ndef plot_categorical(df, feature, target, bins=30, figsize=(14, 5)):\n    \"\"\"Plot histogram, density plot, box plot and swarm plot for feature colour\n    coded by target.\n    Parameters\n    ----------\n    df: A pandas DataFrame to use\n    feature: A string specifying the name of the feature column\n    target: A string specifying the name of the target column\n    bins: (optional) An integer for number of bins in histogram\n    figsize: (optional) A tuple specifying the shape of the plot\n    Returns\n    -------\n    A plot containing 4 subplots. Top left subplot shows number of histogram.\n    Top right subplot shows density plot. Bottom left subplot shows box plot.\n    Bottom right subplot shows swarm plot. Each contains overlaying graphs for\n    each class in target.\n    \"\"\"\n    fig, ax = plt.subplots(2, 2, figsize=(14,8))\n\n    sns.histplot(data=df, x=target, hue=feature, bins=bins, ax=ax[0,0])\n    ax[0,0].set_title(f'Histogram of {feature} by {target}')\n\n    sns.kdeplot(data=df, x=target, hue=feature, fill=True, common_norm=False, ax=ax[0,1])\n    ax[0,1].set_title(f'Density plot of {feature} by {target}')\n\n    sns.boxplot(data=df, y=target, x=feature, ax=ax[1,0])\n    ax[1,0].set_title(f'Box plot of {feature} by {target}')\n\n    sns.violinplot(data=df, y=target, x=feature, ax=ax[1,1])\n    ax[1,1].set_title(f'Violin plot of {feature} by {target}')\n    plt.tight_layout() # To ensure subplots don't overlay\n\n\ncategorical_feats = ['degree_level', 'sex', 'dept', 'employee_level', 'signing_bonus']\ntarget = 'salary'\n\n\nplot_categorical(data, categorical_feats[0], target)\n\n\n\n\n\ndata.groupby(categorical_feats[0])[target].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndegree_level\n\n\n\n\n\n\n\n\n\n\n\n\nBachelor\n2735.0\n187938.939671\n89279.803142\n60000.0\n108000.0\n178000.0\n256000.0\n399000.0\n\n\nHigh_School\n1657.0\n188089.318045\n86725.087545\n60000.0\n113000.0\n181000.0\n249000.0\n397000.0\n\n\nMaster\n2786.0\n188144.651831\n88555.997570\n60000.0\n109000.0\n182000.0\n254000.0\n550000.0\n\n\nPhD\n2822.0\n191803.685330\n90371.052124\n60000.0\n111000.0\n185000.0\n257000.0\n700000.0\n\n\n\n\n\n\n\n\nPhD has the highest median salary, then master.\nInterestingly, high school median salary is higher than bachelor. We may need more data…\n\n\nplot_categorical(data, categorical_feats[1], target)\n\n\n\n\n\ndata.groupby(categorical_feats[1])[target].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n\nF\n3561.0\n171314.518394\n89250.905228\n60000.0\n90000.0\n154000.0\n238000.0\n500000.0\n\n\nM\n6439.0\n198954.340736\n87282.118181\n60000.0\n128000.0\n194000.0\n262000.0\n700000.0\n\n\n\n\n\n\n\n\nMore male workers and the median salary of male is higher than female\nDiversity might be a problem here\n\n\nplot_categorical(data, categorical_feats[2], target)\ndata.groupby(categorical_feats[2])[target].describe()\n\n/var/folders/28/qnfs_5ld2rsdgtf5g7l_vwl80000gn/T/ipykernel_17812/1967144758.py:23: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  sns.kdeplot(data=df, x=target, hue=feature, fill=True, common_norm=False, ax=ax[0,1])\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndept\n\n\n\n\n\n\n\n\n\n\n\n\nCEO\n1.0\n700000.000000\nNaN\n700000.0\n700000.0\n700000.0\n700000.0\n700000.0\n\n\nHR\n1694.0\n84560.212515\n24128.218221\n60000.0\n69000.0\n80000.0\n94000.0\n500000.0\n\n\nengineering\n2696.0\n243524.851632\n87577.443778\n60000.0\n175000.0\n247000.0\n314000.0\n650000.0\n\n\nmarketing\n2010.0\n194623.383085\n75975.972404\n60000.0\n136000.0\n190000.0\n249000.0\n550000.0\n\n\nsales\n3599.0\n194342.039455\n72636.174752\n60000.0\n140000.0\n190000.0\n244500.0\n550000.0\n\n\n\n\n\n\n\n\n\n\n\nHR has the lowest salary\nsales salary is lower than engineering and marketing\nMedian salary for engineering and marketing are the same, while engineering salary has larger variance\n\n\nplot_categorical(data, categorical_feats[3], target)\ndata.groupby(categorical_feats[3])[target].describe()\n\n/var/folders/28/qnfs_5ld2rsdgtf5g7l_vwl80000gn/T/ipykernel_17812/1967144758.py:23: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  sns.kdeplot(data=df, x=target, hue=feature, fill=True, common_norm=False, ax=ax[0,1])\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nemployee_level\n\n\n\n\n\n\n\n\n\n\n\n\nCEO\n1.0\n700000.000000\nNaN\n700000.0\n700000.0\n700000.0\n700000.0\n700000.0\n\n\nD\n160.0\n212343.750000\n89564.076839\n60000.0\n137500.0\n205000.0\n285500.0\n396000.0\n\n\nE\n4.0\n562500.000000\n62915.286961\n500000.0\n537500.0\n550000.0\n575000.0\n650000.0\n\n\nIC\n9000.0\n187909.222222\n88218.397754\n60000.0\n109000.0\n181000.0\n253000.0\n399000.0\n\n\nMM\n800.0\n192473.750000\n90324.193247\n60000.0\n110000.0\n187000.0\n261000.0\n399000.0\n\n\nVP\n35.0\n258028.571429\n80730.665401\n99000.0\n197000.0\n265000.0\n326500.0\n397000.0\n\n\n\n\n\n\n\n\n\n\n\nAs the level goes up, the median salary goes up.\n\n\nplot_categorical(data, categorical_feats[4], target)\ndata.groupby(categorical_feats[4])[target].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsigning_bonus\n\n\n\n\n\n\n\n\n\n\n\n\n0\n6986.0\n182936.587461\n88424.366339\n60000.0\n101000.0\n175000.0\n249000.0\n399000.0\n\n\n1\n3014.0\n203425.016589\n88609.085582\n60000.0\n133000.0\n197500.0\n267000.0\n700000.0\n\n\n\n\n\n\n\n\n\n\n\nMore employees do not have signing bonus and their salary is lower"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#numerical-features",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#numerical-features",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Numerical features",
    "text": "Numerical features\n\nnumerical_feats = ['yrs_experience', 'n_manage_employees']\n\n\nsns.countplot(data=data, x=numerical_feats[0])\nplt.show()\n\n\n\n\n\nsns.lineplot(data=data, x=numerical_feats[0], y=target)\nplt.show()\n\n\n\n\n\nMore years of experience, more salary\n\n\nsns.lineplot(data=data, x=numerical_feats[1], y=target)\nplt.show()\n\n\n\n\n\nsns.lineplot(data=data.query('n_manage_employees &lt; 1000'), x=numerical_feats[1], y=target)\nplt.show()\n\n\n\n\n\nMore managed people, more salary"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#remove-outlier",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#remove-outlier",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Remove outlier",
    "text": "Remove outlier\nWhen I split data into training and testing set, no matter which set contains CEO, their high salary will significantly influence the training or evaluation. So I consider CEO as an outlier, and remove it from the dataset. We also have so few E, remove them as well.\nNormally classes which are rare (for instance N≤5) should be removed, they can only cause overfitting in the model. The goal of a model is to recognize patterns which are sufficiently frequent, not extremely rare events. it’s already hard enough for the model to distinguish between many classes: keep in mind that a baseline model would only reach 0.001 accuracy with 1000 possible classes. (https://datascience.stackexchange.com/questions/112607/some-classes-are-not-present-in-test-set-after-train-test-split)\n\ndf = data.query('employee_level != \"CEO\" and employee_level != \"E\"')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 9995 entries, 0 to 9999\nData columns (total 8 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   signing_bonus       9995 non-null   int64  \n 1   salary              9995 non-null   float64\n 2   degree_level        9995 non-null   object \n 3   sex                 9995 non-null   object \n 4   yrs_experience      9995 non-null   int64  \n 5   dept                9995 non-null   object \n 6   employee_level      9995 non-null   object \n 7   n_manage_employees  9995 non-null   int64  \ndtypes: float64(1), int64(3), object(4)\nmemory usage: 702.8+ KB"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#split-training-and-validation-set",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#split-training-and-validation-set",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Split training and validation set",
    "text": "Split training and validation set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(df.drop(columns=[target]), df[target], random_state=42)\n\n\nX_train.employee_level.value_counts()\n\nIC    6747\nMM     605\nD      119\nVP      25\nName: employee_level, dtype: int64\n\n\n\nX_val.employee_level.value_counts()\n\nIC    2253\nMM     195\nD       41\nVP      10\nName: employee_level, dtype: int64"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#raw-models",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#raw-models",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Raw models",
    "text": "Raw models\nNo hyper parameter tuning, no target variable transformation\n\nRandom forest - ordinal\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestRegressor(n_jobs=-1, random_state=0))\n])\npipe.fit(X_train, y_train)\n\ndef evaluate_model(pipe, X_train, y_train, X_val, y_val):\n    print(f'r2 train = {pipe.score(X_train, y_train)}')\n    print(f'mse train = {mean_squared_error(pipe.predict(X_train), y_train, squared=False)}')\n    print(f'r2 val = {pipe.score(X_val, y_val)}')\n    print(f'mse val = {mean_squared_error(pipe.predict(X_val), y_val, squared=False)}')\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.4437248733608671\nmse train = 66284.03299623956\nr2 val = 0.2859499802795721\nmse val = 73907.70804312085\n\n\n\n\nRandom forest - one hot\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', RandomForestRegressor(n_jobs=-1, random_state=0))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.44391405467842493\nmse train = 66272.76090584774\nr2 val = 0.28509157630306814\nmse val = 73952.11922797018\n\n\nNo model performance difference between ordinal encoder and one hot encoder\n\n\nLinear regression\n\nfrom sklearn.linear_model import LinearRegression\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', LinearRegression(n_jobs=-1))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.3480947549199027\nmse train = 71755.68948556198\nr2 val = 0.33337111204042913\nmse val = 71411.38918760384\n\n\n\n\nRidge regression\n\nfrom sklearn.linear_model import Ridge\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', Ridge())\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.34809455613123663\nmse train = 71755.70042596843\nr2 val = 0.3333738824145265\nmse val = 71411.24080184381"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#raw-models-transform-target-variable",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#raw-models-transform-target-variable",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Raw models, transform target variable",
    "text": "Raw models, transform target variable\n\nfrom sklearn.model_selection import train_test_split\n\ndf = data.query('employee_level != \"CEO\"')\nX_train, X_val, y_train, y_val = train_test_split(df.drop(columns=[target]), df[target], random_state=99)\n\n# transform into log scale\n# y_train = np.log(y_train)\n# y_val = np.log(y_val)\n\n\nX_train.employee_level.value_counts()\n\nIC    6758\nMM     599\nD      115\nVP      25\nE        2\nName: employee_level, dtype: int64\n\n\n\nX_val.employee_level.value_counts()\n\nIC    2242\nMM     201\nD       45\nVP      10\nE        2\nName: employee_level, dtype: int64\n\n\n\nRandom forest - ordinal\n\nfrom sklearn.compose import TransformedTargetRegressor\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1, random_state=0), func=np.log, inverse_func=np.exp))\n])\n\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.4170272266344167\nmse train = 67606.39620278434\nr2 val = 0.27173442168376716\nmse val = 76508.16556251363\n\n\n\n\nRandom forest - one hot\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1, random_state=0), func=np.log, inverse_func=np.exp))\n])\n\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.41730140410011096\nmse train = 67590.49637764174\nr2 val = 0.27166109383831527\nmse val = 76512.0172053033\n\n\n\n\nLinear regression - log transform\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=LinearRegression(n_jobs=-1), func=np.log, inverse_func=np.exp))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.3176719409092008\nmse train = 73140.89419230737\nr2 val = 0.3177779920634983\nmse val = 74050.1188598946\n\n\n\n\nLinear regression - quantile normal\n\nfrom sklearn.preprocessing import QuantileTransformer\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=LinearRegression(n_jobs=-1), transformer=QuantileTransformer(n_quantiles=1000, output_distribution=\"normal\")))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.34229521534154883\nmse train = 71809.04494921435\nr2 val = 0.34944070093022395\nmse val = 72311.32873087155\n\n\n\n\nLinear regression - quantile uniform\n\nfrom sklearn.preprocessing import QuantileTransformer\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=LinearRegression(n_jobs=-1), transformer=QuantileTransformer(n_quantiles=1000, output_distribution=\"uniform\")))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.3356540649357288\nmse train = 72170.67896973116\nr2 val = 0.34852666330346427\nmse val = 72362.10970939188\n\n\n\n\nRidge - log transform\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=Ridge(), func=np.log, inverse_func=np.exp))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.315697108288448\nmse train = 73246.6619827823\nr2 val = 0.32749880053581226\nmse val = 73520.66539391871\n\n\n\n\nLinear regression - quantile\n\nfrom sklearn.preprocessing import QuantileTransformer\n\ncategorical_pipe = Pipeline([\n    ('encoder', OneHotEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough')\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=Ridge(), transformer=QuantileTransformer(n_quantiles=1000, output_distribution=\"normal\")))\n])\npipe.fit(X_train, y_train)\n\nevaluate_model(pipe, X_train, y_train, X_val, y_val)\n\nr2 train = 0.34208961522430625\nmse train = 71820.26791335705\nr2 val = 0.3501447649223953\nmse val = 72272.1888822311\n\n\nafter transformation, the mse is larger!?"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#random-forest-tuning",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#random-forest-tuning",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Random forest, tuning",
    "text": "Random forest, tuning\n\nfrom sklearn import set_config\n\n# set_config(transform_output=\"pandas\")\nset_config(transform_output=\"default\")\n\n\nfrom sklearn import get_config\n\nget_config()\n\n{'assume_finite': False,\n 'working_memory': 1024,\n 'print_changed_only': True,\n 'display': 'diagram',\n 'pairwise_dist_chunk_size': 256,\n 'enable_cython_pairwise_dist': True,\n 'array_api_dispatch': False,\n 'transform_output': 'default'}\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough', verbose_feature_names_out=False)\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1, random_state=0),\n                                         transformer=QuantileTransformer(n_quantiles=1000, output_distribution=\"normal\")))\n])\n\n# pipe = Pipeline([\n#     ('preprocessors', preprocessors),\n#     ('model', TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1, random_state=0), func=np.log, inverse_func=np.exp))\n# ])\n\nparam_grid = {\n    \"model__regressor__n_estimators\": [10, 25, 30, 40],\n    'model__regressor__max_depth': [4, 5, 7]\n}\n\nsearch = GridSearchCV(pipe, param_grid, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nGridSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                        ColumnTransformer(remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         Pipeline(steps=[('encoder',\n                                                                                          OrdinalEncoder())]),\n                                                                         ['degree_level',\n                                                                          'sex',\n                                                                          'dept',\n                                                                          'employee_level',\n                                                                          'signing_bonus'])],\n                                                          verbose_feature_names_out=False)),\n                                       ('model',\n                                        TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                                                                   random_state=0),\n                                                                   transformer=QuantileTransformer(output_distribution='normal')))]),\n             n_jobs=-1,\n             param_grid={'model__regressor__max_depth': [4, 5, 7],\n                         'model__regressor__n_estimators': [10, 25, 30, 40]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                        ColumnTransformer(remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         Pipeline(steps=[('encoder',\n                                                                                          OrdinalEncoder())]),\n                                                                         ['degree_level',\n                                                                          'sex',\n                                                                          'dept',\n                                                                          'employee_level',\n                                                                          'signing_bonus'])],\n                                                          verbose_feature_names_out=False)),\n                                       ('model',\n                                        TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                                                                   random_state=0),\n                                                                   transformer=QuantileTransformer(output_distribution='normal')))]),\n             n_jobs=-1,\n             param_grid={'model__regressor__max_depth': [4, 5, 7],\n                         'model__regressor__n_estimators': [10, 25, 30, 40]})estimator: PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder())]),\n                                                  ['degree_level', 'sex',\n                                                   'dept', 'employee_level',\n                                                   'signing_bonus'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                                            random_state=0),\n                                            transformer=QuantileTransformer(output_distribution='normal')))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder())]),\n                                 ['degree_level', 'sex', 'dept',\n                                  'employee_level', 'signing_bonus'])],\n                  verbose_feature_names_out=False)cat['degree_level', 'sex', 'dept', 'employee_level', 'signing_bonus']OrdinalEncoderOrdinalEncoder()remainderpassthroughpassthroughmodel: TransformedTargetRegressorTransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                           random_state=0),\n                           transformer=QuantileTransformer(output_distribution='normal'))regressor: RandomForestRegressorRandomForestRegressor(n_jobs=-1, random_state=0)RandomForestRegressorRandomForestRegressor(n_jobs=-1, random_state=0)transformer: QuantileTransformerQuantileTransformer(output_distribution='normal')QuantileTransformerQuantileTransformer(output_distribution='normal')\n\n\n\npipe.get_params()\n\n{'memory': None,\n 'steps': [('preprocessors',\n   ColumnTransformer(remainder='passthrough',\n                     transformers=[('cat',\n                                    Pipeline(steps=[('encoder',\n                                                     OrdinalEncoder())]),\n                                    ['degree_level', 'sex', 'dept',\n                                     'employee_level', 'signing_bonus'])],\n                     verbose_feature_names_out=False)),\n  ('model',\n   TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                              random_state=0),\n                              transformer=QuantileTransformer(output_distribution='normal')))],\n 'verbose': False,\n 'preprocessors': ColumnTransformer(remainder='passthrough',\n                   transformers=[('cat',\n                                  Pipeline(steps=[('encoder',\n                                                   OrdinalEncoder())]),\n                                  ['degree_level', 'sex', 'dept',\n                                   'employee_level', 'signing_bonus'])],\n                   verbose_feature_names_out=False),\n 'model': TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1,\n                                                            random_state=0),\n                            transformer=QuantileTransformer(output_distribution='normal')),\n 'preprocessors__n_jobs': None,\n 'preprocessors__remainder': 'passthrough',\n 'preprocessors__sparse_threshold': 0.3,\n 'preprocessors__transformer_weights': None,\n 'preprocessors__transformers': [('cat',\n   Pipeline(steps=[('encoder', OrdinalEncoder())]),\n   ['degree_level', 'sex', 'dept', 'employee_level', 'signing_bonus'])],\n 'preprocessors__verbose': False,\n 'preprocessors__verbose_feature_names_out': False,\n 'preprocessors__cat': Pipeline(steps=[('encoder', OrdinalEncoder())]),\n 'preprocessors__cat__memory': None,\n 'preprocessors__cat__steps': [('encoder', OrdinalEncoder())],\n 'preprocessors__cat__verbose': False,\n 'preprocessors__cat__encoder': OrdinalEncoder(),\n 'preprocessors__cat__encoder__categories': 'auto',\n 'preprocessors__cat__encoder__dtype': numpy.float64,\n 'preprocessors__cat__encoder__encoded_missing_value': nan,\n 'preprocessors__cat__encoder__handle_unknown': 'error',\n 'preprocessors__cat__encoder__unknown_value': None,\n 'model__check_inverse': True,\n 'model__func': None,\n 'model__inverse_func': None,\n 'model__regressor__bootstrap': True,\n 'model__regressor__ccp_alpha': 0.0,\n 'model__regressor__criterion': 'squared_error',\n 'model__regressor__max_depth': None,\n 'model__regressor__max_features': 1.0,\n 'model__regressor__max_leaf_nodes': None,\n 'model__regressor__max_samples': None,\n 'model__regressor__min_impurity_decrease': 0.0,\n 'model__regressor__min_samples_leaf': 1,\n 'model__regressor__min_samples_split': 2,\n 'model__regressor__min_weight_fraction_leaf': 0.0,\n 'model__regressor__n_estimators': 100,\n 'model__regressor__n_jobs': -1,\n 'model__regressor__oob_score': False,\n 'model__regressor__random_state': 0,\n 'model__regressor__verbose': 0,\n 'model__regressor__warm_start': False,\n 'model__regressor': RandomForestRegressor(n_jobs=-1, random_state=0),\n 'model__transformer__copy': True,\n 'model__transformer__ignore_implicit_zeros': False,\n 'model__transformer__n_quantiles': 1000,\n 'model__transformer__output_distribution': 'normal',\n 'model__transformer__random_state': None,\n 'model__transformer__subsample': 10000,\n 'model__transformer': QuantileTransformer(output_distribution='normal')}\n\n\n\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\n# pipe.fit(X_train, y_train)\n\nevaluate_model(search.best_estimator_,  X_train, y_train, X_val, y_val)\n\nBest parameter (CV score=0.334):\n{'model__regressor__max_depth': 4, 'model__regressor__n_estimators': 25}\nr2 train = 0.3453204144601556\nmse train = 71643.70706296277\nr2 val = 0.34253068731868885\nmse val = 72694.34718177075\n\n\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough', verbose_feature_names_out=False)\n\n# pipe = Pipeline([\n#     ('preprocessors', preprocessors),\n#     ('model', RandomForestRegressor(n_jobs=-1, random_state=0))\n# ])\n\npipe = Pipeline([\n    ('preprocessors', preprocessors),\n    ('model', TransformedTargetRegressor(regressor=RandomForestRegressor(n_jobs=-1, random_state=0), func=np.log, inverse_func=np.exp))\n])\n\nn_estimators = [25,50,100] # number of trees in the random forest\n# max_features = ['auto', 'sqrt'] # number of features in consideration at every split\nmax_depth = [int(x) for x in np.linspace(1, 19, num = 10)] # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10] # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\n\nrandom_grid = {'model__regressor__n_estimators': n_estimators,\n               # 'model__regressor__max_features': max_features,\n               'model__regressor__max_depth': max_depth,\n               'model__regressor__min_samples_split': min_samples_split,\n               'model__regressor__min_samples_leaf': min_samples_leaf}\n\n# random_grid = {\n#     \"model__n_estimators\": [10, 25, 30, 40],\n#     'model__max_depth': [4, 5, 7]\n# }\n\nrandom_search = RandomizedSearchCV(pipe, random_grid, n_iter=100, n_jobs=-1)\nrandom_search.fit(X_train, y_train)\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder())]),\n                                                                               ['degree_level',\n                                                                                'sex',\n                                                                                'dept',\n                                                                                'employee_level',\n                                                                                'signing_bonus'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              TransformedTargetRegressor(func=&lt;ufunc 'log'&gt;,\n                                                                         inverse_func=&lt;ufunc 'exp'&gt;,\n                                                                         regressor=RandomForestRegressor(n_jobs=-1,\n                                                                                                         random_state=0)))]),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'model__regressor__max_depth': [1, 3, 5,\n                                                                        7, 9,\n                                                                        11, 13,\n                                                                        15, 17,\n                                                                        19],\n                                        'model__regressor__min_samples_leaf': [1,\n                                                                               3,\n                                                                               4],\n                                        'model__regressor__min_samples_split': [2,\n                                                                                6,\n                                                                                10],\n                                        'model__regressor__n_estimators': [25,\n                                                                           50,\n                                                                           100]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(estimator=Pipeline(steps=[('preprocessors',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('encoder',\n                                                                                                OrdinalEncoder())]),\n                                                                               ['degree_level',\n                                                                                'sex',\n                                                                                'dept',\n                                                                                'employee_level',\n                                                                                'signing_bonus'])],\n                                                                verbose_feature_names_out=False)),\n                                             ('model',\n                                              TransformedTargetRegressor(func=&lt;ufunc 'log'&gt;,\n                                                                         inverse_func=&lt;ufunc 'exp'&gt;,\n                                                                         regressor=RandomForestRegressor(n_jobs=-1,\n                                                                                                         random_state=0)))]),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'model__regressor__max_depth': [1, 3, 5,\n                                                                        7, 9,\n                                                                        11, 13,\n                                                                        15, 17,\n                                                                        19],\n                                        'model__regressor__min_samples_leaf': [1,\n                                                                               3,\n                                                                               4],\n                                        'model__regressor__min_samples_split': [2,\n                                                                                6,\n                                                                                10],\n                                        'model__regressor__n_estimators': [25,\n                                                                           50,\n                                                                           100]})estimator: PipelinePipeline(steps=[('preprocessors',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder())]),\n                                                  ['degree_level', 'sex',\n                                                   'dept', 'employee_level',\n                                                   'signing_bonus'])],\n                                   verbose_feature_names_out=False)),\n                ('model',\n                 TransformedTargetRegressor(func=&lt;ufunc 'log'&gt;,\n                                            inverse_func=&lt;ufunc 'exp'&gt;,\n                                            regressor=RandomForestRegressor(n_jobs=-1,\n                                                                            random_state=0)))])preprocessors: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('encoder',\n                                                  OrdinalEncoder())]),\n                                 ['degree_level', 'sex', 'dept',\n                                  'employee_level', 'signing_bonus'])],\n                  verbose_feature_names_out=False)cat['degree_level', 'sex', 'dept', 'employee_level', 'signing_bonus']OrdinalEncoderOrdinalEncoder()remainderpassthroughpassthroughmodel: TransformedTargetRegressorTransformedTargetRegressor(func=&lt;ufunc 'log'&gt;, inverse_func=&lt;ufunc 'exp'&gt;,\n                           regressor=RandomForestRegressor(n_jobs=-1,\n                                                           random_state=0))regressor: RandomForestRegressorRandomForestRegressor(n_jobs=-1, random_state=0)RandomForestRegressorRandomForestRegressor(n_jobs=-1, random_state=0)\n\n\n\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\n# pipe.fit(X_train, y_train)\n\nevaluate_model(random_search.best_estimator_,  X_train, y_train, X_val, y_val)\n\nBest parameter (CV score=0.334):\n{'model__regressor__max_depth': 4, 'model__regressor__n_estimators': 25}\nr2 train = 0.3160787126598168\nmse train = 73226.23598092796\nr2 val = 0.32019738738618575\nmse val = 73918.69856771182"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#gini-importance",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#gini-importance",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Gini importance",
    "text": "Gini importance\n\nfeat_names = search.best_estimator_[0].get_feature_names_out()#search.best_estimator_[-1].regressor_.feature_names_in_  # TODO: ordinal encoding version\n# one hot encoding version, feat_names = search.best_estimator_[:-1].get_feature_names_out()\nfeat_importances = search.best_estimator_[-1].regressor_.feature_importances_\n\nfeat_importances = pd.DataFrame({\"name\":feat_names,\"importance\":feat_importances})\nfeat_importances = feat_importances[['name','importance']]# reorder the columns\nfeat_importances.sort_values(by=\"importance\",inplace=True,ascending=False)\nfeat_importances\n\n\n\n\n\n\n\n\nname\nimportance\n\n\n\n\n2\ndept\n0.958797\n\n\n6\nn_manage_employees\n0.026348\n\n\n5\nyrs_experience\n0.009654\n\n\n3\nemployee_level\n0.002117\n\n\n0\ndegree_level\n0.001590\n\n\n1\nsex\n0.001309\n\n\n4\nsigning_bonus\n0.000185\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 8))\nsns.barplot(data=feat_importances, x='importance', y='name', ax=ax)\nplt.show()\n\n\n\n\n\nfeat_names = random_search.best_estimator_[0].get_feature_names_out()#random_search.best_estimator_[-1].feature_names_in_  # TODO: ordinal encoding version\n# one hot encoding version, feat_names = search.best_estimator_[:-1].get_feature_names_out()\nfeat_importances = random_search.best_estimator_[-1].regressor_.feature_importances_\n\nfeat_importances = pd.DataFrame({\"name\":feat_names,\"importance\":feat_importances})\nfeat_importances = feat_importances[['name','importance']]# reorder the columns\nfeat_importances.sort_values(by=\"importance\",inplace=True,ascending=False)\nfeat_importances\n\n\n\n\n\n\n\n\nname\nimportance\n\n\n\n\n2\ndept\n0.980856\n\n\n6\nn_manage_employees\n0.014369\n\n\n5\nyrs_experience\n0.004323\n\n\n3\nemployee_level\n0.000216\n\n\n0\ndegree_level\n0.000191\n\n\n4\nsigning_bonus\n0.000028\n\n\n1\nsex\n0.000017\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 8))\nsns.barplot(data=feat_importances, x='importance', y='name', ax=ax)\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#permutation-importance",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#permutation-importance",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Permutation importance",
    "text": "Permutation importance\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    search.best_estimator_, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=random_search.best_estimator_[0].get_feature_names_out()#X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    search.best_estimator_, X_val, y_val, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=random_search.best_estimator_[0].get_feature_names_out()#X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (validation set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#partial-dependence",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#partial-dependence",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Partial dependence",
    "text": "Partial dependence\n\nX_val.columns\n\nIndex(['signing_bonus', 'degree_level', 'sex', 'yrs_experience', 'dept',\n       'employee_level', 'n_manage_employees'],\n      dtype='object')\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\ncommon_params = {\n    \"subsample\": 50,\n    \"n_jobs\": 2,\n    \"grid_resolution\": 20,\n    \"random_state\": 0,\n}\n\nprint(\"Computing partial dependence plots...\")\nfeatures_info = {\n    # features of interest\n    \"features\": ['signing_bonus', 'degree_level', 'sex', 'yrs_experience', 'dept', 'employee_level'],\n    # type of partial dependence plot\n    \"kind\": \"average\",\n    # information regarding categorical features\n    \"categorical_features\": categorical_feats,\n}\n\n_, ax = plt.subplots(ncols=3, nrows=2, figsize=(9, 8), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(\n    search.best_estimator_,\n    X_train,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n# print(f\"done in {time() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"Partial dependence of the salary\\n\"\n    \"for the random forest\",\n    fontsize=16,\n)\n\nComputing partial dependence plots...\n\n\n\n\n\n\n_, ax = plt.subplots(ncols=3, nrows=2, figsize=(9, 8), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(\n    search.best_estimator_,\n    X_val,\n    **features_info,\n    ax=ax,\n    **common_params,\n)\n# print(f\"done in {time() - tic:.3f}s\")\n_ = display.figure_.suptitle(\n    \"Partial dependence of the salary\\n\"\n    \"for the random forest\",\n    fontsize=16,\n)"
  },
  {
    "objectID": "posts/2022-12-11-data15-diversity/diversity15.html#residual",
    "href": "posts/2022-12-11-data15-diversity/diversity15.html#residual",
    "title": "Data challenge - 15. diversity workplace",
    "section": "Residual",
    "text": "Residual\n\nfrom sklearn.metrics import r2_score\n\n\ndef compute_score(y_true, y_pred):\n    return {\n        \"R2\": f\"{r2_score(y_true, y_pred):.3f}\",\n        \"RMSE\": f\"{mean_squared_error(y_true, y_pred, squared=False):.3f}\",\n    }\n\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n\ny_pred_search = search.best_estimator_.predict(X_val)\ny_pred_random_search = random_search.best_estimator_.predict(X_val)\n\nPredictionErrorDisplay.from_predictions(\n    y_val,\n    y_pred_search,\n    kind=\"actual_vs_predicted\",\n    ax=ax0,\n    scatter_kwargs={\"alpha\": 0.5},\n)\nPredictionErrorDisplay.from_predictions(\n    y_val,\n    y_pred_random_search,\n    kind=\"actual_vs_predicted\",\n    ax=ax1,\n    scatter_kwargs={\"alpha\": 0.5},\n)\n\n# Add the score in the legend of each axis\nfor ax, y_pred in zip([ax0, ax1], [y_pred_search, y_pred_random_search]):\n    for name, score in compute_score(y_val, y_pred).items():\n        ax.plot([], [], \" \", label=f\"{name}={score}\")\n    ax.legend(loc=\"upper left\")\n\n# ax0.set_title(\"Ridge regression \\n without target transformation\")\n# ax1.set_title(\"Ridge regression \\n with target transformation\")\nf.suptitle(\"Synthetic data\", y=1.05)\nplt.tight_layout()\n\n\n\n\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n\ny_pred_search = search.best_estimator_.predict(X_val)\ny_pred_random_search = random_search.best_estimator_.predict(X_val)\n\nPredictionErrorDisplay.from_predictions(\n    y_val,\n    y_pred_search,\n    kind=\"residual_vs_predicted\",\n    ax=ax0,\n    scatter_kwargs={\"alpha\": 0.5},\n)\nPredictionErrorDisplay.from_predictions(\n    y_val,\n    y_pred_random_search,\n    kind=\"residual_vs_predicted\",\n    ax=ax1,\n    scatter_kwargs={\"alpha\": 0.5},\n)\n\n# Add the score in the legend of each axis\nfor ax, y_pred in zip([ax0, ax1], [y_pred_search, y_pred_random_search]):\n    for name, score in compute_score(y_val, y_pred).items():\n        ax.plot([], [], \" \", label=f\"{name}={score}\")\n    ax.legend(loc=\"upper left\")\n\n# ax0.set_title(\"Ridge regression \\n without target transformation\")\n# ax1.set_title(\"Ridge regression \\n with target transformation\")\nf.suptitle(\"Synthetic data\", y=1.05)\nplt.tight_layout()\n\n\n\n\n\nBias\n\nbias = y_val - y_pred_search\n\n\nplt.hist(bias, bins=60)\n\n(array([  1.,   0.,   0.,   7.,   9.,  10.,   9.,  14.,  19.,  48.,  43.,\n         60.,  73.,  68.,  75.,  70.,  92., 102.,  96.,  68., 167., 201.,\n        185., 160., 121.,  99., 105.,  78.,  81.,  64.,  65.,  66.,  56.,\n         42.,  35.,  35.,  28.,  11.,   9.,   7.,   7.,   8.,   4.,   0.,\n          0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   1.]),\n array([-218000.        , -208033.33333333, -198066.66666667,\n        -188100.        , -178133.33333333, -168166.66666667,\n        -158200.        , -148233.33333333, -138266.66666667,\n        -128300.        , -118333.33333333, -108366.66666667,\n         -98400.        ,  -88433.33333333,  -78466.66666667,\n         -68500.        ,  -58533.33333333,  -48566.66666667,\n         -38600.        ,  -28633.33333333,  -18666.66666667,\n          -8700.        ,    1266.66666667,   11233.33333333,\n          21200.        ,   31166.66666667,   41133.33333333,\n          51100.        ,   61066.66666667,   71033.33333333,\n          81000.        ,   90966.66666667,  100933.33333333,\n         110900.        ,  120866.66666667,  130833.33333333,\n         140800.        ,  150766.66666667,  160733.33333333,\n         170700.        ,  180666.66666667,  190633.33333333,\n         200600.        ,  210566.66666667,  220533.33333333,\n         230500.        ,  240466.66666667,  250433.33333333,\n         260400.        ,  270366.66666667,  280333.33333333,\n         290300.        ,  300266.66666667,  310233.33333333,\n         320200.        ,  330166.66666667,  340133.33333333,\n         350100.        ,  360066.66666667,  370033.33333333,\n         380000.        ]),\n &lt;BarContainer object of 60 artists&gt;)\n\n\n\n\n\n\nplt.scatter(y_val, bias)\nplt.xlabel(\"true salary\")\nplt.ylabel(\"bias\")\nplt.show()\n\n\n\n\nfrom above plot, we can see that, employee with high salary are more likely to be overpaid.\nhttps://github.com/stasi009/TakeHomeDataChallenges/blob/master/15.WorkPlace/diverse_work_place.ipynb\n\nX_train\n\n\n\n\n\n\n\n\nsigning_bonus\ndegree_level\nsex\nyrs_experience\ndept\nemployee_level\nn_manage_employees\n\n\n\n\n8016\n1\nPhD\nF\n4\nHR\nIC\n0\n\n\n6613\n1\nBachelor\nM\n2\nsales\nIC\n0\n\n\n4993\n0\nMaster\nM\n2\nHR\nIC\n0\n\n\n7805\n1\nHigh_School\nM\n1\nengineering\nIC\n0\n\n\n7028\n0\nMaster\nM\n1\nengineering\nIC\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7891\n0\nPhD\nM\n1\nsales\nIC\n0\n\n\n1768\n0\nMaster\nM\n3\nHR\nIC\n0\n\n\n9930\n0\nPhD\nF\n3\nmarketing\nIC\n0\n\n\n3240\n0\nBachelor\nM\n6\nHR\nIC\n0\n\n\n7204\n0\nBachelor\nM\n9\nsales\nIC\n0\n\n\n\n\n7499 rows × 7 columns\n\n\n\n\nfrom sklearn.feature_selection import chi2\n\ncategorical_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\npreprocessors = ColumnTransformer(transformers=[\n    ('cat', categorical_pipe, categorical_feats)], remainder = 'passthrough', verbose_feature_names_out=False)\n\ntemp = preprocessors.fit(X_train)\n# [['signing_bonus', 'degree_level', 'sex', 'yrs_expe']]\nscores, pvalues = chi2(temp.transform(X_train),y_train)\n\npd.DataFrame({'score': scores, 'pvalue': pvalues},index = temp.get_feature_names_out()).sort_values(by='pvalue')\n\n\n\n\n\n\n\n\nscore\npvalue\n\n\n\n\nn_manage_employees\n4.595120e+06\n0.000000e+00\n\n\ndept\n1.425509e+03\n2.514362e-132\n\n\nyrs_experience\n1.102725e+03\n4.301914e-81\n\n\nsigning_bonus\n3.018100e+02\n9.377990e-01\n\n\ndegree_level\n2.941529e+02\n9.683774e-01\n\n\nsex\n1.833096e+02\n1.000000e+00\n\n\nemployee_level\n2.949487e+01\n1.000000e+00\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.scatter(X_val.n_manage_employees,bias)\nplt.xlabel(\"#subordinates\")\nplt.ylabel(\"bias\")\n\nText(0, 0.5, 'bias')\n\n\n\n\n\nfrom above plot, we can see that, more subordinates (i.e., higher level), more positive bias, means more likely to be overpaid.\nI think, in large scope, the company treat its employee fairly. the difference among different sex and departments seems normal and fair enough.\nthe only bias I can see is that, the management level is likely to be overpaid. Higher the level, more likely to be overpaid.\nI may suggest him/her considering lower down the salaries in high management level, which may be more fair to the workforce."
  },
  {
    "objectID": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html",
    "href": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html",
    "title": "pandas groupby, apply and agg",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#show-1-column",
    "href": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#show-1-column",
    "title": "pandas groupby, apply and agg",
    "section": "show 1 column",
    "text": "show 1 column\n\nplanets.groupby('method')['orbital_period'].median()\n\nmethod\nAstrometry                         631.180000\nEclipse Timing Variations         4343.500000\nImaging                          27500.000000\nMicrolensing                      3300.000000\nOrbital Brightness Modulation        0.342887\nPulsar Timing                       66.541900\nPulsation Timing Variations       1170.000000\nRadial Velocity                    360.200000\nTransit                              5.714932\nTransit Timing Variations           57.011000\nName: orbital_period, dtype: float64"
  },
  {
    "objectID": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#describe-different-subsets",
    "href": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#describe-different-subsets",
    "title": "pandas groupby, apply and agg",
    "section": "describe different subsets",
    "text": "describe different subsets\n\nplanets.groupby('method')['year'].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nmethod\n\n\n\n\n\n\n\n\n\n\n\n\nAstrometry\n2.0\n2011.500000\n2.121320\n2010.0\n2010.75\n2011.5\n2012.25\n2013.0\n\n\nEclipse Timing Variations\n9.0\n2010.000000\n1.414214\n2008.0\n2009.00\n2010.0\n2011.00\n2012.0\n\n\nImaging\n38.0\n2009.131579\n2.781901\n2004.0\n2008.00\n2009.0\n2011.00\n2013.0\n\n\nMicrolensing\n23.0\n2009.782609\n2.859697\n2004.0\n2008.00\n2010.0\n2012.00\n2013.0\n\n\nOrbital Brightness Modulation\n3.0\n2011.666667\n1.154701\n2011.0\n2011.00\n2011.0\n2012.00\n2013.0\n\n\nPulsar Timing\n5.0\n1998.400000\n8.384510\n1992.0\n1992.00\n1994.0\n2003.00\n2011.0\n\n\nPulsation Timing Variations\n1.0\n2007.000000\nNaN\n2007.0\n2007.00\n2007.0\n2007.00\n2007.0\n\n\nRadial Velocity\n553.0\n2007.518987\n4.249052\n1989.0\n2005.00\n2009.0\n2011.00\n2014.0\n\n\nTransit\n397.0\n2011.236776\n2.077867\n2002.0\n2010.00\n2012.0\n2013.00\n2014.0\n\n\nTransit Timing Variations\n4.0\n2012.500000\n1.290994\n2011.0\n2011.75\n2012.5\n2013.25\n2014.0\n\n\n\n\n\n\n\n\nplanets.groupby('method')['year'].apply('describe')\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nmethod\n\n\n\n\n\n\n\n\n\n\n\n\nAstrometry\n2.0\n2011.500000\n2.121320\n2010.0\n2010.75\n2011.5\n2012.25\n2013.0\n\n\nEclipse Timing Variations\n9.0\n2010.000000\n1.414214\n2008.0\n2009.00\n2010.0\n2011.00\n2012.0\n\n\nImaging\n38.0\n2009.131579\n2.781901\n2004.0\n2008.00\n2009.0\n2011.00\n2013.0\n\n\nMicrolensing\n23.0\n2009.782609\n2.859697\n2004.0\n2008.00\n2010.0\n2012.00\n2013.0\n\n\nOrbital Brightness Modulation\n3.0\n2011.666667\n1.154701\n2011.0\n2011.00\n2011.0\n2012.00\n2013.0\n\n\nPulsar Timing\n5.0\n1998.400000\n8.384510\n1992.0\n1992.00\n1994.0\n2003.00\n2011.0\n\n\nPulsation Timing Variations\n1.0\n2007.000000\nNaN\n2007.0\n2007.00\n2007.0\n2007.00\n2007.0\n\n\nRadial Velocity\n553.0\n2007.518987\n4.249052\n1989.0\n2005.00\n2009.0\n2011.00\n2014.0\n\n\nTransit\n397.0\n2011.236776\n2.077867\n2002.0\n2010.00\n2012.0\n2013.00\n2014.0\n\n\nTransit Timing Variations\n4.0\n2012.500000\n1.290994\n2011.0\n2011.75\n2012.5\n2013.25\n2014.0\n\n\n\n\n\n\n\n\nplanets.groupby('method')['year'].apply('max')\n\nmethod\nAstrometry                       2013\nEclipse Timing Variations        2012\nImaging                          2013\nMicrolensing                     2013\nOrbital Brightness Modulation    2013\nPulsar Timing                    2011\nPulsation Timing Variations      2007\nRadial Velocity                  2014\nTransit                          2014\nTransit Timing Variations        2014\nName: year, dtype: int64\n\n\n\nrng = np.random.RandomState(0)\ndf = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n                   'data1': range(6),\n                   'data2': rng.randint(0, 10, 6)},\n                   columns = ['key', 'data1', 'data2'])\ndf\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\nA\n0\n5\n\n\n1\nB\n1\n0\n\n\n2\nC\n2\n3\n\n\n3\nA\n3\n3\n\n\n4\nB\n4\n7\n\n\n5\nC\n5\n9"
  },
  {
    "objectID": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#aggregate",
    "href": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#aggregate",
    "title": "pandas groupby, apply and agg",
    "section": "aggregate()",
    "text": "aggregate()\n\ndf['data1'].aggregate(['min', np.median, max])\n\nmin       0.0\nmedian    2.5\nmax       5.0\nName: data1, dtype: float64\n\n\n\ndf.groupby('key').aggregate(['min', np.median, max])\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nkey\n\n\n\n\n\n\n\n\n\n\nA\n0\n1.5\n3\n3\n4.0\n5\n\n\nB\n1\n2.5\n4\n0\n3.5\n7\n\n\nC\n2\n3.5\n5\n3\n6.0\n9\n\n\n\n\n\n\n\n\ndf.groupby('key').aggregate({'data1': 'min',\n                             'data2': 'max'})\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\nkey\n\n\n\n\n\n\nA\n0\n5\n\n\nB\n1\n7\n\n\nC\n2\n9"
  },
  {
    "objectID": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#filter",
    "href": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#filter",
    "title": "pandas groupby, apply and agg",
    "section": "filter()",
    "text": "filter()\ndrop data based on the group properties\n\ndef filter_func(x):\n    return x['data2'].std() &gt; 4\n\n\ndf.groupby('key').std()\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\nkey\n\n\n\n\n\n\nA\n2.12132\n1.414214\n\n\nB\n2.12132\n4.949747\n\n\nC\n2.12132\n4.242641\n\n\n\n\n\n\n\n\ndf.groupby('key').filter(filter_func)\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n1\nB\n1\n0\n\n\n2\nC\n2\n3\n\n\n4\nB\n4\n7\n\n\n5\nC\n5\n9"
  },
  {
    "objectID": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#transform",
    "href": "posts/2022-12-01-pandas-groupby-agg/pandas_groupby_agg.html#transform",
    "title": "pandas groupby, apply and agg",
    "section": "transform()",
    "text": "transform()\ntransformation can return some transformed version of the full data to recombine\n\ndf.groupby('key').transform(lambda x: x - x.mean())\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\n\n\n0\n-1.5\n1.0\n\n\n1\n-1.5\n-3.5\n\n\n2\n-1.5\n-3.0\n\n\n3\n1.5\n-1.0\n\n\n4\n1.5\n3.5\n\n\n5\n1.5\n3.0\n\n\n\n\n\n\n\n\ndef center_func(df):\n    return df - df.mean()\n\ndf.groupby('key').transform(center_func)\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\n\n\n0\n-1.5\n1.0\n\n\n1\n-1.5\n-3.5\n\n\n2\n-1.5\n-3.0\n\n\n3\n1.5\n-1.0\n\n\n4\n1.5\n3.5\n\n\n5\n1.5\n3.0"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html",
    "href": "posts/2022-08-01-leetcode/index.html",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -&gt; Optional[ListNode]:\n        # iterative way\n        \n        # create dummy head\n        dummy = ListNode(-1)\n        p = dummy  # p pointer\n        \n        while list1 != None and list2 != None:\n            if list1.val &lt; list2.val:\n                p.next = list1  # attach all list2 to the end of p, p is dynamically changing\n                list1 = list1.next  # remove the first node\n            else:\n                p.next = list2\n                list2 = list2.next\n            p = p.next  # remove the first node\n            \n        if list1 != None:\n            p.next = list1\n            \n        if list2 != None:\n            p.next = list2\n            \n        return dummy.next\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def partition(self, head: Optional[ListNode], x: int) -&gt; Optional[ListNode]:\n        # create 2 linked lists and join them\n        p1 = dummy1 = ListNode(-1)  # p: pointer, before moves the pointer, before_head keeps the linked list\n        p2 = dummy2 = ListNode(-1)\n        p = head\n        \n        while p != None:\n            if p.val &lt; x:\n                p1.next = p\n                p1 = p1.next\n            else:\n                p2.next = p\n                p2 = p2.next\n            p = p.next\n            \n        p2.next = None  #\n        \n        p1.next = dummy2.next\n        return dummy1.next\n\n\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -&gt; Optional[ListNode]:\n        # brute force\n        \n        nodes = []\n        p = dummy = ListNode(-1)\n        \n        # go through all nodes\n        for list_ in lists:\n            while list_:  # cannot use for loop to traverse\n                nodes.append(list_.val)\n                list_ = list_.next\n        \n        # sort nodes by value and link them\n        for val in sorted(nodes):\n            p.next = ListNode(val)\n            p = p.next\n        \n        return dummy.next\n\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def deleteDuplicatesUnsorted(self, head: ListNode) -&gt; ListNode:\n        p = head\n        head_dict = {}\n        # go over head, store value and number of times in dict\n        while p:  # while p != None:\n            if p.val in head_dict:\n                head_dict[p.val] += 1\n            else:\n                head_dict[p.val] = 1\n            p = p.next\n\n        # create nodes\n        node_list = []\n        for key, value in head_dict.items():\n            if value == 1:\n                node_list.append(ListNode(key))\n        \n        # create linked list from nodes\n        for i in range(len(node_list) - 1):\n            node_list[i].next = node_list[i + 1]\n        \n        if len(node_list) == 0:\n            return None\n        else:\n            return node_list[0]\n\n\n\npython PriorityQueue, https://www.educative.io/answers/what-is-the-python-priority-queue\nheap\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -&gt; Optional[ListNode]:\n        # create dummy node\n        p = dummy = ListNode(-1)\n        heap = []\n        \n        # store the first node of all the lists\n        import heapq\n        counter = 0\n        for l in lists:\n            if l:\n                heapq.heappush(heap, (l.val, counter, l))\n                counter += 1\n                \n        # merge lists\n        while heap:\n            val, counter, l = heapq.heappop(heap)\n            #p.next = ListNode(val) \n            p.next = l\n            p = p.next\n            l = l.next\n            if l:\n                heapq.heappush(heap, (l.val, counter, l))\n                counter += 1\n        \n        return dummy.next\n    # n_lists = k, n_all_nodes = n\n    \n    # time complexity: nlog(k), heap has k elements, push or pop needs log(k) time, n times in total\n    # space complexity: n (dummy) + k (heap)\n\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -&gt; Optional[ListNode]:\n        dummy = ListNode(-1)\n        dummy.next = head\n        \n        # first pointer\n        p1 = dummy\n        # move n steps\n        for i in range(n):\n            p1 = p1.next\n        \n        # second pointer\n        p2 = dummy\n        while p1.next != None:\n            p1 = p1.next\n            p2 = p2.next\n        \n        p2.next = p2.next.next\n        return dummy.next\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def middleNode(self, head):\n        slow = fast = head\n        while fast and fast.next:  # here is tricky for odd and even, if fast.next != None, fast.next.next can be None\n            slow = slow.next\n            fast = fast.next.next\n        return slow\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def hasCycle(self, head: Optional[ListNode]) -&gt; bool:\n        # fast and slow\n        fast = slow = head\n        \n        while fast and fast.next:\n            fast = fast.next.next\n            slow = slow.next\n            if fast == slow:  # if fast meets slow, fast walks one cycle more\n                return True\n        \n        return False\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def detectCycle(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n        # create fast and slow\n        fast = slow = head\n        \n        while fast and fast.next:\n            fast = fast.next.next\n            slow = slow.next\n            if fast == slow:\n                break\n        \n        # no cycle case\n        if fast == None or fast.next == None:\n            return None\n        \n        # cycle case\n        # fast return to the beginning\n        fast = head\n        while fast != slow:\n            fast = fast.next\n            slow = slow.next\n        \n        return slow\n\n\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def getIntersectionNode(self, headA: ListNode, headB: ListNode) -&gt; Optional[ListNode]:\n        # brute force\n        node_in_a = set()\n        \n        while headA:\n            node_in_a.add(headA)\n            headA = headA.next\n            \n        while headB:\n            if headB in node_in_a:\n                return headB\n            headB = headB.next\n            \n        return None\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def getIntersectionNode(self, headA: ListNode, headB: ListNode) -&gt; Optional[ListNode]:\n        # go through headA, attach headB\n        # go through headB, attach headA\n        pA = headA\n        pB = headB\n        \n        while pA != pB:\n            if pA:\n                pA = pA.next\n            else:  # if pA is None, then go through headB\n                #pA.next = headB  # this changes the linked list\n                pA = headB  # this change the pointer's binding of object\n            if pB:\n                pB = pB.next\n            else:\n                #pB.next = headA\n                pB = headA\n                \n        return pA"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section",
    "href": "posts/2022-08-01-leetcode/index.html#section",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -&gt; Optional[ListNode]:\n        # iterative way\n        \n        # create dummy head\n        dummy = ListNode(-1)\n        p = dummy  # p pointer\n        \n        while list1 != None and list2 != None:\n            if list1.val &lt; list2.val:\n                p.next = list1  # attach all list2 to the end of p, p is dynamically changing\n                list1 = list1.next  # remove the first node\n            else:\n                p.next = list2\n                list2 = list2.next\n            p = p.next  # remove the first node\n            \n        if list1 != None:\n            p.next = list1\n            \n        if list2 != None:\n            p.next = list2\n            \n        return dummy.next"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-1",
    "href": "posts/2022-08-01-leetcode/index.html#section-1",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def partition(self, head: Optional[ListNode], x: int) -&gt; Optional[ListNode]:\n        # create 2 linked lists and join them\n        p1 = dummy1 = ListNode(-1)  # p: pointer, before moves the pointer, before_head keeps the linked list\n        p2 = dummy2 = ListNode(-1)\n        p = head\n        \n        while p != None:\n            if p.val &lt; x:\n                p1.next = p\n                p1 = p1.next\n            else:\n                p2.next = p\n                p2 = p2.next\n            p = p.next\n            \n        p2.next = None  #\n        \n        p1.next = dummy2.next\n        return dummy1.next"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-2",
    "href": "posts/2022-08-01-leetcode/index.html#section-2",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -&gt; Optional[ListNode]:\n        # brute force\n        \n        nodes = []\n        p = dummy = ListNode(-1)\n        \n        # go through all nodes\n        for list_ in lists:\n            while list_:  # cannot use for loop to traverse\n                nodes.append(list_.val)\n                list_ = list_.next\n        \n        # sort nodes by value and link them\n        for val in sorted(nodes):\n            p.next = ListNode(val)\n            p = p.next\n        \n        return dummy.next"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-3",
    "href": "posts/2022-08-01-leetcode/index.html#section-3",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def deleteDuplicatesUnsorted(self, head: ListNode) -&gt; ListNode:\n        p = head\n        head_dict = {}\n        # go over head, store value and number of times in dict\n        while p:  # while p != None:\n            if p.val in head_dict:\n                head_dict[p.val] += 1\n            else:\n                head_dict[p.val] = 1\n            p = p.next\n\n        # create nodes\n        node_list = []\n        for key, value in head_dict.items():\n            if value == 1:\n                node_list.append(ListNode(key))\n        \n        # create linked list from nodes\n        for i in range(len(node_list) - 1):\n            node_list[i].next = node_list[i + 1]\n        \n        if len(node_list) == 0:\n            return None\n        else:\n            return node_list[0]\n\n\n\npython PriorityQueue, https://www.educative.io/answers/what-is-the-python-priority-queue\nheap\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -&gt; Optional[ListNode]:\n        # create dummy node\n        p = dummy = ListNode(-1)\n        heap = []\n        \n        # store the first node of all the lists\n        import heapq\n        counter = 0\n        for l in lists:\n            if l:\n                heapq.heappush(heap, (l.val, counter, l))\n                counter += 1\n                \n        # merge lists\n        while heap:\n            val, counter, l = heapq.heappop(heap)\n            #p.next = ListNode(val) \n            p.next = l\n            p = p.next\n            l = l.next\n            if l:\n                heapq.heappush(heap, (l.val, counter, l))\n                counter += 1\n        \n        return dummy.next\n    # n_lists = k, n_all_nodes = n\n    \n    # time complexity: nlog(k), heap has k elements, push or pop needs log(k) time, n times in total\n    # space complexity: n (dummy) + k (heap)"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-4",
    "href": "posts/2022-08-01-leetcode/index.html#section-4",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -&gt; Optional[ListNode]:\n        dummy = ListNode(-1)\n        dummy.next = head\n        \n        # first pointer\n        p1 = dummy\n        # move n steps\n        for i in range(n):\n            p1 = p1.next\n        \n        # second pointer\n        p2 = dummy\n        while p1.next != None:\n            p1 = p1.next\n            p2 = p2.next\n        \n        p2.next = p2.next.next\n        return dummy.next"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-5",
    "href": "posts/2022-08-01-leetcode/index.html#section-5",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def middleNode(self, head):\n        slow = fast = head\n        while fast and fast.next:  # here is tricky for odd and even, if fast.next != None, fast.next.next can be None\n            slow = slow.next\n            fast = fast.next.next\n        return slow"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-6",
    "href": "posts/2022-08-01-leetcode/index.html#section-6",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def hasCycle(self, head: Optional[ListNode]) -&gt; bool:\n        # fast and slow\n        fast = slow = head\n        \n        while fast and fast.next:\n            fast = fast.next.next\n            slow = slow.next\n            if fast == slow:  # if fast meets slow, fast walks one cycle more\n                return True\n        \n        return False"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-7",
    "href": "posts/2022-08-01-leetcode/index.html#section-7",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def detectCycle(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n        # create fast and slow\n        fast = slow = head\n        \n        while fast and fast.next:\n            fast = fast.next.next\n            slow = slow.next\n            if fast == slow:\n                break\n        \n        # no cycle case\n        if fast == None or fast.next == None:\n            return None\n        \n        # cycle case\n        # fast return to the beginning\n        fast = head\n        while fast != slow:\n            fast = fast.next\n            slow = slow.next\n        \n        return slow"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-8",
    "href": "posts/2022-08-01-leetcode/index.html#section-8",
    "title": "LeetCode",
    "section": "",
    "text": "# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def getIntersectionNode(self, headA: ListNode, headB: ListNode) -&gt; Optional[ListNode]:\n        # brute force\n        node_in_a = set()\n        \n        while headA:\n            node_in_a.add(headA)\n            headA = headA.next\n            \n        while headB:\n            if headB in node_in_a:\n                return headB\n            headB = headB.next\n            \n        return None\n\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def getIntersectionNode(self, headA: ListNode, headB: ListNode) -&gt; Optional[ListNode]:\n        # go through headA, attach headB\n        # go through headB, attach headA\n        pA = headA\n        pB = headB\n        \n        while pA != pB:\n            if pA:\n                pA = pA.next\n            else:  # if pA is None, then go through headB\n                #pA.next = headB  # this changes the linked list\n                pA = headB  # this change the pointer's binding of object\n            if pB:\n                pB = pB.next\n            else:\n                #pB.next = headA\n                pB = headA\n                \n        return pA"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-9",
    "href": "posts/2022-08-01-leetcode/index.html#section-9",
    "title": "LeetCode",
    "section": "26",
    "text": "26\n\nMy way\nclass Solution:\n    def removeDuplicates(self, nums: List[int]) -&gt; int:\n        # 2 pointers, fast and slow\n        fast = slow = 0\n        fast += 1\n        \n        while fast &lt; len(nums):  # &lt; or &lt;=?\n            if nums[fast] != nums[slow]:\n                nums[slow + 1] = nums[fast]\n                slow += 1\n            fast += 1\n            \n        return slow + 1\n\n\nMore elegant\nclass Solution:\n    def removeDuplicates(self, nums: List[int]) -&gt; int:\n        # 2 pointers, fast and slow\n        fast = slow = 0\n        \n        while fast &lt; len(nums):  # &lt; or &lt;=?\n            if nums[fast] != nums[slow]:\n                slow += 1\n                nums[slow] = nums[fast]\n            fast += 1\n            \n        return slow + 1"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-10",
    "href": "posts/2022-08-01-leetcode/index.html#section-10",
    "title": "LeetCode",
    "section": "83",
    "text": "83\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def deleteDuplicates(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n        if not head:  # extreme case, head is None\n            return None\n        \n        # fast and slow pointers\n        fast = slow = head\n        \n        while fast:  # fast is not None\n            if fast.val != slow.val:\n                slow.next = fast\n                slow = slow.next\n            fast = fast.next\n            \n        slow.next = None  # slow is still connected to the rest, manually point it to None\n        return head"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-11",
    "href": "posts/2022-08-01-leetcode/index.html#section-11",
    "title": "LeetCode",
    "section": "27",
    "text": "27\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -&gt; int:\n        slow = fast = 0\n        \n        while fast &lt; len(nums):\n            if nums[fast] != val:\n                nums[slow] = nums[fast]\n                slow += 1\n            fast += 1\n        return slow"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-12",
    "href": "posts/2022-08-01-leetcode/index.html#section-12",
    "title": "LeetCode",
    "section": "283",
    "text": "283\n\nMethod 1\nclass Solution:\n    def moveZeroes(self, nums: List[int]) -&gt; None:\n        \"\"\"\n        Do not return anything, modify nums in-place instead.\n        \"\"\"\n        # remove 0, then make the last elements 0, not swap\n        fast = slow = 0\n        \n        while fast &lt; len(nums):\n            if nums[fast] != 0:\n                nums[slow] = nums[fast]\n                slow += 1\n            fast += 1\n        \n        #nums[slow:] = 0\n        nums[slow:] = [0] * (len(nums) - slow)  # can only assign an iterable\n        \n        return nums\n\n\nMethod 2, swap\nhttps://leetcode.com/problems/move-zeroes/discuss/1592151/Python-solution-super-simple-clear-explanation\nclass Solution:\n    def moveZeroes(self, nums: List[int]) -&gt; None:\n        \"\"\"\n        Do not return anything, modify nums in-place instead.\n        \"\"\"\n        # 2 pointers\n        # one pointer points to first 0\n        # one pointer points to first non 0\n        i = 0\n        for j in range(len(nums)):\n            if nums[j] != 0:\n                nums[i], nums[j] = nums[j], nums[i]\n                i += 1\n        return nums"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-13",
    "href": "posts/2022-08-01-leetcode/index.html#section-13",
    "title": "LeetCode",
    "section": "167",
    "text": "167\nclass Solution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        left = 0\n        right = len(numbers) - 1\n        \n        while left &lt; right:\n            sum_ = numbers[left] + numbers[right]\n            if sum_ == target:\n                return [left + 1, right + 1]\n            elif sum_ &lt; target:\n                left += 1\n            elif sum_ &gt; target:\n                right -= 1"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-14",
    "href": "posts/2022-08-01-leetcode/index.html#section-14",
    "title": "LeetCode",
    "section": "344",
    "text": "344\nclass Solution:\n    def reverseString(self, s: List[str]) -&gt; None:\n        \"\"\"\n        Do not return anything, modify s in-place instead.\n        \"\"\"\n        # 2 pointers\n        left = 0\n        right = len(s) - 1\n        \n        while left &lt; right:\n            s[left], s[right] = s[right], s[left]\n            left += 1\n            right -= 1\n        \n        return s"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-15",
    "href": "posts/2022-08-01-leetcode/index.html#section-15",
    "title": "LeetCode",
    "section": "5",
    "text": "5\nclass Solution:\n    def longestPalindrome(self, s: str) -&gt; str:\n        \n        # function palindrome\n        def palindrome(s: str, left: int, right: int) -&gt; str:\n            while left &gt;=0 and right &lt; len(s) and s[left] == s[right]:\n                left -= 1\n                right += 1\n            return s[left + 1:right]  # tricky, when the \"while\" fails, left is already left - 1, needs to go to previous place\n        \n        res = \"\"\n        for i in range(len(s)):\n            str1 = palindrome(s, i, i)\n            str2 = palindrome(s, i, i + 1)\n            \n            if len(str1) &gt; len(res):\n                res = str1\n            if len(str2) &gt; len(res):\n                res = str2\n        \n        return res"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-16",
    "href": "posts/2022-08-01-leetcode/index.html#section-16",
    "title": "LeetCode",
    "section": "704",
    "text": "704\nclass Solution:\n    def search(self, nums: List[int], target: int) -&gt; int:\n        left = 0\n        right = len(nums) - 1\n        \n        while left &lt;= right:\n            mid = int((left + right) / 2)\n            if nums[mid] == target:\n                return mid\n            elif nums[mid] &lt; target:\n                left = mid + 1\n            elif nums[mid] &gt; target:\n                right = mid - 1\n        \n        return -1"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-17",
    "href": "posts/2022-08-01-leetcode/index.html#section-17",
    "title": "LeetCode",
    "section": "34",
    "text": "34\n\nMethod 1\nclass Solution:\n    def searchRange(self, nums: List[int], target: int) -&gt; List[int]:\n        \n        def find_start(nums: List[int], target: int) -&gt; int:\n            # tighten right\n            left = 0\n            right = len(nums) - 1\n            \n            while left &lt;= right:\n                mid = (left + right) // 2\n                if nums[mid] == target:\n                    right = mid - 1  # need to -1 or not? when is left, if right is at the end position?\n                    # search [left, mid - 1]\n                elif nums[mid] &lt; target:\n                    left = mid + 1\n                    # search [mid + 1, right]\n                elif nums[mid] &gt; target:\n                    right = mid - 1\n                    # search [left, mid - 1]\n                    \n            if left &gt;= len(nums) or nums[left] != target:\n                return -1\n            return left\n        \n        \n        def find_end(nums: List[int], target: int) -&gt; int:\n            # tighten left bound\n            left = 0\n            right = len(nums) - 1\n            \n            while left &lt;= right:\n                mid = (left + right) // 2\n                if nums[mid] == target:\n                    left = mid + 1  # tighten left\n                elif nums[mid] &lt; target:\n                    left = mid + 1\n                elif nums[mid] &gt; target:\n                    right = mid - 1\n            \n            if right &lt; 0 or nums[right] != target:\n                return -1\n            return right\n        \n        start = find_start(nums, target)\n        end = find_end(nums, target)\n        return [start, end]\n\n\nMethod 2\nhttps://leetcode.com/problems/find-first-and-last-position-of-element-in-sorted-array/discuss/2330593/Binary-search-or-beats-99-or-fully-explained-with-intuition-or-Image-explanation\nclass Solution:\n    def searchRange(self, nums: List[int], target: int) -&gt; List[int]:\n        # find starting position, decrease the right index\n        def find_start(nums: List[int], target: int) -&gt; List[int]:\n            start_idx = -1\n            left = 0\n            right = len(nums) - 1\n            \n            while left &lt;= right:\n                mid = left + (right - left) // 2\n                if nums[mid] == target:\n                    start_idx = mid\n                    right = mid - 1\n                elif nums[mid] &lt; target:\n                    left = mid + 1\n                elif nums[mid] &gt; target:\n                    right = mid - 1\n            \n            return start_idx\n        \n        # find ending position, increase the left index\n        def find_end(nums: List[int], target: int) -&gt; List[int]:\n            end_idx = -1\n            left = 0\n            right = len(nums) - 1\n            \n            while left &lt;= right:\n                mid = left + (right - left) // 2\n                if nums[mid] == target:\n                    end_idx = mid\n                    left = mid + 1\n                elif nums[mid] &lt; target:\n                    left = mid + 1\n                elif nums[mid] &gt; target:\n                    right = mid - 1\n                    \n            return end_idx\n        \n        start_idx = find_start(nums, target)\n        end_idx = find_end(nums, target)\n        return [start_idx, end_idx]"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-18",
    "href": "posts/2022-08-01-leetcode/index.html#section-18",
    "title": "LeetCode",
    "section": "1011",
    "text": "1011\nhttps://leetcode.com/problems/capacity-to-ship-packages-within-d-days/discuss/1581292/Well-Explained-oror-Thought-process-oror-94-faster\nclass Solution:\n    def shipWithinDays(self, weights: List[int], days: int) -&gt; int:\n        def f(capacity):\n            current_weight = 0\n            days_needed = 1  # days needed\n            for weight in weights:\n                current_weight += weight\n                if current_weight &gt; capacity:\n                    current_weight = weight  # current_weight are shipped, weight is added as new day\n                    days_needed += 1\n            return days_needed\n                    \n        \n        # binary search\n        left = max(weights)  # at least, or impossible to ship\n        right = sum(weights)  # ship in one day\n        \n        # find left boundary, decrease right\n        while left &lt;= right:\n            mid = left + (right - left) // 2\n            days_needed = f(mid)\n            if days_needed == days:\n                right = mid - 1\n            elif days_needed &lt; days:  # right to left\n                right = mid - 1\n            elif days_needed &gt; days:  # left go right\n                left = mid + 1\n        \n        return left\nTODO: learn counter: https://realpython.com/python-counter/"
  },
  {
    "objectID": "posts/2022-08-01-leetcode/index.html#section-19",
    "href": "posts/2022-08-01-leetcode/index.html#section-19",
    "title": "LeetCode",
    "section": "76",
    "text": "76\n\nincorrect\nclass Solution:\n    def minWindow(self, s: str, t: str) -&gt; str:\n        # frame of sliding window\n        # [left, right)\n        \n        t_counter = Counter(t)\n        left = right = 0\n        window = Counter()\n        n_chars_needed = len(t)\n        \n        while right &lt; len(s):\n            window.update(s[right])\n            right += 1\n            \n            while window - t_counter and left &lt;= right:\n                #s[left]\n                window.subtract(Counter(s[left]))\n                left += 1\n        # implement substract in dict\n        return s[left:right]\n\n\ncorrect\nclass Solution:\n    def minWindow(self, s: str, t: str) -&gt; str:\n        left = right = 0\n        t_dict = {}\n        window = {}\n        for char in t:\n            if char not in t_dict:\n                t_dict[char] = 0\n                window[char] = 0\n            t_dict[char] += 1\n        \n        n_matched = 0\n        total = len(t_dict)\n        length = float(\"inf\")\n        \n        while right &lt; len(s):\n            right_char = s[right]\n            if right_char in t_dict:\n                window[right_char] += 1  # update window\n                if t_dict[right_char] == window[right_char]:\n                    n_matched += 1\n            \n            while n_matched == total:\n                # save the smallest window until now\n                if right - left &lt; length:  # if n_matched != total, start and length will not be updated\n                    start = left\n                    length = right - left + 1\n                \n                left_char = s[left]\n                if left_char in t_dict:\n                    if t_dict[left_char] == window[left_char]:\n                        n_matched -= 1\n                    window[left_char] -= 1\n                left += 1\n            \n            right += 1\n        \n        if length == float(\"inf\"):\n            return \"\"\n        else:\n            return s[start:start + length]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Haoqi Wang\nPh.D. Candidate  Biomedical Engineering  The University of Texas at Austin \nhaoqiwang@utexas.edu  Resume LinkedIn GitHub  Google Scholar ORCID\n\n\n\n\n\n\n\nAbout me\nI am currently a Ph.D. Candidate in Biomedical Engineering at The University of Texas at Austin. My research focuses on building tools to support clinical decision making.\nMy research interests include:\n\nBiomedical informatics\nMedical biophysics\nMachine learning\nClinical decision support\n\nI received my MS degree in Statistics in 2021 at The University of Texas at Austin and BS degree in Physics at Nanjing University in 2019. I did internship in Scientific Solution Engineering Team at flywheel.io during 2023 summer, where I worked on medical image segmentation and cloud deployment. \n\n\nProjects\nMedical Image Segmentation (code), June. 2023-Aug. 2023\n\nEvaluated 3D image segmentation deep learning models and investigated factors influencing model performance\nCleaned and processed medical images, including DICOM to NIfTI conversion, image augmentation, etc.\nFine-tuned neural networks on Google Cloud Vertex AI GPU CUDA with small dataset, reaching accuracy of 0.95\nManaged CI/CD, developed Docker image and deployed models to Kubernetes cloud, enabling large-scale inference\nBuilt statistical model and demonstrated the importance of acquisition parameters in analyzing outliers\n\nText to SQL (code), Mar. 2021-May 2021\n\nLed a team to build a natural language processing (NLP) model for translating textual questions into SQL queries\nImplemented RNN-LSTM generative model and attention, beam search, grammar check, which increased the accuracy by 27.2% compared with baseline\n\nTime Series Counts Models (code), Sept. 2021-present\n\nProposed recurrent neural networks to forecast crash counts and trained on 700k records from 2010 to 2019 in Texas\nMerged geometric, demographic, and temporal features, resulting an increase in R2 by 15.9% compared to baseline\n\nApplication of online learning in clinical decision support system (code), Oct. 2021-Dec. 2021\n\nProposed to identify a clinical photograph of a prior patient that the current patient perceives as being similar to her mental image of what she will look like post-operatively as fast as possible\nImplemented several bandit algorithms, including Upper Confidence Bound (UCB), tree-based adaptive recommender, graph-based UCB, near neighbor UCB and found near neighbor UCB performs the best with small number of rounds\n\nDeep reinforcement learning in multi-agent environment (code), Mar. 2022-May 2022\n\nBuilt deep reinforcement learning model in multi-agent environment of StarCraft II\nImplemented deep graph neural network to model inter-agent interaction, especially added relation kernel (attention mechanism by transformer) for agents to cooperate, which increased reward more than 90% compared with baseline of deep Q learning (PyTorch)\n\n\n\nPublications\n\nYiran Li*, Jing Cheng*, Peyman Delparastan*, Haoqi Wang*, Severin J. Sigg, Kelsey G. DeFrates, Yi Cao and Phillip B. Messersmith. Molecular design principles of Lysine-DOPA wet adhesion. Nature Communications (2020). (*equal contribution) (paper)\nKrista M. Nicklaus, Haoqi Wang, Mary Catherine Bordes, Alex Zaharan, Urmila Sampathkumar, Audrey L. Cheong, Gregory P. Reece, Summer E. Hanson, Fatima A. Merchant, Mia K. Markey. Potential of intra-operative 3D photography and 3D visualization in breast reconstruction. Plastic and Reconstructive Surgery Global Open (2021). (paper)\nHaoqi Wang, Jun Liu, Mary Catherine Bordes, Deepti Chopra, Gregory P. Reece, Mia K. Markey, Aubri S. Hoffman*. The role of psychosocial factors in patients’ recollections of breast reconstruction options discussed with their surgeons. Scientific Reports (2022). (paper, code)\nTheodore Charm, Haoqi Wang, Natalia Zuniga-Garcia, Mostaq Ahmed, Kara M Kockelman. Predicting crash occurrence at intersections in Texas: an opportunity for machine learning. Transportation Planning and Technology (2023). (paper, code)\nHaoqi Wang, Krista Nicklaus, Eloise Jewett, Eeshaan Rehani, Tzuan A. Chen, Jeff Engelmann, Mary Catherine Bordes, Deepti Chopra, Gregory P. Reece, Z-Hye Lee, Mia K. Markey. Assessing Saliency Models of Observers’ Visual Attention on Acquired Facial Differences. Journal of Medical Imaging (2023). (paper)"
  }
]